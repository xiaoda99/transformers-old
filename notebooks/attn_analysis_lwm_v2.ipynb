{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid/xd/.cache/torch/transformers\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "# from transformers.trainer_utils import EvaluationStrategy\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-large', cache_dir=cache_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', cache_dir=cache_dir)\n",
    "models['roberta-large'] = (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-11b'\n",
    "proxies = {'http': '192.168.50.1:1081'}\n",
    "model = model11b = T5ForConditionalGeneration.from_pretrained(model_name, proxies=proxies)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-11b')\n",
    "tokenizer.decode_strip_special_tokens = MethodType(decode_strip_special_tokens, tokenizer)\n",
    "tokenizer.decode_old = MethodType(decode_old, tokenizer)\n",
    "\n",
    "models['t5-11b'] = model, tokenizer\n",
    "\n",
    "device_map = {0: list(range(0, 6)), 1: list(range(6, 15)), 2: list(range(15, 24))}\n",
    "model.parallelize(device_map)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-xl'  # medium / large / xl\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-2.7B/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/3c80ef2946e1aacc6dd37cb986ea989c29c92775701655bedf14d8791825a30b.180e94c8adf77aa69b51b48271e6cd2b143ce422d10bdefa9ace1512346c33c1\n",
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-2.7B/resolve/main/pytorch_model.bin\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/0839a11efa893f2a554f8f540f904b0db0e5320a2b1612eb02c3fd25471c189a.a144c17634fa6a7823e398888396dd623e204dce9e33c3175afabfbf24bd8f56\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/vocab.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/merges.txt\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/added_tokens.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/special_tokens_map.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'roberta-large'\n",
    "# model_name = 'gpt2-xl'\n",
    "model_name = 'EleutherAI/gpt-neo-2.7B'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "model, tokenizer = models[model_name]\n",
    "\n",
    "masked_lm = tokenizer.mask_token is not None and len(tokenizer.additional_special_tokens) == 0\n",
    "if masked_lm:\n",
    "    mask_token = tokenizer.mask_token  # '<mask>' for roberta\n",
    "elif len(tokenizer.additional_special_tokens) > 0:\n",
    "    mask_token = tokenizer.additional_special_tokens[0]  # '<sxtra_id_0>' for t5\n",
    "else:\n",
    "    mask_token = ''  # for gpt2\n",
    "if masked_lm: nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from attattr\n",
    "def scaled_input(emb, num_points, baseline=None):\n",
    "    # shape of emb: (bsz, num_head, seq_len, seq_len)\n",
    "    assert emb.size(0) == 1\n",
    "    if baseline is None: baseline = torch.zeros_like(emb)   \n",
    "    step = (emb - baseline) / num_points\n",
    "#     res = torch.cat([baseline + step * i for i in range(num_points)], dim=0)\n",
    "    res = torch.cat([baseline + step * (i + 1) for i in range(num_points)], dim=0)  # XD\n",
    "    return res, step\n",
    "\n",
    "# from https://discuss.pytorch.org/t/get-top-k-indices-values-of-all-rows/89354\n",
    "def unravel_index(index, shape):\n",
    "    out = []\n",
    "    for dim in reversed(shape):\n",
    "        out.append(index % dim)\n",
    "        index = index // dim\n",
    "    r = tuple(reversed(out))\n",
    "    return torch.cat([i.unsqueeze(-1) for i in r], dim=-1).cpu().tolist() if type(index) in [torch.Tensor] else r\n",
    "\n",
    "def numpy(a, decimals=4): return a.detach().cpu().numpy().round(decimals)\n",
    "\n",
    "def h2topk(h, k=4, return_probs=True):\n",
    "    if not hasattr(h2topk, 'ln') or h2topk.ln.normalized_shape[0] != h.size(-1):\n",
    "        h2topk.ln = nn.LayerNorm(h.size(-1))\n",
    "#     r = model.lm_head(h2topk.ln(h))\n",
    "    r = model.lm_head(h)\n",
    "    if return_probs: r = r.softmax(-1)\n",
    "    return r.topk(k, dim=-1) if k > 0 else r\n",
    "\n",
    "def globalize(tensor):\n",
    "    if tensor.dim() == 4: return tensor  # global attention\n",
    "    assert tensor.dim() == 5, str(tensor.dim())\n",
    "    assert tensor.size(1) == 1, str(tensor.size(1))  # num_blocks\n",
    "    seq_len = tensor.size(3)\n",
    "    return tensor.squeeze(1)[:, :, :, -seq_len:]  # (bsz, num_blocks, H, seq_len, block_len) -> (bsz, H, seq_len, seq_len)\n",
    "\n",
    "def show_topk(values, indices, values_fn=numpy, indices_fn=tokenizer.convert_ids_to_tokens):\n",
    "    return dict(OrderedDict(zip(indices_fn(indices), values_fn(values))))\n",
    "\n",
    "def append_tokens_to_positions(position_tensor):\n",
    "    positions = numpy(position_tensor)\n",
    "    return ['%d %s' % (p, tokens[p]) for p in positions]\n",
    "\n",
    "def getdelattr(obj, name):\n",
    "    r = getattr(obj, name, None)\n",
    "    if hasattr(obj, name): delattr(obj, name)\n",
    "    return r\n",
    "\n",
    "def get_attn_module(block):\n",
    "    m = block.attn\n",
    "    if hasattr(m, 'attention'): m = m.attention  # for gpt-neo\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to('cuda:6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /usr/local/cuda/version.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import choice, choices, shuffle, sample\n",
    "import string\n",
    "vocab = list(string.ascii_uppercase) #+ ['_'] * 16\n",
    "#vocab = list(string.digits)[1:]\n",
    "num = list(string.digits)[1:]\n",
    "aph = list(string.ascii_uppercase)\n",
    "query_vocab = list(string.ascii_uppercase)\n",
    "nrows, ncols = 8, 4\n",
    "has_query = False#True#\n",
    "has_output = True\n",
    "def map_fn(x): return x.lower()\n",
    "\n",
    "for _ in range(nrows):\n",
    "    input_tokens = sample(vocab, ncols)#位置\n",
    "#     input_tokens = [choice(vocab) for _ in range(ncols)]\n",
    "    i = random.randint(0, len(input_tokens) - 1)\n",
    "    input_tokens[i] = choice(num)\n",
    "    special = input_tokens[i]\n",
    "#    input_tokens[i] = input_tokens[i].lower()\n",
    "#    special = input_tokens[i]\n",
    "#     input_tokens[-1] = input_tokens[0]\n",
    "#    special = choice(vocab)#找特殊\n",
    "#    input_tokens = [choice(vocab).lower()] * (ncols - 1) + [special.lower()]#找特殊\n",
    "#    shuffle(input_tokens)\n",
    "    print(' '.join(input_tokens), end='')\n",
    "    if has_query:\n",
    "#         query_tokens = sample(input_tokens, ncols - 1)\n",
    "#        query_tokens = input_tokens.copy()\n",
    "#        i = random.randint(0, len(input_tokens) - 1)\n",
    " #       query_tokens = input_tokens[i]\n",
    "        query_tokens = choice(input_tokens)\n",
    "#        query_tokens[i] = choice(vocab)\n",
    "#         query_tokens = [t.lower() for t in query_tokens]\n",
    "        print(',', ' '.join(query_tokens), end='')\n",
    "    print(' -> ', end='')\n",
    "    if has_output:\n",
    " #       output_tokens = input_tokens[2]#.upper()#位置\n",
    "#        output_tokens = special.lower()\n",
    "#        output_tokens = choice(input_tokens)\n",
    " #       output_tokens = list(set(input_tokens) - set(query_tokens))\n",
    "#         output_tokens = map(map_fn, input_tokens)\n",
    "#         output_tokens = reverse(input_tokens)\n",
    "#         output_tokens = query_tokens[i].lower()\n",
    "#        output_tokens = special\n",
    "        output_tokens = input_tokens[i]\n",
    "        print(''.join(output_tokens), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_d = {\n",
    "    'find diff1':'''\n",
    "2 8 2 2 -> 8\n",
    "3 6 3 3 -> 6\n",
    "7 9 7 7 -> 9\n",
    "3 1 3 3 -> 1\n",
    "7 7 1 7 -> 1\n",
    "9 5 9 9 -> 5\n",
    "5 5 5 7 -> 7\n",
    "7 2 7 7 -> 2''',\n",
    "    'find diff2':'''\n",
    "1 4 4 4 -> 1\n",
    "4 8 4 4 -> 8\n",
    "5 5 4 5 -> 4\n",
    "8 5 5 5 -> 8\n",
    "8 8 8 7 -> 7\n",
    "2 2 9 2 -> 9\n",
    "1 8 1 1 -> 8\n",
    "3 3 2 3 -> 2''',\n",
    "    'find diff3':'''\n",
    "8 8 8 7 -> 7\n",
    "7 7 9 7 -> 9\n",
    "8 8 8 1 -> 1\n",
    "3 3 3 1 -> 1\n",
    "8 6 8 8 -> 6\n",
    "9 6 9 9 -> 6\n",
    "8 8 8 5 -> 5\n",
    "9 9 3 9 -> 3''',\n",
    "    'find figure1':'''\n",
    "8 W U E -> 8\n",
    "R F 2 D -> 2\n",
    "7 M U P -> 7\n",
    "J 3 U F -> 3\n",
    "4 E S L -> 4\n",
    "Z 6 K L -> 6\n",
    "9 D M C -> 9\n",
    "C N 8 U -> 8''',\n",
    "    'find figure2':'''\n",
    "F V 4 A -> 4\n",
    "V H Z 1 -> 1\n",
    "Z K L 3 -> 3\n",
    "S 9 K M -> 9\n",
    "R V 5 L -> 5\n",
    "3 B E D -> 3\n",
    "Y O 6 N -> 6\n",
    "Q L W 8 -> 8''',\n",
    "    'find figure3':'''\n",
    "V T 2 X -> 2\n",
    "W X 6 F -> 6\n",
    "O X 8 S -> 8\n",
    "K Z I 2 -> 2\n",
    "R E K 9 -> 9\n",
    "H 8 F U -> 8\n",
    "E L 8 V -> 8\n",
    "X 4 C Q -> 4''',\n",
    "     'find figure4':'''\n",
    "3 Q D A -> 3\n",
    "6 Y Q L -> 6\n",
    "G X I 8 -> 8\n",
    "A 2 J D -> 2\n",
    "S B 1 I -> 1\n",
    "Y 7 F K -> 7\n",
    "I J M 6 -> 6\n",
    "O P 1 R -> 1''',\n",
    "     'find figure5':'''\n",
    "G U Z 5 -> 5\n",
    "U 6 K S -> 6\n",
    "Y 7 C G -> 7\n",
    "8 Z H U -> 8\n",
    "1 V J G -> 1\n",
    "2 Q V H -> 2\n",
    "I 5 U R -> 5\n",
    "F 5 Q K -> 5''',\n",
    "     'find figure6':'''\n",
    "2 R L Z -> 2\n",
    "E I 3 M -> 3\n",
    "R 8 D F -> 8\n",
    "A V P 4 -> 4\n",
    "T G W 4 -> 4\n",
    "L 6 P Y -> 6\n",
    "X 6 V T -> 6\n",
    "1 A X I -> 1''',\n",
    "     'find figure7':'''\n",
    "V A 4 P -> 4\n",
    "Y S 9 U -> 9\n",
    "R U 1 K -> 1\n",
    "V T S 8 -> 8\n",
    "N 5 M L -> 5\n",
    "C S 2 N -> 2\n",
    "A 1 J K -> 1\n",
    "3 P T B -> 3''',\n",
    "     'find figure8':'''\n",
    "3 Z S J -> 3\n",
    "P J N 1 -> 1\n",
    "L K H 9 -> 9\n",
    "A 5 M P -> 5\n",
    "C N F 6 -> 6\n",
    "U S 5 T -> 5\n",
    "R F 5 C -> 5\n",
    "N E G 5 -> 5''',\n",
    "     'find figure9':'''\n",
    "8 H R U -> 8\n",
    "1 Y T X -> 1\n",
    "A X 1 D -> 1\n",
    "F Z 6 Y -> 6\n",
    "G P 1 J -> 1\n",
    "T K 4 J -> 4\n",
    "W I C 8 -> 8\n",
    "Y 8 K R -> 8''',\n",
    "    'find figure10':'''\n",
    "8 H R U -> 8\n",
    "1 Y T X -> 1\n",
    "A X 1 D -> 1\n",
    "F Z 6 Y -> 6\n",
    "G P 1 J -> 1\n",
    "T K 4 J -> 4\n",
    "W I C 8 -> 8\n",
    "Y 8 K R -> 8''',\n",
    "     'find aph1':'''\n",
    "V T 2 X -> 2\n",
    "W X 6 F -> 6\n",
    "O X 8 S -> 8\n",
    "K Z I 2 -> 2\n",
    "R E K 9 -> 9\n",
    "H 8 F U -> 8\n",
    "E L 8 V -> 8\n",
    "X 4 C Q -> 4''',\n",
    "     'find aph2':'''\n",
    "Y 4 5 1 -> Y\n",
    "6 2 4 W -> W\n",
    "1 X 2 3 -> X\n",
    "4 5 1 U -> U\n",
    "1 Q 3 2 -> Q\n",
    "9 1 2 N -> N\n",
    "J 4 1 5 -> J\n",
    "9 6 L 1 -> L''',\n",
    "    'find aph3':'''\n",
    "R 8 7 3 -> R\n",
    "9 5 P 3 -> P\n",
    "3 2 M 7 -> M\n",
    "1 4 W 7 -> W\n",
    "1 2 H 5 -> H\n",
    "H 7 8 9 -> H\n",
    "3 6 5 P -> P\n",
    "D 9 1 3 -> D''',\n",
    "     'find aph4':'''\n",
    "5 I 4 6 -> I\n",
    "8 4 B 9 -> B\n",
    "9 I 2 8 -> I\n",
    "3 7 6 F -> F\n",
    "6 1 F 2 -> F\n",
    "3 O 7 5 -> O\n",
    "6 Z 5 7 -> Z\n",
    "J 9 1 6 -> J''',\n",
    "    'find aph5':'''\n",
    "1 G 7 3 -> G\n",
    "4 3 2 O -> O\n",
    "1 2 D 7 -> D\n",
    "1 H 7 6 -> H\n",
    "3 5 1 B -> B\n",
    "9 L 1 4 -> L\n",
    "7 I 9 6 -> I\n",
    "2 3 5 U -> U''',\n",
    "    'find aph6':'''\n",
    "K 5 8 6 -> K\n",
    "3 M 2 4 -> M\n",
    "4 5 L 9 -> L\n",
    "3 2 T 4 -> T\n",
    "G 9 3 7 -> G\n",
    "H 4 3 8 -> H\n",
    "7 4 Q 1 -> Q\n",
    "9 1 R 6 -> R''',\n",
    "     'find aph7':'''\n",
    "9 B 8 6 -> B\n",
    "X 7 2 8 -> X\n",
    "B 7 1 3 -> B\n",
    "4 5 3 K -> K\n",
    "6 3 5 H -> H\n",
    "5 2 C 6 -> C\n",
    "3 2 8 I -> I\n",
    "9 2 4 Y -> Y''',\n",
    "     'find aph8':'''\n",
    "6 2 F 7 -> F\n",
    "2 3 Z 7 -> Z\n",
    "6 2 3 B -> B\n",
    "8 4 2 J -> J\n",
    "5 T 7 2 -> T\n",
    "3 N 8 9 -> N\n",
    "9 1 G 3 -> G\n",
    "9 8 6 U -> U''',\n",
    "     'find aph9':'''\n",
    "G 1 8 5 -> G\n",
    "6 4 8 R -> R\n",
    "I 5 2 1 -> I\n",
    "9 1 I 4 -> I\n",
    "1 2 9 O -> O\n",
    "4 1 2 W -> W\n",
    "2 Y 5 8 -> Y\n",
    "2 N 3 7 -> N''',\n",
    "     'find aph10':'''\n",
    "3 8 4 W -> W\n",
    "4 8 7 F -> F\n",
    "S 3 9 4 -> S\n",
    "1 K 2 6 -> K\n",
    "1 I 6 3 -> I\n",
    "8 E 2 9 -> E\n",
    "2 7 6 R -> R\n",
    "2 T 6 5 -> T''',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_p = {\n",
    "    '1 2 3 -> 1a':'''\n",
    "5 3 1 -> 5\n",
    "9 1 5 -> 9\n",
    "1 4 5 -> 1\n",
    "6 7 9 -> 6\n",
    "8 5 3 -> 8\n",
    "1 9 2 -> 1\n",
    "2 8 5 -> 2\n",
    "5 8 7 -> 5''',\n",
    "    '1 2 3 -> 1b':'''\n",
    "5 9 7 -> 5\n",
    "7 5 1 -> 7\n",
    "4 3 8 -> 4\n",
    "8 4 3 -> 8\n",
    "9 6 5 -> 9\n",
    "7 9 6 -> 7\n",
    "1 5 7 -> 1\n",
    "7 5 9 -> 7''',\n",
    "    '1 2 3 -> 1c':'''\n",
    "4 7 1 -> 4\n",
    "2 9 4 -> 2\n",
    "2 8 9 -> 2\n",
    "6 4 3 -> 6\n",
    "9 3 7 -> 9\n",
    "5 6 1 -> 5\n",
    "5 8 3 -> 5\n",
    "8 6 9 -> 8''',\n",
    "    '1 2 3 -> 2a':'''\n",
    "7 5 2 -> 5\n",
    "8 3 5 -> 3\n",
    "9 7 6 -> 7\n",
    "9 2 5 -> 2\n",
    "3 6 5 -> 6\n",
    "8 3 4 -> 3\n",
    "4 1 5 -> 1\n",
    "3 1 5 -> 1''',\n",
    "    '1 2 3 -> 2b':'''\n",
    "4 3 7 -> 3\n",
    "7 1 3 -> 1\n",
    "6 7 9 -> 7\n",
    "6 7 3 -> 7\n",
    "1 5 7 -> 5\n",
    "4 3 2 -> 3\n",
    "9 6 4 -> 6\n",
    "8 1 6 -> 1''',\n",
    "     '1 2 3 -> 2c':'''\n",
    "4 3 7 -> 3\n",
    "5 2 3 -> 2\n",
    "8 4 6 -> 4\n",
    "9 1 3 -> 1\n",
    "4 6 8 -> 6\n",
    "1 8 2 -> 8\n",
    "4 5 8 -> 5\n",
    "9 2 8 -> 2''',\n",
    "     '1 2 3 -> 3a':'''\n",
    "4 8 5 -> 5\n",
    "5 2 8 -> 8\n",
    "1 6 3 -> 3\n",
    "5 2 1 -> 1\n",
    "7 1 8 -> 8\n",
    "7 4 9 -> 9\n",
    "3 4 5 -> 5\n",
    "1 4 5 -> 5''',\n",
    "     '1 2 3 -> 3b':'''\n",
    "9 1 4 -> 4\n",
    "8 9 5 -> 5\n",
    "9 8 2 -> 2\n",
    "5 8 6 -> 6\n",
    "6 8 3 -> 3\n",
    "9 5 7 -> 7\n",
    "7 8 9 -> 9\n",
    "5 2 8 -> 8''',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspecial = {\n",
    "     'find diff_alphabet1':'''\n",
    "r o o o -> r\n",
    "k k k x -> x\n",
    "a a a s -> s\n",
    "u h h h -> u\n",
    "y y c y -> c\n",
    "r f f f -> r\n",
    "f f f c -> c\n",
    "h m h h -> m''',\n",
    "    'find diff_alphabet2':'''\n",
    "c m m m -> c\n",
    "j t j j -> t\n",
    "d d d x -> x\n",
    "w w w j -> j\n",
    "p p h p -> h\n",
    "p a p p -> a\n",
    "t k t t -> k\n",
    "q q h q -> h''',\n",
    "    'find diff_alphabet3':'''\n",
    "s l s s -> l\n",
    "d d m d -> m\n",
    "f r f f -> r\n",
    "n n n b -> b\n",
    "l c c c -> l\n",
    "k k k t -> t\n",
    "c q c c -> q\n",
    "y y y p -> p''',\n",
    "    'find diff_alphabet4':'''\n",
    "b b b w -> w\n",
    "w s s s -> w\n",
    "v k v v -> k\n",
    "y y c y -> c\n",
    "x x e x -> e\n",
    "e d e e -> d\n",
    "o a a a -> o\n",
    "b b x b -> x''',\n",
    "    'find diff_alphabet5':'''\n",
    "f f f f -> f\n",
    "w w w b -> b\n",
    "k s s s -> k\n",
    "u u e u -> e\n",
    "v y y y -> v\n",
    "m m d m -> d\n",
    "b b b g -> g\n",
    "t t t a -> a''',\n",
    "    'find diff_alphabet6':'''\n",
    "u k u u -> k\n",
    "i c i i -> c\n",
    "n k k k -> n\n",
    "u i i i -> u\n",
    "m d m m -> d\n",
    "t t t i -> i\n",
    "n m m m -> n\n",
    "t t a t -> a''',\n",
    "    'find diff_alphabet7':'''\n",
    "f f g f -> g\n",
    "o y o o -> y\n",
    "d d n d -> n\n",
    "m c m m -> c\n",
    "h l h h -> l\n",
    "l n n n -> l\n",
    "a k k k -> a\n",
    "d d y d -> y''',\n",
    "    'find diff_alphabet8':'''\n",
    "o e e e -> o\n",
    "x x x u -> u\n",
    "m m m u -> u\n",
    "f f h f -> h\n",
    "g g g p -> p\n",
    "v v n v -> n\n",
    "y y w y -> w\n",
    "e e e u -> u''',\n",
    "    'find diff_alphabet9':'''\n",
    "a a o a -> o\n",
    "o o o x -> x\n",
    "l e e e -> l\n",
    "c h h h -> c\n",
    "g g g g -> g\n",
    "q d d d -> q\n",
    "p w w w -> p\n",
    "f z z z -> f''',\n",
    "    'find diff_alphabet10':'''\n",
    "k k o k -> o\n",
    "r r r c -> c\n",
    "a p p p -> a\n",
    "q q q l -> l\n",
    "g g g w -> w\n",
    "n n g n -> g\n",
    "h n n n -> h\n",
    "r a r r -> a''',\n",
    "    'find lowercase1':'''\n",
    "j J P -> j\n",
    "V I w -> w\n",
    "h B T -> h\n",
    "M m G -> m\n",
    "u Q K -> u\n",
    "c X Q -> c\n",
    "J Y d -> d\n",
    "A k G -> k\n",
    "v E H -> v\n",
    "F c G -> c''',\n",
    "    'find lowercase2':'''\n",
    "D O a -> a\n",
    "p A W -> p\n",
    "R k V -> k\n",
    "g G D -> g\n",
    "j R X -> j\n",
    "h S G -> h\n",
    "A X s -> s\n",
    "R m V -> m\n",
    "y D O -> y\n",
    "v M F -> v''',\n",
    "    'find lowercase3':'''\n",
    "G R z -> z\n",
    "Z O q -> q\n",
    "a D V -> a\n",
    "v H L -> v\n",
    "v M A -> v\n",
    "L k D -> k\n",
    "O V r -> r\n",
    "T a O -> a\n",
    "v W M -> v\n",
    "i E K -> i''',\n",
    "    'find lowercase4':'''\n",
    "A i B -> i\n",
    "C v F -> v\n",
    "K b Y -> b\n",
    "R H u -> u\n",
    "G o R -> o\n",
    "u H W -> u\n",
    "l G K -> l\n",
    "R L s -> s\n",
    "f H S -> f\n",
    "k Z I -> k''',\n",
    "    'find lowercase5':'''\n",
    "W B o -> o\n",
    "F X a -> a\n",
    "N R i -> i\n",
    "Y i U -> i\n",
    "V Q f -> f\n",
    "M T h -> h\n",
    "D R l -> l\n",
    "x J U -> x\n",
    "q L P -> q\n",
    "E q Z -> q''',\n",
    "    'find lowercase6':'''\n",
    "A R c -> c\n",
    "J z L -> z\n",
    "b Y H -> b\n",
    "t J Q -> t\n",
    "C K n -> n\n",
    "G x I -> x\n",
    "A K u -> u\n",
    "t P Z -> t\n",
    "H m R -> m\n",
    "L f K -> f''',\n",
    "    'find lowercase7':'''\n",
    "T I w -> w\n",
    "e Z N -> e\n",
    "E Y h -> h\n",
    "F j U -> j\n",
    "J S g -> g\n",
    "B D m -> m\n",
    "J n E -> n\n",
    "I p Z -> p\n",
    "T l P -> l\n",
    "L D o -> o''',\n",
    "    'find lowercase8':'''\n",
    "V O j -> j\n",
    "D g N -> g\n",
    "N R x -> x\n",
    "K V s -> s\n",
    "K R w -> w\n",
    "D C j -> j\n",
    "Z K j -> j\n",
    "S A d -> d\n",
    "D R i -> i\n",
    "V T r -> r''',\n",
    "    'find lowercase9':'''\n",
    "N O p -> p\n",
    "W k P -> k\n",
    "u B P -> u\n",
    "I l A -> l\n",
    "p V F -> p\n",
    "c R Z -> c\n",
    "M d A -> d\n",
    "Q z P -> z\n",
    "w K L -> w\n",
    "d I N -> d''',\n",
    "    'find lowercase10':'''\n",
    "J c Y -> c\n",
    "q U W -> q\n",
    "F D r -> r\n",
    "q F A -> q\n",
    "G L h -> h\n",
    "m L C -> m\n",
    "v N J -> v\n",
    "N r S -> r\n",
    "i P W -> i\n",
    "D r M -> r''',\n",
    "    'find del1':'''\n",
    "Z Y, Y -> Z\n",
    "K B, K -> K\n",
    "N E, E -> N\n",
    "J S, J -> S\n",
    "O W, O -> W\n",
    "F R, F -> R\n",
    "J S, S -> J\n",
    "N O, O -> N\n",
    "P R, P -> R''',\n",
    "    'find del2':'''\n",
    "J S, J -> S\n",
    "G A, G -> A\n",
    "F S, F -> S\n",
    "B X, X -> B\n",
    "O M, M -> O\n",
    "C Y, C -> Y\n",
    "U W, U -> W\n",
    "B X, X -> B\n",
    "T I, I -> T''',\n",
    "    'find del3':'''\n",
    "M D, M -> D\n",
    "U S, S -> U\n",
    "Q W, W -> Q\n",
    "T A, A -> T\n",
    "W T, W -> T\n",
    "J W, W -> J\n",
    "Q U, Q -> U\n",
    "W R, W -> R\n",
    "U D, U -> D''',\n",
    "    'find del4':'''\n",
    "X E, X -> E\n",
    "S M, M -> S\n",
    "T L, L -> T\n",
    "Y X, Y -> X\n",
    "B T, T -> B\n",
    "Z C, C -> Z\n",
    "N X, X -> N\n",
    "I F, I -> F\n",
    "J Z, Z -> J''',\n",
    "    'find del5':'''\n",
    "X Q, X -> Q\n",
    "N I, I -> N\n",
    "D T, D -> T\n",
    "X E, X -> E\n",
    "T Z, T -> Z\n",
    "O J, O -> J\n",
    "A R, R -> A\n",
    "I N, N -> I\n",
    "V F, F -> V''',\n",
    "    'find del6':'''\n",
    "F O, F -> O\n",
    "J C, J -> C\n",
    "W S, S -> W\n",
    "H G, G -> H\n",
    "D Q, D -> Q\n",
    "K E, K -> E\n",
    "Z F, F -> Z\n",
    "N M, M -> N\n",
    "V G, V -> G''',\n",
    "    'find del7':'''\n",
    "C O, C -> O\n",
    "U P, P -> U\n",
    "R W, R -> W\n",
    "S U, S -> U\n",
    "Y M, M -> Y\n",
    "W N, N -> W\n",
    "C G, C -> G\n",
    "S I, S -> I\n",
    "D W, D -> W''',\n",
    "    'find del8':'''\n",
    "N D, D -> N\n",
    "M Y, Y -> M\n",
    "H Z, H -> Z\n",
    "N J, N -> J\n",
    "M X, X -> M\n",
    "R G, G -> R\n",
    "V R, R -> V\n",
    "F S, F -> S\n",
    "O Y, O -> Y''',\n",
    "    'find del9':'''\n",
    "B U, U -> B\n",
    "N V, N -> V\n",
    "J U, J -> U\n",
    "F O, O -> F\n",
    "L R, L -> R\n",
    "L Q, Q -> L\n",
    "I G, G -> I\n",
    "E Q, E -> Q\n",
    "B V, V -> B''',\n",
    "    'find del10':'''\n",
    "D Z, D -> Z\n",
    "R W, W -> R\n",
    "F C, C -> F\n",
    "F C, C -> F\n",
    "C K, K -> C\n",
    "W J, J -> W\n",
    "Y B, Y -> B\n",
    "L U, U -> L\n",
    "B I, B -> I''',\n",
    "    'find specia0': '''\n",
    "n d d -> n\n",
    "f f d -> d\n",
    "e b e -> b\n",
    "s q s -> q\n",
    "d d o -> o\n",
    "c e e -> c\n",
    "g g t -> t\n",
    "w i i -> w''',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {\n",
    "    'A B C -> B':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E''',\n",
    "    'A B C -> b':'''\n",
    "M T L -> t\n",
    "X I J -> i\n",
    "T D U -> d\n",
    "K L H -> l\n",
    "L C V -> c\n",
    "J Y D -> y\n",
    "A K G -> k\n",
    "V E H -> e''',\n",
    "    'A B C -> a':'''\n",
    "M T L -> m\n",
    "X I J -> x\n",
    "T D U -> t\n",
    "K L H -> k\n",
    "L C V -> l\n",
    "J Y D -> j\n",
    "A K G -> a\n",
    "V E H -> v''',\n",
    "    'A B C -> a2':'''\n",
    "D Q H -> d\n",
    "I J L -> i\n",
    "O V R -> o\n",
    "T F E -> t\n",
    "Q N R -> q\n",
    "W Q B -> w\n",
    "Z H J -> z\n",
    "W V Y -> w''',\n",
    "     'A B C -> b':'''\n",
    "T N X -> n\n",
    "D E U -> e\n",
    "U R Q -> r\n",
    "C T B -> t\n",
    "X B F -> b\n",
    "Q G V -> g''',\n",
    "    'A B C -> c':'''\n",
    "P Q Z -> z\n",
    "C E E -> e\n",
    "U G H -> h\n",
    "X B F -> f\n",
    "K P Y -> y\n",
    "A M A -> a\n",
    "K E T -> t''',\n",
    "    'A B A -> b': '''\n",
    "A L A -> l\n",
    "F B F -> b\n",
    "M A M -> a\n",
    "O W O -> w\n",
    "W Y W -> y\n",
    "D G D -> g''',\n",
    "     'A B C -> A':'''\n",
    "M T L -> M\n",
    "X I J -> X\n",
    "T D U -> T\n",
    "K L H -> K\n",
    "L C V -> L\n",
    "J Y D -> J\n",
    "A K G -> A\n",
    "V E H -> V''',\n",
    "    'A B C D -> A': '''\n",
    "A H S H -> A\n",
    "S D N F -> S\n",
    "U D B S -> U\n",
    "Z G M E -> Z''',\n",
    "     'A B C D -> a': '''\n",
    "A H S H -> a\n",
    "S D N F -> s\n",
    "U D B S -> u\n",
    "Z G M E -> z''',\n",
    "    'A B C -> C': '''\n",
    "M T L -> L\n",
    "X I J -> J\n",
    "T D U -> U\n",
    "K L H -> H\n",
    "L C V -> V\n",
    "J Y D -> D\n",
    "A K G -> G\n",
    "V E H -> H''',\n",
    "    'A B C -> C2': '''\n",
    "Z J V -> V\n",
    "H V G -> G\n",
    "H X L -> L\n",
    "S X M -> M\n",
    "R N Y -> Y\n",
    "Z D A -> A\n",
    "V W M -> M\n",
    "D T C -> C''',\n",
    "    'A B A -> B': '''\n",
    "N S N -> S\n",
    "N K N -> K\n",
    "O M O -> M\n",
    "T V T -> V''',\n",
    "    'find lowercase': '''\n",
    "X C e -> e\n",
    "S f Z -> f\n",
    "K y N -> y\n",
    "q M N -> q\n",
    "u S N -> u\n",
    "S v Y -> v\n",
    "v I J -> v\n",
    "Y a N -> a''',\n",
    "    'AbC->B': '''\n",
    "X C e -> E\n",
    "S f Z -> F\n",
    "K y N -> Y\n",
    "q M N -> Q\n",
    "u S N -> U\n",
    "S v Y -> V\n",
    "g I J -> G\n",
    "Y a N -> A\n",
    "L n J -> N\n",
    "d H I -> D\n",
    "Z r C -> R\n",
    "U S t -> T\n",
    "K r A -> R\n",
    "I G m -> M\n",
    "t O X -> T''',  # failed\n",
    "    'set diff': '''\n",
    "G L C, G L -> C\n",
    "Y P J, Y P -> J\n",
    "E S A, S A -> E\n",
    "U P W, U P -> W\n",
    "W Z A, W A -> Z\n",
    "Z Q J, Z J -> Q\n",
    "C Y L, Y L -> C\n",
    "C K Z, C Z -> K''',\n",
    "    'find special': '''\n",
    "n d d -> n\n",
    "f f d -> d\n",
    "e b e -> b\n",
    "s q s -> q\n",
    "d d o -> o\n",
    "c e e -> c\n",
    "g g t -> t\n",
    "w i i -> w''',  # failed\n",
    "    'find special2': '''\n",
    "n d d d -> n\n",
    "f f d f -> d\n",
    "e b e e -> b\n",
    "s q s s -> q\n",
    "d d d o -> o\n",
    "e c e e -> c\n",
    "a z z z -> a\n",
    "t t t w -> w''',\n",
    "    'ABC,AXC->X': '''\n",
    "D O Q, K O Q -> K\n",
    "K H N, K O N -> O\n",
    "R X P, R U P -> U\n",
    "Z D V, Z I V -> I\n",
    "X C G, X C J -> J\n",
    "Z G V, B G V -> B\n",
    "H V T, H V L -> L\n",
    "J E M, J E L -> L\n",
    "A W K, A W U -> U\n",
    "F B Y, F B D -> D''',\n",
    "    'A*BC->B': '''\n",
    "V * L M -> L\n",
    "V * Q L -> Q\n",
    "X M * C -> C\n",
    "G * W Q -> W\n",
    "D L * J -> J\n",
    "* F H L -> F\n",
    "* X L A -> X\n",
    "Z U * E -> E''', # failed\n",
    "    'set diff2': '''\n",
    "Z Y, y -> z\n",
    "K B, b -> k\n",
    "N E, e -> n\n",
    "J S, j -> s\n",
    "O W, o -> w\n",
    "F R, f -> r\n",
    "J S, s -> j\n",
    "N O, o -> n\n",
    "P R, p -> r''',\n",
    "    'tmp': '''\n",
    "F G D H, G -> D\n",
    "C Y U K, C -> Y\n",
    "W E I H, E -> I\n",
    "R T U Q, U -> Q\n",
    "K U H O, U -> H\n",
    "F C P E, P -> E\n",
    "N H J Q, N -> H\n",
    "Z X S I, X -> S\n",
    "U T R L, R -> L\n",
    "J W G L, J -> W\n",
    "O Z Y M, O -> Z''',\n",
    "    'a b c -> A':'''\n",
    "o t j -> O\n",
    "r n k -> R\n",
    "n m c -> N\n",
    "m g d -> M\n",
    "g c j -> G\n",
    "x z o -> X\n",
    "i c p -> I\n",
    "u a o -> U''',\n",
    "     'a b a -> B': '''\n",
    "n s n -> S\n",
    "n k n -> K\n",
    "o m o -> M\n",
    "f b f -> B\n",
    "m a m -> A\n",
    "t v t -> V''',\n",
    "    'a b a -> b': '''\n",
    "n s n -> s\n",
    "n k n -> k\n",
    "o m o -> m\n",
    "f b f -> b\n",
    "m a m -> a\n",
    "t v t -> v''',\n",
    "    'Q N J P, Q J J P -> J':'''\n",
    "B V I F, B V I W -> W\n",
    "J C B T, J H B T -> H\n",
    "Q I M G, Q M M G -> M\n",
    "J D Q K, J U Q K -> U\n",
    "J E E J, J E V J -> V\n",
    "V T H V, V T S V -> S\n",
    "A H Z G, A H O G -> O''',\n",
    "    'N J , J J  -> J':'''\n",
    "I F, I W -> W\n",
    "C B, H B -> H\n",
    "A G, M G -> M\n",
    "J D, J U -> U\n",
    "M Q, K Q -> K\n",
    "E C, L C -> L''', \n",
    "    'Q N J , Q J J -> J':'''\n",
    "Q N J, Q J J -> J\n",
    "V I F, V I W -> W\n",
    "J C B, J H B -> H\n",
    "Q I M, Q M M -> M\n",
    "D Q K, U Q K -> U\n",
    "Q M A, Q Q A -> Q\n",
    "J E E, J E V -> V\n",
    "H Z G, H O G -> O''',\n",
    "    'G L C, G L -> C':'''\n",
    "G L C, G L -> C\n",
    "Y P J, P Y -> J\n",
    "E S A, A S -> E\n",
    "U P W, P U -> W\n",
    "Z Q J, Z J -> Q\n",
    "C K Z, Z C -> K\n",
    "B L M, L M -> B''',\n",
    "    'Z Y, y -> z':'''\n",
    "Z Y, y -> z\n",
    "K B, b -> k\n",
    "N E, e -> n\n",
    "J S, j -> s\n",
    "O W, o -> w\n",
    "F R, f -> r\n",
    "J S, s -> j\n",
    "N O, o -> n\n",
    "P R, p -> r''',\n",
    "    'b 1 g t -> 1':'''\n",
    "b 1 g t -> 1\n",
    "v p 3 y -> 3\n",
    "u 2 a h -> 2\n",
    "m d j 5 -> 5\n",
    "t o s 6 -> 6\n",
    "9 v i q -> 9\n",
    "m 5 p w -> 5''',\n",
    "    'b k d e -> k':'''\n",
    "a 1 c d -> 1\n",
    "b k d e -> k\n",
    "d e q g -> q\n",
    "f g h p -> p\n",
    "o p t r -> t\n",
    "h y j k -> y''',\n",
    "     'a b b b -> a':'''\n",
    "n d d d -> n\n",
    "f f d f -> d\n",
    "e b e e -> b\n",
    "s q s s -> q\n",
    "d d d o -> o\n",
    "e c e e -> c''',\n",
    "    'find lowercase2':'''\n",
    "Q j J P -> j\n",
    "B V I w -> w\n",
    "J h B T -> h\n",
    "F M m G -> m\n",
    "J U Q k -> k\n",
    "V I w O -> w\n",
    "n U Q K -> n''',#第一个和最后一个变都failed\n",
    "     'find lowercase3':'''\n",
    "j J P -> j\n",
    "V I w -> w\n",
    "h B T -> h\n",
    "M m G -> m\n",
    "u Q K -> u\n",
    "c X Q -> c\n",
    "J Y d -> d\n",
    "A k G -> k\n",
    "v E H -> v\n",
    "F c G -> c''',\n",
    "    '1 1 3 1 -> 3':'''\n",
    "1 3 1 -> 3\n",
    "5 4 4 -> 5\n",
    "6 6 2 -> 2\n",
    "2 9 2 -> 9\n",
    "7 7 1 -> 1\n",
    "3 5 5 -> 3\n",
    "8 4 8 -> 4''',# failed\n",
    "     'b 3 1 -> b':'''\n",
    "b 3 1 -> b\n",
    "5 c 4 -> c\n",
    "6 h 2 -> h\n",
    "2 9 d -> d\n",
    "q 7 1 -> q\n",
    "3 v 5 -> v\n",
    "a 4 8 -> a''',\n",
    "    'j 5 p -> 5':'''\n",
    "j 5 p -> 5\n",
    "1 i w -> 1\n",
    "h 2 t -> 2\n",
    "m m 6 -> 6\n",
    "u q 9 -> 9\n",
    "c 4 f -> 4\n",
    "7 y d -> 7''',\n",
    "    'a b b b -> a2':'''\n",
    "a b b b -> a\n",
    "g g d g -> d\n",
    "i i i w -> w\n",
    "s q s s -> q\n",
    "h h h o -> o\n",
    "q e q q -> e\n",
    "d d c d -> c''',\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = {\n",
    "     'A B C -> A1':'''\n",
    "M T L -> M\n",
    "X I J -> X\n",
    "T D U -> T\n",
    "K L H -> K\n",
    "L C V -> L\n",
    "J Y D -> J\n",
    "A K G -> A\n",
    "V E H -> V''',\n",
    "    'A B C -> A2':'''\n",
    "I W D -> I\n",
    "L N A -> L\n",
    "Z B E -> Z\n",
    "F L O -> F\n",
    "Y F L -> Y\n",
    "T X J -> T\n",
    "Z T J -> Z\n",
    "N E W -> N''',\n",
    "    'A B C -> A3':'''\n",
    "N H O -> N\n",
    "P K X -> P\n",
    "E F B -> E\n",
    "H B U -> H\n",
    "Z H P -> Z\n",
    "D Y S -> D\n",
    "F V E -> F\n",
    "E P H -> E''',\n",
    "    'A B C -> A4':'''\n",
    "U Y L -> U\n",
    "A M S -> A\n",
    "C G D -> C\n",
    "H M P -> H\n",
    "U V K -> U\n",
    "R U P -> R\n",
    "D L M -> D\n",
    "J W U -> J''',\n",
    "    'A B C -> A5':'''\n",
    "D W P -> D\n",
    "Y Z D -> Y\n",
    "T I C -> T\n",
    "L U Q -> L\n",
    "R V F -> R\n",
    "S J K -> S\n",
    "U K R -> U\n",
    "Q B H -> Q''',\n",
    "    'A B C -> A6':'''\n",
    "B X M -> B\n",
    "Q D G -> Q\n",
    "M I V -> M\n",
    "Q R Y -> Q\n",
    "B C J -> B\n",
    "L N U -> L\n",
    "G H T -> G\n",
    "C O S -> C''',\n",
    "     'A B C -> A7':'''\n",
    "N H J -> N\n",
    "O F Z -> O\n",
    "H I V -> H\n",
    "U J C -> U\n",
    "Q R S -> Q\n",
    "C H S -> C\n",
    "I S U -> I\n",
    "A Q V -> A''',\n",
    "        'A B C -> A8':'''\n",
    "Y Z G -> Y\n",
    "H L C -> H\n",
    "T W K -> T\n",
    "C J P -> C\n",
    "E P M -> E\n",
    "X I K -> X\n",
    "R P U -> T\n",
    "P Q T -> P''',\n",
    "    'A B C -> A9':'''\n",
    "W D A -> W\n",
    "G C K -> G\n",
    "M V Y -> M\n",
    "Z G O -> Z\n",
    "V T L -> V\n",
    "L U A -> L\n",
    "O T Z -> O\n",
    "X V C -> X''',\n",
    "    'A B C -> A10':'''\n",
    "M O N -> M\n",
    "X P N -> X\n",
    "G K J -> G\n",
    "W O T -> W\n",
    "N W H -> N\n",
    "X P L -> X\n",
    "J Z A -> J\n",
    "P T C -> P''',\n",
    "     'A B C -> A1ebb':'''\n",
    "M T L -> M\n",
    "X I J -> X\n",
    "T D U -> T\n",
    "K L H -> K\n",
    "L C V -> L''',\n",
    "    'A B C -> A2ebb':'''\n",
    "I W D -> I\n",
    "L N A -> L\n",
    "Z B E -> Z\n",
    "F L O -> F\n",
    "Y F L -> Y''',\n",
    "    'A B C -> A3ebb':'''\n",
    "N H O -> N\n",
    "P K X -> P\n",
    "E F B -> E\n",
    "H B U -> H\n",
    "Z H P -> Z''',\n",
    "    'A B C -> A4ebb':'''\n",
    "U Y L -> U\n",
    "A M S -> A\n",
    "C G D -> C\n",
    "H M P -> H\n",
    "U V K -> U''',\n",
    "    'A B C -> A5ebb':'''\n",
    "D W P -> D\n",
    "Y Z D -> Y\n",
    "T I C -> T\n",
    "L U Q -> L\n",
    "R V F -> R''',\n",
    "    'A B C -> A6ebb':'''\n",
    "B X M -> B\n",
    "Q D G -> Q\n",
    "M I V -> M\n",
    "Q R Y -> Q\n",
    "B C J -> B''',\n",
    "    'A B C -> A7ebb':'''\n",
    "N H J -> N\n",
    "O F Z -> O\n",
    "H I V -> H\n",
    "U J C -> U\n",
    "Q R S -> Q''',\n",
    "    'A B C -> A8ebb':'''\n",
    "Y Z G -> Y\n",
    "H L C -> H\n",
    "T W K -> T\n",
    "C J P -> C\n",
    "E P M -> E''',\n",
    "    'A B C -> A9ebb':'''\n",
    "W D A -> W\n",
    "G C K -> G\n",
    "M V Y -> M\n",
    "Z G O -> Z\n",
    "V T L -> V''',\n",
    "    'A B C -> A10ebb':'''\n",
    "M O N -> M\n",
    "X P N -> X\n",
    "G K J -> G\n",
    "W O T -> W\n",
    "N W H -> N''',\n",
    "    'A B C -> A1add':'''\n",
    "M T L -> M\n",
    "X I J -> X\n",
    "T D U -> T\n",
    "K L H -> K\n",
    "L C V -> L\n",
    "J Y D -> J\n",
    "A K G -> A\n",
    "V E H -> V\n",
    "Q G V -> Q''',\n",
    "    'A B C -> A2add':'''\n",
    "I W D -> I\n",
    "L N A -> L\n",
    "Z B E -> Z\n",
    "F L O -> F\n",
    "Y F L -> Y\n",
    "T X J -> T\n",
    "Z T J -> Z\n",
    "N E W -> N\n",
    "X B F -> X''',\n",
    "    'A B C -> A3add':'''\n",
    "N H O -> N\n",
    "P K X -> P\n",
    "E F B -> E\n",
    "H B U -> H\n",
    "Z H P -> Z\n",
    "D Y S -> D\n",
    "F V E -> F\n",
    "E P H -> E\n",
    "I F Q -> I''',\n",
    "    'A B C -> A4add':'''\n",
    "U Y L -> U\n",
    "A M S -> A\n",
    "C G D -> C\n",
    "H M P -> H\n",
    "U V K -> U\n",
    "R U P -> R\n",
    "D L M -> D\n",
    "J W U -> J\n",
    "G R X -> G''',\n",
    "    'A B C -> A5add':'''\n",
    "D W P -> D\n",
    "Y Z D -> Y\n",
    "T I C -> T\n",
    "L U Q -> L\n",
    "R V F -> R\n",
    "S J K -> S\n",
    "U K R -> U\n",
    "Q B H -> Q\n",
    "N A D -> N''',\n",
    "    'A B C -> A6add':'''\n",
    "B X M -> B\n",
    "Q D G -> Q\n",
    "M I V -> M\n",
    "Q R Y -> Q\n",
    "B C J -> B\n",
    "L N U -> L\n",
    "G H T -> G\n",
    "C O S -> C\n",
    "L J E -> L''',\n",
    "     'A B C -> A7add':'''\n",
    "N H J -> N\n",
    "O F Z -> O\n",
    "H I V -> H\n",
    "U J C -> U\n",
    "Q R S -> Q\n",
    "C H S -> C\n",
    "I S U -> I\n",
    "A Q V -> A\n",
    "Y P A -> Y''',\n",
    "        'A B C -> A8add':'''\n",
    "Y Z G -> Y\n",
    "H L C -> H\n",
    "T W K -> T\n",
    "C J P -> C\n",
    "E P M -> E\n",
    "X I K -> X\n",
    "R P U -> T\n",
    "P Q T -> P\n",
    "M N F -> M''',\n",
    "    'A B C -> A9add':'''\n",
    "W D A -> W\n",
    "G C K -> G\n",
    "M V Y -> M\n",
    "Z G O -> Z\n",
    "V T L -> V\n",
    "L U A -> L\n",
    "O T Z -> O\n",
    "X V C -> X\n",
    "C J U -> C''',\n",
    "    'A B C -> A10add':'''\n",
    "M O N -> M\n",
    "X P N -> X\n",
    "G K J -> G\n",
    "W O T -> W\n",
    "N W H -> N\n",
    "X P L -> X\n",
    "J Z A -> J\n",
    "P T C -> P\n",
    "K E D -> K''',\n",
    "    'A B C -> B1':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E''',\n",
    "    'A B C -> B2':'''\n",
    "I W D -> W\n",
    "L N A -> N\n",
    "Z B E -> B\n",
    "F L O -> L\n",
    "Y F L -> F\n",
    "T X J -> X\n",
    "Z T J -> Z\n",
    "N E W -> E''',\n",
    "    'A B C -> B3':'''\n",
    "N H O -> H\n",
    "P K X -> K\n",
    "E F B -> F\n",
    "H B U -> B\n",
    "Z H P -> H\n",
    "D Y S -> Y\n",
    "F V E -> V\n",
    "E P H -> P''',\n",
    "    'A B C -> B4':'''\n",
    "U Y L -> Y\n",
    "A M S -> M\n",
    "C G D -> G\n",
    "H M P -> M\n",
    "U V K -> V\n",
    "R U P -> U\n",
    "D L M -> L\n",
    "J W U -> W''',\n",
    "    'A B C -> B5':'''\n",
    "D W P -> W\n",
    "Y Z D -> Z\n",
    "T I C -> I\n",
    "L U Q -> U\n",
    "R V F -> V\n",
    "S J K -> J\n",
    "U K R -> K\n",
    "Q B H -> B''',\n",
    "     'A B C -> B6':'''\n",
    "B X M -> X\n",
    "Q D G -> D\n",
    "M I V -> I\n",
    "Q R Y -> R\n",
    "B C J -> C\n",
    "L N U -> N\n",
    "G H T -> H\n",
    "C O S -> O''',\n",
    "     'A B C -> B7':'''\n",
    "N H J -> H\n",
    "O F Z -> F\n",
    "H I V -> I\n",
    "U J C -> J\n",
    "Q R S -> R\n",
    "C H S -> H\n",
    "I S U -> S\n",
    "A Q V -> Q''',\n",
    "    'A B C -> B8':'''\n",
    "Y Z G -> Z\n",
    "H L C -> L\n",
    "T W K -> W\n",
    "C J P -> J\n",
    "E P M -> P\n",
    "X I K -> I\n",
    "R P U -> P\n",
    "P Q T -> Q''',\n",
    "    'A B C -> B9':'''\n",
    "W D A -> D\n",
    "G C K -> C\n",
    "M V Y -> V\n",
    "Z G O -> G\n",
    "V T L -> T\n",
    "L U A -> U\n",
    "O T Z -> T\n",
    "X V C -> V''',\n",
    "    'A B C -> B10':'''\n",
    "M O N -> O\n",
    "X P N -> P\n",
    "G K J -> K\n",
    "W O T -> O\n",
    "N W H -> W\n",
    "X P L -> P\n",
    "J Z A -> Z\n",
    "P T C -> T''',\n",
    "     'A B C -> B1ebb':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C''',\n",
    "    'A B C -> B2ebb':'''\n",
    "I W D -> W\n",
    "L N A -> N\n",
    "Z B E -> B\n",
    "F L O -> L\n",
    "Y F L -> F''',\n",
    "    'A B C -> B3ebb':'''\n",
    "N H O -> H\n",
    "P K X -> K\n",
    "E F B -> F\n",
    "H B U -> B\n",
    "Z H P -> H''',\n",
    "    'A B C -> B4ebb':'''\n",
    "U Y L -> Y\n",
    "A M S -> M\n",
    "C G D -> G\n",
    "H M P -> M\n",
    "U V K -> V''',\n",
    "    'A B C -> B5ebb':'''\n",
    "D W P -> W\n",
    "Y Z D -> Z\n",
    "T I C -> I\n",
    "L U Q -> U\n",
    "R V F -> V''',\n",
    "    'A B C -> B6ebb':'''\n",
    "B X M -> X\n",
    "Q D G -> D\n",
    "M I V -> I\n",
    "Q R Y -> R\n",
    "B C J -> C''',\n",
    "    'A B C -> B7ebb':'''\n",
    "N H J -> H\n",
    "O F Z -> F\n",
    "H I V -> I\n",
    "U J C -> J\n",
    "Q R S -> R''',\n",
    "    'A B C -> B8ebb':'''\n",
    "Y Z G -> Z\n",
    "H L C -> L\n",
    "T W K -> W\n",
    "C J P -> J\n",
    "E P M -> P''',\n",
    "    'A B C -> B9ebb':'''\n",
    "W D A -> D\n",
    "G C K -> C\n",
    "M V Y -> V\n",
    "Z G O -> G\n",
    "V T L -> T''',\n",
    "    'A B C -> B10ebb':'''\n",
    "M O N -> O\n",
    "X P N -> P\n",
    "G K J -> K\n",
    "W O T -> O\n",
    "N W H -> W''',\n",
    "    'A B C -> B1add':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E\n",
    "Q G V -> G''',\n",
    "    'A B C -> B2add':'''\n",
    "I W D -> W\n",
    "L N A -> N\n",
    "Z B E -> B\n",
    "F L O -> L\n",
    "Y F L -> F\n",
    "T X J -> X\n",
    "Z T J -> Z\n",
    "N E W -> E\n",
    "X B F -> B''',\n",
    "    'A B C -> B3add':'''\n",
    "N H O -> H\n",
    "P K X -> K\n",
    "E F B -> F\n",
    "H B U -> B\n",
    "Z H P -> H\n",
    "D Y S -> Y\n",
    "F V E -> V\n",
    "E P H -> P\n",
    "I F Q -> F''',\n",
    "    'A B C -> B4add':'''\n",
    "U Y L -> Y\n",
    "A M S -> M\n",
    "C G D -> G\n",
    "H M P -> M\n",
    "U V K -> V\n",
    "R U P -> U\n",
    "D L M -> L\n",
    "J W U -> W\n",
    "G R X -> R''',\n",
    "    'A B C -> B5add':'''\n",
    "D W P -> W\n",
    "Y Z D -> Z\n",
    "T I C -> I\n",
    "L U Q -> U\n",
    "R V F -> V\n",
    "S J K -> J\n",
    "U K R -> K\n",
    "Q B H -> B\n",
    "N A D -> A''',\n",
    "    'A B C -> B6add':'''\n",
    "B X M -> X\n",
    "Q D G -> D\n",
    "M I V -> I\n",
    "Q R Y -> R\n",
    "B C J -> C\n",
    "L N U -> N\n",
    "G H T -> H\n",
    "C O S -> O\n",
    "L J E -> J''',\n",
    "     'A B C -> B7add':'''\n",
    "N H J -> H\n",
    "O F Z -> F\n",
    "H I V -> I\n",
    "U J C -> J\n",
    "Q R S -> R\n",
    "C H S -> H\n",
    "I S U -> S\n",
    "A Q V -> Q\n",
    "Y P A -> P''',\n",
    "    'A B C -> B8add':'''\n",
    "Y Z G -> Z\n",
    "H L C -> L\n",
    "T W K -> W\n",
    "C J P -> J\n",
    "E P M -> P\n",
    "X I K -> I\n",
    "R P U -> P\n",
    "P Q T -> Q\n",
    "M N F -> N''',\n",
    "    'A B C -> B9add':'''\n",
    "W D A -> D\n",
    "G C K -> C\n",
    "M V Y -> V\n",
    "Z G O -> G\n",
    "V T L -> T\n",
    "L U A -> U\n",
    "O T Z -> T\n",
    "X V C -> V\n",
    "C J U -> J''',\n",
    "    'A B C -> B10add':'''\n",
    "M O N -> O\n",
    "X P N -> P\n",
    "G K J -> K\n",
    "W O T -> O\n",
    "N W H -> W\n",
    "X P L -> P\n",
    "J Z A -> Z\n",
    "P T C -> T\n",
    "K E D -> E''',\n",
    "    'A B A -> B': '''\n",
    "N S N -> S\n",
    "N K N -> K\n",
    "O M O -> M\n",
    "T V T -> V''',\n",
    "    'A B C -> C1': '''\n",
    "M T L -> L\n",
    "X I J -> J\n",
    "T D U -> U\n",
    "K L H -> H\n",
    "L C V -> V\n",
    "J Y D -> D\n",
    "A K G -> G\n",
    "V E H -> H''',\n",
    "    'A B C -> C2': '''\n",
    "I W D -> D\n",
    "L N A -> A\n",
    "Z B E -> E\n",
    "F L O -> O\n",
    "Y F L -> L\n",
    "T X J -> J\n",
    "Z T J -> J\n",
    "N E W -> W''',\n",
    "     'A B C -> C3':'''\n",
    "N H O -> O\n",
    "P K X -> X\n",
    "E F B -> B\n",
    "H B U -> U\n",
    "Z H P -> P\n",
    "D Y S -> S\n",
    "F V E -> E\n",
    "E P H -> H''',\n",
    "    'A B C -> C4':'''\n",
    "U Y L -> L\n",
    "A M S -> S\n",
    "C G D -> D\n",
    "H M P -> P\n",
    "U V K -> K\n",
    "R U P -> P\n",
    "D L M -> M\n",
    "J W U -> U''',\n",
    "    'A B C -> C5':'''\n",
    "D W P -> P\n",
    "Y Z D -> D\n",
    "T I C -> C\n",
    "L U Q -> Q\n",
    "R V F -> F\n",
    "S J K -> K\n",
    "U K R -> R\n",
    "Q B H -> H''',\n",
    "     'A B C -> C6':'''\n",
    "B X M -> M\n",
    "Q D G -> G\n",
    "M I V -> V\n",
    "Q R Y -> Y\n",
    "B C J -> J\n",
    "L N U -> N\n",
    "G H T -> T\n",
    "C O S -> S''',\n",
    "     'A B C -> C7':'''\n",
    "N H J -> J\n",
    "O F Z -> Z\n",
    "H I V -> V\n",
    "U J C -> C\n",
    "Q R S -> S\n",
    "C H S -> S\n",
    "I S U -> U\n",
    "A Q V -> V''',\n",
    "    'A B C -> C8':'''\n",
    "Y Z G -> G\n",
    "H L C -> C\n",
    "T W K -> K\n",
    "C J P -> P\n",
    "E P M -> M\n",
    "X I K -> K\n",
    "R P U -> U\n",
    "P Q T -> T''',\n",
    "    'A B C -> C9':'''\n",
    "W D A -> A\n",
    "G C K -> K\n",
    "M V Y -> Y\n",
    "Z G O -> O\n",
    "V T L -> L\n",
    "L U A -> A\n",
    "O T Z -> Z\n",
    "X V C -> C''',\n",
    "    'A B C -> C10':'''\n",
    "M O N -> N\n",
    "X P N -> N\n",
    "G K J -> J\n",
    "W O T -> T\n",
    "N W H -> H\n",
    "X P L -> L\n",
    "J Z A -> A\n",
    "P T C -> C''',\n",
    "     'A B C -> C1ebb':'''\n",
    "M T L -> L\n",
    "X I J -> J\n",
    "T D U -> U\n",
    "K L H -> H\n",
    "L C V -> V''',\n",
    "    'A B C -> C2ebb':'''\n",
    "I W D -> D\n",
    "L N A -> A\n",
    "Z B E -> E\n",
    "F L O -> O\n",
    "Y F L -> L''',\n",
    "    'A B C -> C3ebb':'''\n",
    "N H O -> O\n",
    "P K X -> X\n",
    "E F B -> B\n",
    "H B U -> U\n",
    "Z H P -> P''',\n",
    "    'A B C -> C4ebb':'''\n",
    "U Y L -> L\n",
    "A M S -> S\n",
    "C G D -> D\n",
    "H M P -> P\n",
    "U V K -> K''',\n",
    "    'A B C -> C5ebb':'''\n",
    "D W P -> P\n",
    "Y Z D -> D\n",
    "T I C -> C\n",
    "L U Q -> Q\n",
    "R V F -> F''',\n",
    "    'A B C -> C6ebb':'''\n",
    "B X M -> M\n",
    "Q D G -> G\n",
    "M I V -> V\n",
    "Q R Y -> Y\n",
    "B C J -> J''',\n",
    "    'A B C -> C7ebb':'''\n",
    "N H J -> J\n",
    "O F Z -> Z\n",
    "H I V -> V\n",
    "U J C -> C\n",
    "Q R S -> S''',\n",
    "    'A B C -> C8ebb':'''\n",
    "Y Z G -> G\n",
    "H L C -> C\n",
    "T W K -> K\n",
    "C J P -> P\n",
    "E P M -> M''',\n",
    "    'A B C -> C9ebb':'''\n",
    "W D A -> A\n",
    "G C K -> K\n",
    "M V Y -> Y\n",
    "Z G O -> O\n",
    "V T L -> L''',\n",
    "    'A B C -> C10ebb':'''\n",
    "M O N -> N\n",
    "X P N -> N\n",
    "G K J -> J\n",
    "W O T -> T\n",
    "N W H -> H''',\n",
    "    'A B C -> C1add':'''\n",
    "M T L -> L\n",
    "X I J -> J\n",
    "T D U -> U\n",
    "K L H -> H\n",
    "L C V -> V\n",
    "J Y D -> D\n",
    "A K G -> G\n",
    "V E H -> H\n",
    "Q G V -> V''',\n",
    "    'A B C -> C2add':'''\n",
    "I W D -> D\n",
    "L N A -> A\n",
    "Z B E -> E\n",
    "F L O -> O\n",
    "Y F L -> L\n",
    "T X J -> J\n",
    "Z T J -> J\n",
    "N E W -> W\n",
    "X B F -> F''',\n",
    "    'A B C -> C3add':'''\n",
    "N H O -> O\n",
    "P K X -> X\n",
    "E F B -> B\n",
    "H B U -> U\n",
    "Z H P -> P\n",
    "D Y S -> S\n",
    "F V E -> E\n",
    "E P H -> H\n",
    "I F Q -> Q''',\n",
    "    'A B C -> C4add':'''\n",
    "U Y L -> L\n",
    "A M S -> S\n",
    "C G D -> D\n",
    "H M P -> P\n",
    "U V K -> K\n",
    "R U P -> P\n",
    "D L M -> M\n",
    "J W U -> U\n",
    "G R X -> X''',\n",
    "    'A B C -> C5add':'''\n",
    "D W P -> P\n",
    "Y Z D -> D\n",
    "T I C -> C\n",
    "L U Q -> Q\n",
    "R V F -> F\n",
    "S J K -> K\n",
    "U K R -> R\n",
    "Q B H -> H\n",
    "N A D -> D''',\n",
    "    'A B C -> C6add':'''\n",
    "B X M -> M\n",
    "Q D G -> G\n",
    "M I V -> V\n",
    "Q R Y -> Y\n",
    "B C J -> J\n",
    "L N U -> N\n",
    "G H T -> T\n",
    "C O S -> S\n",
    "L J E -> E''',\n",
    "     'A B C -> C7add':'''\n",
    "N H J -> J\n",
    "O F Z -> Z\n",
    "H I V -> V\n",
    "U J C -> C\n",
    "Q R S -> S\n",
    "C H S -> S\n",
    "I S U -> U\n",
    "A Q V -> V\n",
    "Y P A -> A''',\n",
    "    'A B C -> B8add':'''\n",
    "Y Z G -> Z\n",
    "H L C -> L\n",
    "T W K -> W\n",
    "C J P -> J\n",
    "E P M -> P\n",
    "X I K -> I\n",
    "R P U -> P\n",
    "P Q T -> Q\n",
    "M N F -> N''',\n",
    "    'A B C -> C9add':'''\n",
    "W D A -> A\n",
    "G C K -> K\n",
    "M V Y -> Y\n",
    "Z G O -> O\n",
    "V T L -> L\n",
    "L U A -> A\n",
    "O T Z -> Z\n",
    "X V C -> C\n",
    "C J U -> U''',\n",
    "    'A B C -> B10add':'''\n",
    "M O N -> N\n",
    "X P N -> N\n",
    "G K J -> J\n",
    "W O T -> T\n",
    "N W H -> H\n",
    "X P L -> L\n",
    "J Z A -> A\n",
    "P T C -> C\n",
    "K E D -> D''',\n",
    "    'A B C -> a1':'''\n",
    "M T L -> m\n",
    "X I J -> x\n",
    "T D U -> t\n",
    "K L H -> k\n",
    "L C V -> l\n",
    "J Y D -> j\n",
    "A K G -> a\n",
    "V E H -> v''',\n",
    "    'A B C -> a2':'''\n",
    "I W D -> i\n",
    "L N A -> l\n",
    "Z B E -> z\n",
    "F L O -> f\n",
    "Y F L -> y\n",
    "T X J -> t\n",
    "Z T J -> z\n",
    "N E W -> n''',\n",
    "    'A B C -> a3':'''\n",
    "N H O -> n\n",
    "P K X -> p\n",
    "E F B -> e\n",
    "H B U -> h\n",
    "Z H P -> z\n",
    "D Y S -> d\n",
    "F V E -> f\n",
    "E P H -> e''',\n",
    "    'A B C -> a4':'''\n",
    "U Y L -> u\n",
    "A M S -> a\n",
    "C G D -> c\n",
    "H M P -> h\n",
    "U V K -> u\n",
    "R U P -> r\n",
    "D L M -> d\n",
    "J W U -> j''',\n",
    "    'A B C -> a5':'''\n",
    "D W P -> d\n",
    "Y Z D -> y\n",
    "T I C -> t\n",
    "L U Q -> l\n",
    "R V F -> r\n",
    "S J K -> s\n",
    "U K R -> u\n",
    "Q B H -> q''',\n",
    "    'A B C -> a6':'''\n",
    "B X M -> b\n",
    "Q D G -> q\n",
    "M I V -> m\n",
    "Q R Y -> q\n",
    "B C J -> b\n",
    "L N U -> l\n",
    "G H T -> g\n",
    "C O S -> c''',\n",
    "     'A B C -> a7':'''\n",
    "N H J -> n\n",
    "O F Z -> o\n",
    "H I V -> h\n",
    "U J C -> u\n",
    "Q R S -> q\n",
    "C H S -> c\n",
    "I S U -> i\n",
    "A Q V -> a''',\n",
    "    'A B C -> a8':'''\n",
    "Y Z G -> y\n",
    "H L C -> h\n",
    "T W K -> t\n",
    "C J P -> c\n",
    "E P M -> e\n",
    "X I K -> x\n",
    "R P U -> t\n",
    "P Q T -> p''',\n",
    "    'A B C -> a9':'''\n",
    "W D A -> w\n",
    "G C K -> g\n",
    "M V Y -> m\n",
    "Z G O -> z\n",
    "V T L -> v\n",
    "L U A -> l\n",
    "O T Z -> o\n",
    "X V C -> x''',\n",
    "    'A B C -> a10':'''\n",
    "M O N -> m\n",
    "X P N -> x\n",
    "G K J -> g\n",
    "W O T -> w\n",
    "N W H -> n\n",
    "X P L -> x\n",
    "J Z A -> j\n",
    "P T C -> p''',\n",
    "     'A B C -> Aa1ebb':'''\n",
    "M T L -> m\n",
    "X I J -> x\n",
    "T D U -> t\n",
    "K L H -> k\n",
    "L C V -> l''',\n",
    "    'A B C -> a2ebb':'''\n",
    "I W D -> i\n",
    "L N A -> l\n",
    "Z B E -> z\n",
    "F L O -> f\n",
    "Y F L -> y''',\n",
    "    'A B C -> a3ebb':'''\n",
    "N H O -> n\n",
    "P K X -> p\n",
    "E F B -> e\n",
    "H B U -> h\n",
    "Z H P -> z''',\n",
    "    'A B C -> a4ebb':'''\n",
    "U Y L -> u\n",
    "A M S -> a\n",
    "C G D -> c\n",
    "H M P -> h\n",
    "U V K -> u''',\n",
    "    'A B C -> a5ebb':'''\n",
    "D W P -> d\n",
    "Y Z D -> y\n",
    "T I C -> t\n",
    "L U Q -> l\n",
    "R V F -> r''',\n",
    "    'A B C -> a6ebb':'''\n",
    "B X M -> b\n",
    "Q D G -> q\n",
    "M I V -> m\n",
    "Q R Y -> q\n",
    "B C J -> b''',\n",
    "    'A B C -> a7ebb':'''\n",
    "N H J -> n\n",
    "O F Z -> o\n",
    "H I V -> h\n",
    "U J C -> u\n",
    "Q R S -> q''',\n",
    "    'A B C -> a8ebb':'''\n",
    "Y Z G -> y\n",
    "H L C -> h\n",
    "T W K -> t\n",
    "C J P -> c\n",
    "E P M -> e''',\n",
    "    'A B C -> a9ebb':'''\n",
    "W D A -> w\n",
    "G C K -> g\n",
    "M V Y -> m\n",
    "Z G O -> z\n",
    "V T L -> v''',\n",
    "    'A B C -> a10ebb':'''\n",
    "M O N -> m\n",
    "X P N -> x\n",
    "G K J -> g\n",
    "W O T -> w\n",
    "N W H -> n''',\n",
    "    'A B C -> a1add':'''\n",
    "M T L -> m\n",
    "X I J -> x\n",
    "T D U -> t\n",
    "K L H -> k\n",
    "L C V -> l\n",
    "J Y D -> j\n",
    "A K G -> a\n",
    "V E H -> v\n",
    "Q G V -> q''',\n",
    "    'A B C -> a2add':'''\n",
    "I W D -> i\n",
    "L N A -> l\n",
    "Z B E -> z\n",
    "F L O -> f\n",
    "Y F L -> y\n",
    "T X J -> t\n",
    "Z T J -> z\n",
    "N E W -> n\n",
    "X B F -> x''',\n",
    "    'A B C -> a3add':'''\n",
    "N H O -> n\n",
    "P K X -> p\n",
    "E F B -> e\n",
    "H B U -> h\n",
    "Z H P -> z\n",
    "D Y S -> d\n",
    "F V E -> f\n",
    "E P H -> e\n",
    "I F Q -> i''',\n",
    "    'A B C -> a4add':'''\n",
    "U Y L -> u\n",
    "A M S -> a\n",
    "C G D -> c\n",
    "H M P -> h\n",
    "U V K -> u\n",
    "R U P -> r\n",
    "D L M -> d\n",
    "J W U -> j\n",
    "G R X -> g''',\n",
    "    'A B C -> a5add':'''\n",
    "D W P -> d\n",
    "Y Z D -> y\n",
    "T I C -> t\n",
    "L U Q -> l\n",
    "R V F -> r\n",
    "S J K -> s\n",
    "U K R -> u\n",
    "Q B H -> q\n",
    "N A D -> n''',\n",
    "    'A B C -> a6add':'''\n",
    "B X M -> b\n",
    "Q D G -> q\n",
    "M I V -> m\n",
    "Q R Y -> q\n",
    "B C J -> b\n",
    "L N U -> l\n",
    "G H T -> g\n",
    "C O S -> c\n",
    "L J E -> l''',\n",
    "     'A B C -> a7add':'''\n",
    "N H J -> n\n",
    "O F Z -> o\n",
    "H I V -> h\n",
    "U J C -> u\n",
    "Q R S -> q\n",
    "C H S -> c\n",
    "I S U -> i\n",
    "A Q V -> a\n",
    "Y P A -> y''',\n",
    "    'A B C -> a8add':'''\n",
    "Y Z G -> y\n",
    "H L C -> h\n",
    "T W K -> t\n",
    "C J P -> c\n",
    "E P M -> e\n",
    "X I K -> x\n",
    "R P U -> t\n",
    "P Q T -> p\n",
    "M N F -> m''',\n",
    "    'A B C -> a9add':'''\n",
    "W D A -> w\n",
    "G C K -> g\n",
    "M V Y -> m\n",
    "Z G O -> z\n",
    "V T L -> v\n",
    "L U A -> l\n",
    "O T Z -> o\n",
    "X V C -> x\n",
    "C J U -> c''',\n",
    "    'A B C -> a10add':'''\n",
    "M O N -> m\n",
    "X P N -> x\n",
    "G K J -> g\n",
    "W O T -> w\n",
    "N W H -> n\n",
    "X P L -> x\n",
    "J Z A -> j\n",
    "P T C -> p\n",
    "K E D -> k''',\n",
    "         'A B C -> b0':'''\n",
    "T N X -> n\n",
    "D E U -> e\n",
    "U R Q -> r\n",
    "C T B -> t\n",
    "X B F -> b\n",
    "Q G V -> g''',\n",
    "    'A B C -> b':'''\n",
    "M T L -> t\n",
    "X I J -> i\n",
    "T D U -> d\n",
    "K L H -> l\n",
    "L C V -> c\n",
    "J Y D -> y\n",
    "A K G -> k\n",
    "V E H -> e''',\n",
    "    'A B C -> b2':'''\n",
    "I W D -> w\n",
    "L N A -> n\n",
    "Z B E -> b\n",
    "F L O -> l\n",
    "Y F L -> f\n",
    "T X J -> x\n",
    "Z T J -> t\n",
    "N E W -> e''',#failed\n",
    "    'A B C -> b3':'''\n",
    "B X M -> x\n",
    "Q D G -> d\n",
    "M I V -> i\n",
    "Q R Y -> r\n",
    "B C J -> c''',\n",
    "    'A B C -> b4':'''\n",
    "N H J -> h\n",
    "O F Z -> f\n",
    "H I V -> i\n",
    "U J C -> j\n",
    "Q R S -> r''',\n",
    "    'A B C -> b5':'''\n",
    "M T L -> t\n",
    "X I J -> i\n",
    "T D U -> d\n",
    "K L H -> l\n",
    "L C V -> c\n",
    "J Y D -> y\n",
    "A K G -> k\n",
    "V E H -> e\n",
    "Q G V -> g''',\n",
    "    'A B A -> b': '''\n",
    "A L A -> l\n",
    "F B F -> b\n",
    "M A M -> a\n",
    "O W O -> w\n",
    "W Y W -> y\n",
    "D G D -> g''',\n",
    "    'A B C -> c0':'''\n",
    "P Q Z -> z\n",
    "C E E -> e\n",
    "U G H -> h\n",
    "X B F -> f\n",
    "K P Y -> y\n",
    "A M A -> a\n",
    "K E T -> t''',\n",
    "     'A B C -> c':'''\n",
    "M T L -> l\n",
    "X I J -> j\n",
    "T D U -> u\n",
    "K L H -> h\n",
    "L C V -> v\n",
    "J Y D -> d\n",
    "A K G -> g\n",
    "V E H -> h''',\n",
    "    'A B C -> c2':'''\n",
    "I W D -> d\n",
    "L N A -> a\n",
    "Z B E -> e\n",
    "F L O -> o\n",
    "Y F L -> l\n",
    "T X J -> j\n",
    "Z T J -> j\n",
    "N E W -> w''',\n",
    "    'A B C -> c3':'''\n",
    "B X M -> m\n",
    "Q D G -> g\n",
    "M I V -> v\n",
    "Q R Y -> y\n",
    "B C J -> j''',\n",
    "    'A B C -> c4':'''\n",
    "N H J -> j\n",
    "O F Z -> z\n",
    "H I V -> v\n",
    "U J C -> c\n",
    "Q R S -> s''',\n",
    "    'A B C -> c5':'''\n",
    "M T L -> l\n",
    "X I J -> j\n",
    "T D U -> u\n",
    "K L H -> h\n",
    "L C V -> v\n",
    "J Y D -> d\n",
    "A K G -> g\n",
    "V E H -> h\n",
    "Q G V -> v''',\n",
    "    'digit1_1add':'''\n",
    "9 7 1 -> 9\n",
    "9 3 2 -> 9\n",
    "7 5 8 -> 7\n",
    "5 4 9 -> 5\n",
    "8 1 9 -> 8\n",
    "4 6 2 -> 4\n",
    "5 1 3 -> 5\n",
    "3 7 2 -> 3\n",
    "4 5 2 -> 4''',\n",
    "    'digit1_2add':'''\n",
    "2 7 5 -> 2\n",
    "9 8 6 -> 9\n",
    "3 9 6 -> 3\n",
    "6 5 9 -> 6\n",
    "9 1 7 -> 9\n",
    "3 5 9 -> 3\n",
    "8 6 1 -> 8\n",
    "4 1 9 -> 4\n",
    "1 6 9 -> 1''',\n",
    "     'digit1_1':'''\n",
    "9 7 1 -> 9\n",
    "9 3 2 -> 9\n",
    "7 5 8 -> 7\n",
    "5 4 9 -> 5\n",
    "8 1 9 -> 8\n",
    "4 6 2 -> 4\n",
    "5 1 3 -> 5\n",
    "3 7 2 -> 3''',\n",
    "    'digit1_2':'''\n",
    "2 7 5 -> 2\n",
    "9 8 6 -> 9\n",
    "3 9 6 -> 3\n",
    "6 5 9 -> 6\n",
    "9 1 7 -> 9\n",
    "3 5 9 -> 3\n",
    "8 6 1 -> 8\n",
    "4 1 9 -> 4''',\n",
    "    'digit1_1ebb':'''\n",
    "9 7 1 -> 9\n",
    "9 3 2 -> 9\n",
    "7 5 8 -> 7\n",
    "5 4 9 -> 5\n",
    "8 1 9 -> 8''',\n",
    "    'digit1_2':'''\n",
    "2 7 5 -> 2\n",
    "9 8 6 -> 9\n",
    "3 9 6 -> 3\n",
    "6 5 9 -> 6\n",
    "9 1 7 -> 9''',\n",
    "}\n",
    "\n",
    "figure_p ={\n",
    "     '1 2 3 -> 1':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E''',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 8 4 W -> W\n",
      "4 8 7 F -> F\n",
      "S 3 9 4 -> S\n",
      "1 K 2 6 -> K\n",
      "1 I 6 3 -> I\n",
      "8 E 2 9 -> E\n",
      "2 7 6 R -> R\n",
      "2 T 6 5 -> T\n",
      "ĠW 0.0459 {'Ċ': 0.0565, 'Ġ4': 0.0472, 'ĠW': 0.0459, 'Ġ3': 0.0414, 'Ġ1': 0.0353}\n",
      "ĠF 0.4463 {'ĠF': 0.4463, 'ĠW': 0.3036, 'ĠE': 0.0233, 'ĠG': 0.0142, 'ĠU': 0.0127}\n",
      "ĠS 0.0871 {'ĠW': 0.2034, 'ĠF': 0.1508, 'Ġ4': 0.1095, 'ĠS': 0.0871, 'ĠE': 0.0368}\n",
      "ĠK 0.3913 {'ĠK': 0.3913, 'Ġ1': 0.1142, 'ĠS': 0.071, 'ĠA': 0.0305, 'Ġ2': 0.0281}\n",
      "ĠI 0.7527 {'ĠI': 0.7527, 'ĠK': 0.0482, 'ĠS': 0.0245, 'ĠJ': 0.0194, 'ĠD': 0.0103}\n",
      "ĠE 0.8519 {'ĠE': 0.8519, 'ĠW': 0.0298, 'Ġ8': 0.0132, 'ĠS': 0.0122, 'ĠF': 0.0113}\n",
      "ĠR 0.9529 {'ĠR': 0.9529, 'ĠE': 0.0069, 'ĠS': 0.0051, 'ĠF': 0.003, 'ĠI': 0.0029}\n",
      "ĠT 0.9674 {'ĠT': 0.9674, 'ĠR': 0.0116, 'ĠS': 0.0035, 'ĠI': 0.0016, 'ĠN': 0.0016}\n"
     ]
    }
   ],
   "source": [
    "#task_name = 'find del10'\n",
    "#task_name = 'find lowercase10'\n",
    "#task_name = 'find diff_alphabet3'\n",
    "#task_name =    'A B C -> C10'\n",
    "task_name =  'find aph10'\n",
    "#task_name =  'find figure10'\n",
    "# task_name = 'set diff'\n",
    "# task_name = 'find lowercase'\n",
    "# task_name = 'ABC,AXC->X'\n",
    "# task_name = 'A*BC->B'\n",
    "# task_name = 'set diff2'\n",
    "# task_name = 'tmp'\n",
    "#text = texts[task_name]\n",
    "#text = position[task_name]\n",
    "text = figure_d[task_name]\n",
    "#text = findspecial[task_name]\n",
    "_text = text.replace('_', mask_token).rstrip()\n",
    "# print(_text.count('->') - 1)\n",
    "print(_text)\n",
    "\n",
    "if masked_lm:\n",
    "    print(_text, ['%s %.3f' % (i['token_str'], i['score']) for i in nlp(_text)])\n",
    "    print(tokenizer.tokenize(_text))\n",
    "else:\n",
    "    inputs = tokenizer.encode_plus(_text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "#     max_length = 1 + (inputs['input_ids'].size(1) if mask_token == '' else 0)\n",
    "#     with torch.no_grad(): outputs = model.generate(**inputs, max_length=max_length, top_k=1)\n",
    "#     print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "#     show_topk(*outputs.logits[0, src].softmax(-1).topk(5))\n",
    "    arrow_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "    input_ids = inputs.input_ids\n",
    "    arrow_positions = (input_ids == arrow_id).nonzero()[:,1]\n",
    "    ans_positions = arrow_positions + 1\n",
    "    ans_ids = input_ids[(input_ids == arrow_id).roll(1, dims=-1)]\n",
    "    ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "    ans_prob_dist = outputs.logits[input_ids == arrow_id].softmax(-1)\n",
    "    ans_probs = numpy(ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids])\n",
    "    src = arrow_positions[-1].item()\n",
    "    pred_label = outputs.logits[0, src].argmax().item()\n",
    "    attentions = outputs.attentions\n",
    "    attns = torch.cat([globalize(a) for a in attentions])\n",
    "\n",
    "    for ans_token, ans_prob, dist in zip(ans_tokens, ans_probs, ans_prob_dist):\n",
    "        print(ans_token, ans_prob, show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blocks = model.transformer.h\n",
    "L, H = model.config.num_layers, model.config.num_heads\n",
    "# all_top_heads, all_attns = {}, {}\n",
    "all_attrs = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据集\n",
    "# 验证的数据为，位置类案例，字母变换类案例，堆每个位置的数先保存到列表再转换为数组\n",
    "#data_p_m_12_18 = []\n",
    "#label_p_m_12_18 = []\n",
    "#data_p_12_16 = []\n",
    "#label_p_12_16 = []\n",
    "data_d_10_8 = []\n",
    "label_d_10_8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取样本数据，m组数据，每组数据为一个128维的向量,n = 128\n",
    "layer = 10\n",
    "head = 8\n",
    "src = 54#54,48,61#46,28,52 #？\n",
    "m = blocks[layer].attn.attention\n",
    "# 获取信息向量\n",
    "m.attn_out = None\n",
    "# inputs是输入的案例句子\n",
    "try: outputs = model(**inputs)\n",
    "finally: attn_out = getdelattr(m, 'attn_out')\n",
    "\n",
    "#attn_out.size()\n",
    "\n",
    "attn_out[0, head, src].size()\n",
    "#type(attn_out[0, head, src].detach().numpy())  # .detach().numpy()将tensor转化为数组ndarray\n",
    "#type(list(attn_out[0, head, src]))\n",
    "#temp = attn_out[0, head, src].detach().numpy()\n",
    "#attn_out[0, 16, src]\n",
    "#attn_out[0, 18, src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#位置数据\n",
    "temp = list(attn_out[0, head, src].detach().numpy())\n",
    "\n",
    "#data_p_m_12_18.append(temp)\n",
    "#label_p_m_12_18.append(2)\n",
    "#data_p_12_16.append(temp)\n",
    "#label_p_12_16.append(2)\n",
    "#data_d_10_8.pop()\n",
    "#label_d_10_8.pop()\n",
    "data_d_10_8.append(temp)\n",
    "label_d_10_8.append(2)\n",
    "len(data_d_10_8)\n",
    "label_d_10_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类分析\n",
    "# 数据集为m*n，m是样本个数，n是特征个数,n = 128\n",
    "# 质心用一矩阵表示，k*n的形式，k是聚类的个数，n是数据集的特征个数，也就是列的个数。n = 128\n",
    "#data.extend(position1)\n",
    "#data.extend(position2)\n",
    "#data.extend(position3)\n",
    "#data10_8.extend(position1)\n",
    "#data10_8.extend(position2)\n",
    "#data10_8.extend(position3)\n",
    "#data12_18.extend(position1)\n",
    "#data12_18.extend(position2)\n",
    "#data12_18.extend(position3)\n",
    "#data12_16.extend(position1)\n",
    "#data12_16.extend(position2)\n",
    "#data12_16.extend(position3)\n",
    "#np.save(\"label_p_m_12_18.npy\", label_p_m_12_18) # 将数据集保存到文件.\"data_p_m_10_8.npy\"-\"label_p_m_10_8.npy\".\"data__p_s_10_8.npy\"-\"label_p_s_10_8.npy\"\n",
    "#np.save(\"data_d_12_16.npy\", data_d_12_16) # data_d_10_8.npy,data_d_12_18.npy,\"data_d_12_16.npy\"\n",
    "#np.save(\"label_d_12_16.npy\", label_d_12_16) # abel_d_10_8.npy,abel_d_12_18.npy,\"label_d_12_16.npy\"\n",
    "np.save(\"data_d_10_8t.npy\", data_d_10_8) \n",
    "np.save(\"label_d_10_8t.npy\", label_d_10_8) \n",
    "#len(data12_16)\n",
    "#dataSet12_8 = np.array(data) #共59个用例，0-14为大写位置1，15-29为大写位置2，30-44为大写位置3，45-49小写位置1，50-53为小写位置2，54-58为小写位置3\n",
    "#dataSet_p_upper = np.array(data[:45])\n",
    "#dataSet10_8 = np.array(data_10_8)#共58个用例，0-14为大写位置1，15-28为大写位置2，29-43为大写位置3，44-48小写位置1，49-52为小写位置2，52-56为小写位置3\n",
    "#dataSet10_8_upper = np.array(data_10_8[:44])\n",
    "#dataSet12_18 = np.array(data12_18)\n",
    "#dataSet12_18_upper = np.array(data12_18[:44])\n",
    "#dataSet12_16 = np.array(data12_16)\n",
    "#dataSet12_16_upper = np.array(data12_16[:44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "#dataSet_m = list(np.load(\"data_p_m_10_8.npy\", allow_pickle=True))\n",
    "#tardata_m = list(np.load(\"label_p_m_10_8.npy\", allow_pickle=True))\n",
    "#dataSet_s = list(np.load(\"data__p_s_10_8.npy\", allow_pickle=True))\n",
    "#tardata_s = list(np.load(\"label_p_s_10_8.npy\", allow_pickle=True))\n",
    "#dataSet_m.extend(dataSet_s)\n",
    "#tardata_m.extend(tardata_s)\n",
    "#len(dataSet_m)\n",
    "#len(tardata_m)\n",
    "#dataSet = np.array(dataSet_m)\n",
    "#tardata = np.array(tardata_m)\n",
    "dataSet = np.load(\"data_d_10_8t.npy\", allow_pickle=True)\n",
    "tardata = np.load(\"label_d_10_8t.npy\", allow_pickle=True)\n",
    "#dataSet\n",
    "#print(len(dataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=2)\n",
    "clf = k_means.fit(dataSet)\n",
    "clf.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import mglearn\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")#消除警告\n",
    "#   初始化图形参数\n",
    "#plt.rcParams['figure.figsize'] = (16,9)#  设置大小\n",
    "#  图形美化\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# 欧氏距离计算\n",
    "def distEclud(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "# 为给定数据集构建一个包含K个随机质心的集合\n",
    "def randCent(dataSet,k):\n",
    "    # 获取样本数与特征值\n",
    "    m,n = dataSet.shape#把数据集的行数和列数赋值给m,n\n",
    "    # 初始化质心,创建(k,n)个以零填充的矩阵\n",
    "    centroids = np.zeros((k,n))\n",
    "    # 循环遍历特征值\n",
    "    for i in range(k):\n",
    "        index = int(np.random.uniform(0,m))\n",
    "        # 计算每一列的质心,并将值赋给centroids\n",
    "        centroids[i,:] = dataSet[index,:]\n",
    "        # 返回质心\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# k均值聚类\n",
    "def KMeans(dataSet,k):\n",
    "    m = np.shape(dataSet)[0]\n",
    "    # 初始化一个矩阵来存储每个点的簇分配结果\n",
    "    # clusterAssment包含两个列:一列记录簇索引值,第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果)\n",
    "    clusterAssment = np.mat(np.zeros((m,2))) #m行2列\n",
    "    clusterChange = True\n",
    "    n_iter = 0 #  最多运行迭代的次数超过300\n",
    "\n",
    "    # 创建质心,随机K个质心\n",
    "    centroids = randCent(dataSet,k)\n",
    "    # 初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代，设置一个\n",
    "    while clusterChange or n_iter < 300:\n",
    "        clusterChange = False\n",
    "        n_iter = n_iter + 1\n",
    "\n",
    "        #遍历所有样本（行数）\n",
    "        for i in range(m):\n",
    "            minDist = 100000.0\n",
    "            minIndex = -1\n",
    "            # 遍历所有数据找到距离每个点最近的质心,\n",
    "            # 可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成\n",
    "            for j in range(k):\n",
    "                # 计算数据点到质心的距离\n",
    "                # 计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud\n",
    "                distance = distEclud(centroids[j,:],dataSet[i,:])\n",
    "                # 如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引)\n",
    "                if distance < minDist:\n",
    "                    minDist = distance\n",
    "                    minIndex = j\n",
    "            # 如果任一点的簇分配结果发生改变,则更新clusterChanged标志\n",
    "            if clusterAssment[i,0] != minIndex:\n",
    "                clusterChange = True\n",
    "                # 更新簇分配结果为最小质心的index(索引),minDist(最小距离)\n",
    "                clusterAssment[i,:] = minIndex,minDist\n",
    "        # 遍历所有质心并更新它们的取值\n",
    "        for j in range(k):\n",
    "            # 通过数据过滤来获得给定簇的所有点\n",
    "            pointsInCluster = dataSet[np.nonzero(clusterAssment[:,0].A == j)[0]]\n",
    "            # 计算所有点的均值,axis=0表示沿矩阵的列方向进行均值计算\n",
    "            centroids[j,:] = np.mean(pointsInCluster,axis=0)\n",
    "    print(\"Congratulation,cluster complete!\")\n",
    "    # 返回所有的类质心与点分配结果\n",
    "    return centroids,clusterAssment\n",
    "\n",
    "\n",
    "    \n",
    "# 运行 \n",
    "# dataSet = \n",
    "#data = np.random.randint(1,20,[20,6])\n",
    "k = 3\n",
    "#质心与点分配结果,且每一行对应数据集的数据，如果不可视化，可以直接看原数据被分到的质心，如果每个都已差不多的分布，则表示并不是我们认为的信息\n",
    "centroids,clusterAssment = KMeans(dataSet,k) \n",
    "# 可以把centroids和cluster加入数据矩阵的第一二列，方便直接筛选查看\n",
    "# centroids是类质心；clusterAssment包含两个列:一列记录簇索引值,第二列存储误差\n",
    "print(centroids)\n",
    "print(clusterAssment)\n",
    "print(len(centroids),len(clusterAssment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((dataSet[-5] - dataSet[-1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((result[0] - result[1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute t-SNE Embedding...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute PCA Embedding...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# t-SNE方法\n",
    "# 对样本进行预处理并画图\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_embedding(data, label, title):\n",
    "    \"\"\"\n",
    "    :param data:数据集\n",
    "    :param label:样本标签\n",
    "    :param title:图像标题\n",
    "    :return:图像\n",
    "    \"\"\"\n",
    "    x_min, x_max = np.min(data, 0), np.max(data, 0)\n",
    "    data = (data - x_min) / (x_max - x_min)\t\t# 对数据进行归一化处理\n",
    "    fig = plt.figure()\t\t# 创建图形实例\n",
    "    ax = plt.subplot(111)\t\t# 创建子图\n",
    "    # 遍历所有样本\n",
    "    for i in range(data.shape[0]):\n",
    "        # 在图中为每个数据点画出标签\n",
    "        plt.text(data[i, 0], data[i, 1], str(label[i]), color=plt.cm.Set1(label[i] / 3),\n",
    "                 fontdict={'weight': 'bold', 'size': 11})\n",
    "    plt.xticks()\t\t# 指定坐标的刻度\n",
    "    plt.yticks()\n",
    "    plt.title(title, fontsize=14)\n",
    "    # 返回值\n",
    "    return fig\n",
    "\n",
    "print('Starting compute t-SNE Embedding...')\n",
    "# ts = TSNE(n_components=2, init='pca', random_state=0)\n",
    "ts = TSNE(n_components=2)#, init='pca', random_state=0)\n",
    "# t-SNE降维\n",
    "result = ts.fit_transform(dataSet)\n",
    "# 调用函数，绘制图像\n",
    "fig = plot_embedding(result, tardata, 't-SNE Embedding of position')\n",
    "# 显示图像\n",
    "plt.show()\n",
    "\n",
    "# PCA降维\n",
    "print('Starting compute PCA Embedding...')\n",
    "pca = PCA(n_components=2)\n",
    "result2 = pca.fit_transform(dataSet)\n",
    "fig = plot_embedding(result2, tardata, 'PCA Embedding of position')\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_attn = False\n",
    "#pred_attn = True\n",
    "layer_range = (0, layer2 if pred_attn else 22)\n",
    "attrs = []\n",
    "with torch.no_grad(): attentions = model(**inputs, output_attentions=True).attentions\n",
    "for i in tqdm(range(*layer_range)):\n",
    "    scaled_attn, step = scaled_input(attentions[i], 10)\n",
    "    _ = scaled_attn.requires_grad_(True)\n",
    "    m = get_attn_module(blocks[i])\n",
    "    m.w = scaled_attn\n",
    "    if pred_attn: blocks[layer2].exit = True\n",
    "    try: outputs = model(**inputs, output_attentions=pred_attn)\n",
    "    finally:\n",
    "        m.w = None\n",
    "        if pred_attn: blocks[layer2].exit = None\n",
    "    y = globalize(outputs.attentions[layer2])[:, head2, src, tgt] \\\n",
    "        if pred_attn else outputs.logits.softmax(-1)[:, src, pred_label]\n",
    "    \n",
    "    attn_grad = torch.autograd.grad(torch.unbind(y), scaled_attn)[0]\n",
    "    attn_grad = attn_grad.sum(dim=0, keepdim=True) # (bsz, H, qlen, klen) -> (1, H, qlen, klen)\n",
    "    attr = attn_grad * step\n",
    "    attrs.append(attr.data)\n",
    "attrs = torch.cat([globalize(a) for a in attrs])\n",
    "all_attrs[task_name][int(pred_attn)] = attrs #记录各个样例的注意力值分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.replace('Ġ', '').replace('Ċ', '^') for token in tokenizer.tokenize(_text)]\n",
    "for i, token in enumerate(tokens):\n",
    "    if token in ['Ċ', '^']: print()\n",
    "    else: print((i, token), end=' ')\n",
    "seq_len = len(tokens)\n",
    "answer = tokens[-1]\n",
    "# src = [i for i, token in enumerate(tokens) if token in ['->', 'Ġ->']][-1]\n",
    "# 用来找到答案的位置/变换的位置\n",
    "tgt = [i for i, token in enumerate(tokens[:-1]) if token.lower() == answer.lower()][-1]\n",
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_top_heads(values, indices, topk=20):\n",
    "    val, ind = values.sum(dim=-1).view(-1).topk(topk)\n",
    "    val, ind = numpy(val), unravel_index(ind, values.size()[:-1])\n",
    "    top_heads = []\n",
    "    for (l, h), v in zip(ind, val):\n",
    "        if l < 4: continue\n",
    "        top_heads.append((l, h, v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h])))))\n",
    "        print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1], round(attns[l, h, src, tgt].item(), 4))\n",
    "\n",
    "#pred_attn = False\n",
    "ki = ans_positions[:-1]\n",
    "attrs = all_attrs[task_name][int(pred_attn)]\n",
    "# attrs = attrs / attrs.view(attrs.size(0), -1).norm(dim=1)[:, None, None, None] #.view(attrs.size(0), 1, 1, 1)\n",
    "if not pred_attn:\n",
    "    _attrs = attrs[:, :, src: src + 1, tgt: tgt + 1]\n",
    "    values, indices = _attrs.view(_attrs.size(0), H, -1).topk(1, dim=-1)\n",
    "    show_top_heads(values, indices)\n",
    "    print()\n",
    "\n",
    "_attrs = attrs[:, :, src: src + 1, :]\n",
    "values, indices = _attrs.view(_attrs.size(0), H, -1).topk(5, dim=-1)\n",
    "show_top_heads(values, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer, head = 10, 8 # ABC->B, ABC->A(pred_attn<12-18), ABC->C(only pred_attn), find lowercase, set diff2(only pred_attn)\n",
    "# layer, head = 12, 18 # 12-16, 12-18, 12-1 ABC->A, ABC,AXC->X, set diff2, set diff\n",
    "layer2, head2 = 13, 5 # ABC->B, ABC->A, ABC->C, ABC,AXC->X, set diff, set diff2(<13-12), find lowercase(<15-8)\n",
    "# layer2, head2 = 15, 8  # 15-8, find lowercase\n",
    "# layer2, head2 = 13, 12  # set diff2, ABC,AXC->X(<13-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y\n",
    "# _attn_grad[:, head, src, :]\n",
    "_attn_grad[:, head, src, ki].mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = blocks[layer].attn.attention\n",
    "mask = torch.ones(H, seq_len, seq_len)\n",
    "# mask[:, src] = 0\n",
    "mask[:] = 0\n",
    "mask[head, src] = 0.1\n",
    "_ = mask.requires_grad_(True)\n",
    "# m.attn_mask = mask\n",
    "\n",
    "# scaled_attn = attentions[layer] * mask.to(model.device)\n",
    "# _ = scaled_attn.requires_grad_(True)\n",
    "# m.w = scaled_attn\n",
    "try:\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "    attn = outputs.attentions[layer]\n",
    "    _ = plt.figure(figsize=(15, 1))\n",
    "    _ = sns.heatmap(numpy(attn[:, head, src, :]), cbar=False)\n",
    "    print(attn[:, head, src, ans_positions[:-1]])\n",
    "    \n",
    "    attn[:] = 0\n",
    "#     attn[0, head, src] = 0\n",
    "    attn[0, head, src, ans_positions[:-1]] = 0.1 / len(ans_positions[:-1])\n",
    "#     i = random.randint(0, attn.size(3) - 1); i\n",
    "#     attn[0, head, src] = 0.1 / attn.size(3)\n",
    "    print(attn[0, head, src, ans_positions[:-1]])\n",
    "    m.w = attn\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "#     outputs = model(**inputs, output_attentions=True)\n",
    "finally:\n",
    "#     m.attn_mask = None\n",
    "    m.w = None\n",
    "# y = outputs.logits.softmax(-1)[:, src, pred_label]\n",
    "# mask_grad = torch.autograd.grad(torch.unbind(y), mask)[0]\n",
    "# attn_grad = torch.autograd.grad(torch.unbind(y), scaled_attn)[0]\n",
    "    \n",
    "attn = globalize(outputs.attentions[layer2])[0, head2, src]\n",
    "print(show_topk(*attn.topk(5), indices_fn=append_tokens_to_positions))\n",
    "probs = outputs.logits[0, src].softmax(-1)\n",
    "show_topk(*probs.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_grad[0, head - 2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_grad.size()\n",
    "attn_grad2.size()\n",
    "(attn_grad2[4:5] - attn_grad).abs().max()\n",
    "attn_grad[0, head, src, ki]\n",
    "attn_grad2[4, head, src, ki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "#     with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "#     attn = outputs.attentions[layer]\n",
    "#     print(attn[0, head, src, ans_positions[:-1]])\n",
    "#     attn[0, head, src] = 0\n",
    "#     attn[0, head, src, ans_positions[-8]] = 1\n",
    "#     print(attn[0, head, src, ans_positions[:-1]])\n",
    "#     m.w = attn\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "finally:\n",
    "    m.attn_mask = None\n",
    "#     m.w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[layer].h_in = None\n",
    "h[layer].attn.attention.attn_out = None\n",
    "for l in range(layer, layer2): h[l].attn_output, h[l].ffn_output = None, None\n",
    "am2 = h[layer2].attn.attention\n",
    "am2.hidden_states, am2.attention_mask = None, None\n",
    "try:\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "finally:\n",
    "    am2_hidden_states, am2_attention_mask = am2.hidden_states, am2.attention_mask\n",
    "    delattr(am2, 'hidden_states')\n",
    "    delattr(am2, 'attention_mask')\n",
    "\n",
    "hidden_states = h[layer].h_in\n",
    "for l in range(layer, layer2):\n",
    "    if l in [layer2 - 1]: continue\n",
    "    hidden_states = hidden_states + h[l].attn_output + h[l].ffn_output\n",
    "(hidden_states - h[layer2].h_in).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(tensor): return round(tensor.abs().mean().item(), 4)\n",
    "hidden_states.mean(), hidden_states.std()\n",
    "for l in range(layer, layer2):\n",
    "    print(norm(hidden_states), end=' ')\n",
    "    hidden_states = hidden_states + h[l].attn_output\n",
    "    print(norm(h[l].attn_output), norm(hidden_states), hidden_states.mean(), hidden_states.std())\n",
    "    print(norm(hidden_states), end=' ')\n",
    "    hidden_states = hidden_states + h[l].ffn_output\n",
    "    print(norm(h[l].ffn_output), norm(hidden_states), hidden_states.mean(), hidden_states.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hidden_states = h[layer2].ln_1(hidden_states)\n",
    "try:\n",
    "    with torch.no_grad(): _, attn = am2(am2_hidden_states, am2_attention_mask, output_attentions=True, q_hidden_states=_hidden_states)\n",
    "finally: am2.q_hidden_states = None\n",
    "attn = globalize(attn)\n",
    "attn[0, head2, -1].topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "attn = globalize(outputs.attentions[layer2])[0, head2, -1]\n",
    "show_topk(*attn.topk(5), indices_fn=append_tokens_to_positions)\n",
    "probs = outputs.logits[0, -1].softmax(-1)\n",
    "show_topk(*probs.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)\n",
    "\n",
    "for l in range(layer, layer + 1):\n",
    "    mask_tgt_indices = [41, 35, 29, 23, 17, 11, 5]\n",
    "    excluded_src_indices = [seq_len - 1] #if l == layer else []\n",
    "    attn = outputs.attentions[l]\n",
    "    if attn.dim() == 5:\n",
    "        attn = attn[:, 0, :, :, -seq_len:] # (bsz, num_blokcs, H, seq_len, window_size + seq_len) -> (bsz, H, seq_len, seq_len)\n",
    "#     for hd in range(H):\n",
    "#         for src_idx in range(seq_len):\n",
    "#             if hd != head and src_idx in excluded_src_indices:\n",
    "#                 attn[:, hd, src_idx] = 0\n",
    "    for hd in range(H):\n",
    "        for src_idx in range(seq_len):\n",
    "            for tgt_idx in range(seq_len):\n",
    "                if hd == head and src_idx not in excluded_src_indices and tgt_idx in mask_tgt_indices:\n",
    "                    attn[:, hd, src_idx, tgt_idx] = 0\n",
    "    h[l].attn.attention.w = outputs.attentions[l]\n",
    "try:\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "finally:\n",
    "    for l in range(layer, layer2):\n",
    "        h[l].attn.attention.w = None\n",
    "attn = globalize(outputs.attentions[layer2])[0, head2, -1]\n",
    "show_topk(*attn.topk(5), indices_fn=append_tokens_to_positions)\n",
    "probs = outputs.logits[0, -1].softmax(-1)\n",
    "show_topk(*probs.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_heads = []\n",
    "for l, attr in attrs.items():\n",
    "    values, indices = attr[:, -1, :].reshape(-1).topk(5) # (H, klen) -> (H * klen,), reshape == view\n",
    "    indices = unravel_index(indices, attr[:, -1, :].size())\n",
    "    _top_heads = [(l, h, t, v) for h, t, v in zip(\n",
    "        indices[0].tolist(), indices[1].tolist(), numpy(values))]\n",
    "    print(_top_heads)\n",
    "    top_heads += _top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_attrs = {layer: attr[:, -1].max(dim=-1)[0] for layer, attr in attrs.items()}\n",
    "top_heads = []\n",
    "for l, attr in head_attrs.items():\n",
    "    values, indices = attr.topk(5, dim=-1)\n",
    "    values, indices = numpy(values), indices.tolist()\n",
    "    top_heads += [(l, h, v) for h, v in zip(indices, values)]\n",
    "    print([(l, h, v) for h, v in zip(indices, values)])\n",
    "# top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_attrs = all_attrs['A B C D -> a'].sum(dim=(2, 3))\n",
    "# head_attrs = attrs.sum(dim=(2, 3))\n",
    "\n",
    "values, indices = head_attrs[:, :].view(-1).topk(10)\n",
    "indices = unravel_index(indices, head_attrs.size())\n",
    "top_heads = [(l, h, round(v, 8)) for l, h, v in zip(\n",
    "    indices[0].tolist(), indices[1].tolist(), values.tolist())]\n",
    "top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(_text, return_tensors='pt')\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "y = logits[0, -1].max()\n",
    "attentions = outputs.attentions if hasattr(outputs, 'attentions') else outputs[-1]\n",
    "for a in attentions: a.retain_grad()\n",
    "model.zero_grad()\n",
    "y.backward()\n",
    "\n",
    "# attns = torch.cat(attentions)\n",
    "grads = torch.cat([a.grad for a in attentions])\n",
    "attrs2 = attns * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(top_heads)):\n",
    "    layer, head, v, _ = top_heads[i]\n",
    "    if layer in [0, 1, ]: continue\n",
    "#     layer, head, v = 30, 10, 1.\n",
    "    fig, axs = plt.subplots(1,2,sharey=False, figsize=(10 * 2, 10))\n",
    "    for i, (a, _ax) in enumerate(zip([attns, attrs], axs)):\n",
    "        a = a[layer][head].detach().cpu()\n",
    "        a, annot = ((a * 100).long(), True) if i == -1 else (a, False)\n",
    "        res = sns.heatmap(a, square=True, cbar=False, annot=annot, fmt='d', linewidths=0.1, linecolor='grey', \n",
    "                          xticklabels=tokens, yticklabels=tokens, ax=_ax)\n",
    "        _ = res.set_xticklabels(res.get_xmajorticklabels(), fontsize=9+3-2, rotation=0)\n",
    "        _ = res.set_yticklabels(res.get_ymajorticklabels(), fontsize=9+3-2, rotation=0)\n",
    "        _ = plt.xlabel('%d-%d    %.4f' % (layer, head, v), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 1, 3\n",
    "fig, axs = plt.subplots(n_rows, n_cols, sharey=False, figsize=(5 * n_cols, 2 * 4 * n_rows))\n",
    "A = [attns, grads, attrs]\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        ax = axs[row][col] if row > 1 else axs[col]\n",
    "        a = A[col][:, :, src,tgt].detach()\n",
    "        if col == 0: a[-1, 0] = 1.\n",
    "#         fig = plt.subplots(1,1,sharey=False, figsize=(5 , 8))\n",
    "        ax = sns.heatmap(a, ax=ax)\n",
    "        ax.tick_params(top=True, labeltop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "values, indices = logits[0, -1].softmax(dim=-1).topk(5)\n",
    "list(zip(tokenizer.convert_ids_to_tokens(indices), values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode_plus(_text)['input_ids']\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "outputs = model.generate(torch.LongTensor([input_ids]).to(model.device))\n",
    "print(_text, tokenizer.decode(outputs[0]))\n",
    "\n",
    "input_ids = input_ids[: -3] + input_ids[-2:]\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "outputs = model.generate(torch.LongTensor([input_ids]).to(model.device))\n",
    "print(_text, tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_string(gpt2_tokenizer):\n",
    "    tokens = [gpt2_tokenizer.convert_ids_to_tokens(i) for i in range(120)]\n",
    "    tokens = [token for token in tokens if token not in string.digits + string.ascii_uppercase + string.ascii_lowercase]\n",
    "    tokens = ['Ġ' + token for token in tokens if gpt2_tokenizer._convert_token_to_id('Ġ' + token) != 50256]\n",
    "    token_ids = [gpt2_tokenizer._convert_token_to_id(token) for token in tokens]  # XD\n",
    "    print(tokens, len(tokens))\n",
    "    \n",
    "    sampled_tokens_idx = []\n",
    "    sampled_tokens = []\n",
    "    sampled_token_ids = []  # XD\n",
    "\n",
    "    random.seed(6)  # XD\n",
    "    #get sampled tokens idx\n",
    "    range_ = list(range(len(tokens)))\n",
    "    for i in range(362):\n",
    "        idx = random.choice(range_)\n",
    "        sampled_tokens_idx.append(idx)\n",
    "\n",
    "    for idx in sampled_tokens_idx:\n",
    "        sampled_tokens.append(tokens[idx])\n",
    "        sampled_token_ids.append(token_ids[idx])\n",
    "\n",
    "    text = ''.join(sampled_tokens).replace('Ġ', ' ')  # XD\n",
    "    print(text, len(sampled_token_ids)) # XD\n",
    "    return sampled_token_ids, text  # XD\n",
    "    \n",
    "    # print(\"\".join(sampled_tokens), len(sampled_tokens))\n",
    "    # return \"\".join(sampled_tokens), len(sampled_tokens)\n",
    "\n",
    "token_ids, text = get_random_string(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Big is to small as fast is to _',\n",
    "    'Bread is to eat as gun is to _',\n",
    "    'big: small, fast: _',\n",
    "    'bread: eat, gun: _ .',\n",
    "    'flower: fragrant, fire: hot, bread: delicious, gun: _ ',\n",
    "    'Big and small are _ .',\n",
    "    'What is twice 3? _.',\n",
    "    'What is the half 6? _.',\n",
    "    'There is a sequence: 3, 5, 2, 7. The number immediately precedes 5 is _.',  # :)\n",
    "    'There is a sequence: 3, 5, 2, 7. The number immediately follows 5 is _.',  # :(\n",
    "    'There is a sequence: 3, 5, 2, 7. The number between 5 and 7 is _.',\n",
    "    'There is a sequence of numbers: 3, 5, 2, 4. _ is the first number.',\n",
    "    'There is a sequence of numbers: 3, 5, 2. The reversed sequence is _.',\n",
    "    '''There is a sequence of numbers: 5, 1, 6, 3. The second number is 1.\n",
    "There is a sequence of numbers: 3, 7, 2, 4. The second number is _.''',\n",
    "    '''There is a sequence of letters: e, c, b, a. The last letter is a.\n",
    "There is a sequence of letters: f, d, b, g. The last letter is _.''',\n",
    "    '''The uppercase of c is C. The uppercase of f is _.''',\n",
    "    '''The successor of 3 is 4. The successor of 8 is _.''',\n",
    "    '''The successor of 3 is 4. The successor of _ is 6.''',\n",
    "#     '''The predecessor of 3 is 2. The predecessor of 5 is 4. The predecessor of 6 is _''',\n",
    "#     '''The previous integer of 4 is 3. The previous integer of 3 is _.''',\n",
    "#     '''3 minus 1 equals 2. 5 minus 1 equals _.''',\n",
    "    '''If 2 changes to 3, 5 changes to 6, then _ changes to 9''',\n",
    "    '''If 2 changes to 20, 3 changes to 30, then 5 changes to _''',\n",
    "    '''2 -> 3, 4 -> 5, 5 -> 6, 9 -> _.''',\n",
    "    '''3 -> 2, 5 -> 4, 6 -> 5, 9 -> _''',\n",
    "    '''9 -> 8, 7 -> 6, 6 -> 5, 2 -> _.''',\n",
    "    '''3 is to _ as 4 is to 8 and 5 is to 10.''',\n",
    "#     '''6 : _ :: 5 : 10 :: 7 : 14 :: 8 : 16.''',\n",
    "#     '''a is to _ as f is to g, h to i, i to j, s to t.''',\n",
    "#     '''c is to _ as f is to e, h to g, j to i.''',\n",
    "    '''c is to _ as j is to i, h to g, f to e.''',\n",
    "#     '''Twice 3 is 6, twice 4 is _.''',\n",
    "#     '''Half of 4 is 2, half of 6 is _.''',\n",
    "\n",
    "# '''Shall I compare thee to a summer's day?\n",
    "# Thou''',\n",
    "# '''Do not go gentle into that good night,\n",
    "# Old age should burn and rave at close of day;\n",
    "# Rage'''\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
