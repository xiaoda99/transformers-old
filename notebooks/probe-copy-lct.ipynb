{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, shortcut, mask_token = RobertaForMaskedLM, RobertaTokenizer, 'roberta-base', '<mask>'\n",
    "model, tokenizer = model_class.from_pretrained(shortcut), tokenizer_class.from_pretrained(shortcut)\n",
    "models[shortcut] = (model, tokenizer, mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, shortcut, mask_token = RobertaForMaskedLM, RobertaTokenizer, 'roberta-large', '<mask>'\n",
    "model, tokenizer = model_class.from_pretrained(shortcut), tokenizer_class.from_pretrained(shortcut)\n",
    "models[shortcut] = (model, tokenizer, mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, mask_token = models['roberta-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Paris is the _ of France'\n",
    "text = 'Paris is the captital of'\n",
    "text = 'Q: 4 and 9, which is larger? A:'\n",
    "text = '''Q: 3 and 5, which is larger? A: 5.\n",
    "Q: 4 and 9, which is larger? A:'''\n",
    "text = '''Q: 5, 3, 3, which is different from the others? A: 5.\n",
    "Q: 2, 2, 7, which is different from the others? A: 7.\n",
    "Q: 9, 4, 9, which is different from the others? A:'''\n",
    "text = '''Q: 5, 3, 2. Which is in the middle? A: 3.\n",
    "Q: 5, 4, 2. Which is in the middle? A:'''\n",
    "texts = [\n",
    "'''\\\n",
    "Q: double c. A: c c.\n",
    "Q: double b. A:''',\n",
    "'''\\\n",
    "Q: b b b, c c. Which is more, b or c? A: b.\n",
    "Q: e e , f f f. Which is more, e or f? A: f.\n",
    "Q: g g g, d d. Which is more, g or d? A:''',\n",
    "'''\\\n",
    "a b c changes to a b d.\n",
    "p q r changes to p q s.\n",
    "i j k changes to''',\n",
    "'''\\\n",
    "a b c changes to a b d.\n",
    "p q r s changes to p q r t.\n",
    "i j k l m changes to''',\n",
    "    \n",
    "    \n",
    "    \n",
    "# '''\\\n",
    "# i i j j k k changes to i i j j l l.\n",
    "# a a b b c c changes to a a b b d d.\n",
    "# p p q q r r changes to''',\n",
    "# '''\\\n",
    "# a b c changes to a b d.\n",
    "# e f g h changes to e f g i.\n",
    "# u v w x y changes to''',\n",
    "# '''\\\n",
    "# Q: b b b, c c. Which is more, former or latter? A: former.\n",
    "# Q: e e , f f f. Which is more, former or latter? A: latter.\n",
    "# Q: g g g, d d. Which is more, former or latter? A:''',\n",
    "# '''\\\n",
    "# Q: Surround f with b. A: b f b.\n",
    "# Q: Surround a with d. A: d a d.\n",
    "# Q: Surround c with e. A:''',\n",
    "# '''\\\n",
    "# Q: c, b, f. Which letter follows c? A: b.\n",
    "# Q: b, a, g. Which letter follows a? A: g\n",
    "# Q: f, g, d. Which letter follows f? A:''',\n",
    "]\n",
    "\n",
    "# text = '''Q: 5, 3, 3, which is special? A: 5.\n",
    "# Q: 2, 2, 7, which is special? A: 7.\n",
    "# Q: 9, 4, 9, which is special? A:'''\n",
    "\n",
    "# text = '''Q: 1, 0, 3, 0, 0, 5. Remove 0s. A: 1, 3, 5.\n",
    "# Q: 0, 1, 3, 0, 5, 5. Remove 0s. A: 1, 3, 5.\n",
    "# Q: 2, 0, 0, 4, 1, 0. Remove 0s. A:'''\n",
    "\n",
    "# text = '''Q: 4, 9, 5, which is the largest? A: 9.\n",
    "# Q: 8, 2, 6, which is the largest? A: 8.\n",
    "# Q: 3, 1, 7, which is the largest? A: 7.\n",
    "# Q: 1, 7, 3, which is the largest? A:'''\n",
    "\n",
    "# text = '''Q: Swap 4 and 7. A: 7 and 4.\n",
    "# Q: Swap 9 and 3. A: 3 and 9.\n",
    "# Q: Swap 5 and 2. A:'''\n",
    "\n",
    "# text = '''Q: 4 = 4? A: Yes.\n",
    "# Q: 6 = 2? A: No.\n",
    "# Q: 2 = 2? A: Yes.\n",
    "# Q: 5 = 5? A:'''\n",
    "\n",
    "# text = '''Q: 4 and 4. A: Same.\n",
    "# Q: 6 and 2. A: Different.\n",
    "# Q: 2 and 2. A: Same.\n",
    "# Q: 2 and 2. A:'''\n",
    "\n",
    "# text = '''Q: Which is the former of 5 and 2? A: 5.\n",
    "# Q: Which is the latter of 7 and 9? A: 9.\n",
    "# Q: Which is the latter of 1 and 7? A: 7.\n",
    "# Q: Which is the latter of 6 and 3? A:'''  # former is wrong\n",
    "\n",
    "# text = '''Q: 5 and 2, which is the former? A: 5.\n",
    "# Q: 7 and 9, which is the latter? A: 9.\n",
    "# Q: 1 and 7, Which is the latter? A: 7.\n",
    "# Q: 6 and 3. Which is the latter? A:'''\n",
    "\n",
    "# text = '''Q: 5, 3, 2. Which is the first? A: 5.\n",
    "# Q: 7, 9, 2. Which is the second? A: 9.\n",
    "# Q: 1, 4, 7. Which is the third? A: 7.\n",
    "# Q: 5, 4, 2. Which is the second? A:'''\n",
    "\n",
    "# text = '''Q: 5, 3, 3. How many 3s? A: 2.\n",
    "# Q: 4, 2, 7. How many 4s? A: 1.\n",
    "# Q: 1, 1, 1. How many 1s? A: 3.\n",
    "# Q: 2, 4, 2. How many 5s? A: 0.\n",
    "# Q: 6, 1, 6. How many 6s? A:'''\n",
    "\n",
    "text = texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1:\n",
      "John is tall but Mary is _ .\n",
      "text2:\n",
      "John is tall but Mary is <mask> .\n",
      "token1:\n",
      "['<s>', 'John', 'Ġis', 'Ġtall', 'Ġbut', 'ĠMary', 'Ġis', '<mask>', 'Ġ.', '</s>']\n",
      "token2:\n",
      "['<s>', 'John', 'Ġis', 'Ġtall', 'Ġbut', 'ĠMary', 'Ġis', '<mask>', 'Ġ.', '</s>']\n",
      "--------------------------------------------------------\n",
      "token_ids\n",
      "[0, 10567, 16, 6764, 53, 2708, 16, 50264, 479, 2]\n",
      "[7]\n",
      "['<s>', '@John', 'is', 'tall', 'but', 'Mary', 'is', '<mask>', '.', '</s>']\n",
      "#################################################\n",
      "logits:\n",
      "tensor([[[64.1458, -5.7664, 59.5445,  ...,  1.5092,  1.4614, 33.3519],\n",
      "         [36.0186, -3.9361, 50.4558,  ...,  1.0343,  0.7829, 24.4841],\n",
      "         [36.0185, -4.1961, 49.6261,  ...,  2.0924,  2.1780, 25.6546],\n",
      "         ...,\n",
      "         [43.8663, -4.6278, 52.8192,  ...,  1.2955,  2.3893, 28.9979],\n",
      "         [46.1004, -4.0114, 57.8426,  ...,  2.0919,  2.3034, 29.4621],\n",
      "         [59.1389, -4.9138, 70.4290,  ...,  2.8684,  2.0703, 32.7099]]])\n",
      "probs\n",
      "tensor([[[9.2314e-01, 4.0065e-31, 9.2668e-03,  ..., 5.7882e-28,\n",
      "          5.5181e-28, 3.9052e-14],\n",
      "         [1.0445e-15, 4.6427e-33, 1.9449e-09,  ..., 6.6897e-31,\n",
      "          5.2025e-31, 1.0222e-20],\n",
      "         [2.7352e-15, 9.3761e-33, 2.2219e-09,  ..., 5.0476e-30,\n",
      "          5.4987e-30, 8.6295e-20],\n",
      "         ...,\n",
      "         [2.3727e-09, 2.0631e-30, 1.8341e-05,  ..., 7.7083e-28,\n",
      "          2.3014e-27, 8.2784e-16],\n",
      "         [4.1157e-09, 7.0986e-31, 5.1763e-04,  ..., 3.1753e-28,\n",
      "          3.9231e-28, 2.4464e-16],\n",
      "         [1.2494e-05, 1.9009e-33, 9.9990e-01,  ..., 4.5578e-30,\n",
      "          2.0518e-30, 4.1565e-17]]])\n",
      "7\n",
      "probs[0][i]\n",
      "tensor([2.3727e-09, 2.0631e-30, 1.8341e-05,  ..., 7.7083e-28, 2.3014e-27,\n",
      "        8.2784e-16])\n",
      "probs[0][i].topk(5)\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.3865, 0.0626, 0.0506, 0.0498, 0.0411]),\n",
      "indices=tensor([  765,  6764, 10941,   650,  7174]))\n",
      "['Ġshort', 'Ġtall', 'Ġshorter', 'Ġsmall', 'Ġthin']\n",
      "tensor([0.3865, 0.0626, 0.0506, 0.0498, 0.0411])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-67b1f08487ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mall_attn_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0midx_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mtokenses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mtoken_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ġ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ġ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'The ball does not fit into the box because the _ is larger than the _ .',\n",
    "    'The box can not hold the ball because the _ is small .',\n",
    "    'John borrowed money from Mary because she was rich .',\n",
    "    'John borrowed money from Mary because he was poor .',\n",
    "    'John beat Mary because he was strong .',\n",
    "    'John beat Mary because she was weak .',\n",
    "#     'John borrowed from Mary because she was rich .',\n",
    "    'John surpassed Mary because he was fast .',\n",
    "#     'John surpassed Mary because she was slow .',\n",
    "    'Thing X _broke thing Y because thing X was _brittle .',\n",
    "    'John is _taller than Mary means that Mary is _shorter than John .',\n",
    "    'John is _older than Mary means that Mary is _younger than John .',\n",
    "    'John is _stronger than Mary means that Mary is _weaker than John .',\n",
    "    'John is _old but Mary is _young .',\n",
    "    'John is old but Mary is _ .',\n",
    "    'John is rich but Mary is _ .',\n",
    "    'John is tall but Mary is _ .',\n",
    "#     'Husky is a kind of _ .',\n",
    "#     'Lark is a kind of _ .',\n",
    "#     'Ball A is * bigger than ball B and ball A is * bigger .',\n",
    "#     'Ball A is * bigger than ball B so ball B is * smaller .',\n",
    "#     'Ball A is * bigger than ball B and ball A is * heavier .',\n",
    "#     'Ball A is * bigger than ball B so ball B is * lighter .',\n",
    "#     'A * dog is a kind of * animal .',\n",
    "#     'An * apple is a kind of * fruit .',\n",
    "#     'What is a * dog ? It is a kind of * animal .',\n",
    "#     'What is an * apple ? It is a kind of * fruit .',\n",
    "]\n",
    "templates = [\n",
    "    'A * %s is a kind of * %s .',\n",
    "    'A %s is a kind of %s .',\n",
    "#     'What is an * %s ? It is a kind of * %s .',\n",
    "]\n",
    "pairs = [\n",
    "    ('dog', 'animal'),\n",
    "    ('banana', 'fruit'),\n",
    "    ('hammer', 'tool'),\n",
    "    ('gun', 'weapon'),\n",
    "#     ('dog', '_'),\n",
    "#     ('banana', '_'),\n",
    "#     ('hammer', '_'),\n",
    "#     ('gun', '_'),\n",
    "]\n",
    "# text = texts[-4: -2]\n",
    "all_attn_probs, idx_pairs, tokenses = [], [], []\n",
    "\n",
    "\n",
    "# for text in texts[-3:]:\n",
    "for text in texts[-1:]:\n",
    "# for pair in pairs[-4:]:\n",
    "#     text = templates[1] % pair\n",
    "\n",
    "    print('text1:')\n",
    "    print(text)\n",
    "    \n",
    "    \n",
    "    if mask_token is not None:\n",
    "        text = text.replace(' _ ', ' %s ' % mask_token)\n",
    "    print('text2:')\n",
    "    print(text)\n",
    "    \n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text, add_special_tokens=True))\n",
    "    print('token1:')\n",
    "    print(tokens)\n",
    "    \n",
    "    \n",
    "    tokens = ['*' if token in ['*', 'Ġ*'] else token for token in tokens]\n",
    "    print('token2:')\n",
    "    print(tokens)\n",
    "    \n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    marker = '*'\n",
    "    \n",
    "    if marker in tokens:\n",
    "        print('marker in tokens...')\n",
    "        # 如果 * 在 tokens 中，\n",
    "        assert tokens.count(marker) == 2, str(tokens)\n",
    "        p, h = [i for i, token in enumerate(tokens) if token == marker]\n",
    "        print(marker)\n",
    "        print(p,h)\n",
    "        tokens = [token for token in tokens if token != marker]\n",
    "        h -= 1\n",
    "        print(tokens[p], tokens[h])\n",
    "        \n",
    "        \n",
    "        \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print('token_ids')\n",
    "    print(token_ids)\n",
    "    \n",
    "    # pred_idx 是 <mask> 在句子中的位置\n",
    "    pred_idx = [i for i, token in enumerate(tokens) if token == mask_token] if mask_token is not None else [-1]\n",
    "    print(pred_idx)\n",
    "    \n",
    "    # 如果 token 不以 G 开头，并且不是 <s> </s> <mask> 则在前面加上 @ ，否则将 G 替换为 空\n",
    "    tokens = ['@' + token if not token.startswith('Ġ') and token not in ['<s>', '</s>', '<mask>'] else token.replace('Ġ', '') \n",
    "              for token in tokens] \n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "    print('#################################################')\n",
    "    \n",
    "    input_ids = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        # 不追踪网络参数中的导数的目的，总之是为了减少可能存在的计算和内存消耗\n",
    "        \n",
    "        \n",
    "        logits, attns = model(input_ids, output_attentions=True)\n",
    "        \n",
    "        print('logits:')\n",
    "        print(logits)\n",
    "        \n",
    "        #print('attentions:')\n",
    "        #print(attns)\n",
    "        \n",
    "        # 对n维输入张量运用Softmax函数，将张量的每个元素缩放到（0,1）区间且和为1。\n",
    "        # dim:指明维度，dim=0表示按列计算；dim=-1是对某一维度的行进行softmax运算\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        print('probs')\n",
    "        print(probs)\n",
    "\n",
    "    for i in pred_idx:\n",
    "        print(i)\n",
    "        top_probs, top_indexes = probs[0][i].topk(5)\n",
    "        \n",
    "        print('probs[0][i]')\n",
    "        print(probs[0][i])\n",
    "        \n",
    "        print('probs[0][i].topk(5)')\n",
    "        print(probs[0][i].topk(5))\n",
    "        \n",
    "        \n",
    "        top_tokens = tokenizer.convert_ids_to_tokens(top_indexes)\n",
    "        print(top_tokens)\n",
    "        print(top_probs)\n",
    "\n",
    "    attn_scores, attn_probs = zip(*attns)\n",
    "    attn_scores, attn_probs = torch.cat(attn_scores, dim=0), torch.cat(attn_probs, dim=0)\n",
    "    all_attn_probs.append(attn_probs)\n",
    "    idx_pairs.append((p, h))\n",
    "    tokenses.append(tokens)\n",
    "    token_pairs.append((tokens[p].replace('Ġ', ''), tokens[h].replace('Ġ', '')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
