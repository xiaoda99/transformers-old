{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from itertools import product, chain\n",
    "import numpy as np\n",
    "from pattern.en import comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from child_frames import frames\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src/transformers')\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä unrelated']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(' unrelated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Z is * smaller than X , so X ( [ latter ] ) is * bigger ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is * smaller ( [ same ] ) ? [ Right ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is not * bigger ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is * smaller than X , so X ( [ latter ] ) is not * smaller ( [ same ] ) ? [ Right ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is * bigger ( [ same ] ) ? [ Right ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is * smaller ( [ opposite ] ) ? [ Right ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is not * bigger ( [ same ] ) ? [ Right ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is not * smaller ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is not * bigger ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is not * smaller ( [ same ] ) ? [ Right ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is * bigger ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is * smaller ( [ same ] ) ? [ Right ] .',\n",
       " ' X is not * bigger than Z , so X ( [ former ] ) is not * bigger ( [ same ] ) ? [ Right ] .',\n",
       " ' X is not * bigger than Z , so Z ( [ latter ] ) is not * smaller ( [ opposite ] ) ? [ Right ] .',\n",
       " ' X is not * bigger than Z , so Z ( [ latter ] ) is * bigger ( [ same ] ) ? [ Right ] .',\n",
       " ' X is not * bigger than Z , so X ( [ former ] ) is * smaller ( [ opposite ] ) ? [ Right ] .',\n",
       " ' Z is * smaller than X , so X ( [ latter ] ) is not * bigger ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is not * smaller ( [ same ] ) ? [ Wrong ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is * bigger ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is * smaller than X , so X ( [ latter ] ) is * smaller ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is not * bigger ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is not * smaller ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is * bigger ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is * smaller ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is * bigger ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is * smaller ( [ same ] ) ? [ Wrong ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is not * bigger ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is not * smaller ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is not * bigger than Z , so X ( [ former ] ) is * bigger ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is not * bigger than Z , so Z ( [ latter ] ) is * smaller ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' X is not * bigger than Z , so Z ( [ latter ] ) is not * bigger ( [ same ] ) ? [ Wrong ] .',\n",
       " ' X is not * bigger than Z , so X ( [ former ] ) is not * smaller ( [ opposite ] ) ? [ Wrong ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is not * taller ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is * taller ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is * taller ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is * higher ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is not * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' X is * bigger than Z , so Z ( [ latter ] ) is * higher ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' X is * bigger than Z , so X ( [ former ] ) is * taller ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is * smaller than X , so Z ( [ former ] ) is not * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is * taller ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is * smaller than X , so X ( [ latter ] ) is * higher ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is * smaller than X , so X ( [ latter ] ) is * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' X is not * bigger than Z , so X ( [ former ] ) is not * higher ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so X ( [ latter ] ) is * shorter ( [ unrelated ] ) ? [ Maybe ] .',\n",
       " ' Z is not * smaller than X , so Z ( [ former ] ) is not * taller ( [ unrelated ] ) ? [ Maybe ] .']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_template = \"{rel_prefix} {dt} {ent0} {rel} {dt} {ent1} {rel_suffix}\"\n",
    "B_templates = [\"{pred_prefix} {dt} {ent} {pred}\", \"{pred_prefix} {pred} {dt} {ent}\"]\n",
    "B_template = B_templates[0]\n",
    "entailment_templates = [\n",
    "    \"{A} ? {conj} , {B} .\",  # yes/no/maybe\n",
    "    \"{A} , so {B} ? {conj} .\",\n",
    "]\n",
    "marker = '*'\n",
    "def get_comparative(word, add_marker=True):\n",
    "    compar = comparative(word)\n",
    "    if add_marker:\n",
    "        compar = compar.replace('more ', 'more %s ' % marker) if compar.startswith('more ') else marker + ' ' + compar\n",
    "    return compar\n",
    "    \n",
    "def negate_sent(sent):\n",
    "    assert ' is ' in sent\n",
    "    neg_sents = []\n",
    "    neg_sents.append(sent.replace(' is ', ' is not '))\n",
    "    neg_sents.append('it is unlikely that ' + sent)\n",
    "    return neg_sents\n",
    "\n",
    "def make_sentences(index=-1, entities=[\"X\", \"Z\"], entity_set=string.ascii_uppercase, determiner=\"\",\n",
    "                   relation_group=[[\"big\",], [\"small\"]], rand_relation_group=[[\"short\"], [\"tall\", \"high\"]],\n",
    "                   relation_prefix=\"\", relation_suffix=\"\", predicate_prefix=\"\",\n",
    "                   n_entity_trials=3, has_negA=True, has_negB=True, has_neutral=False, mask_types=['sent_rel'], \n",
    "                   lexical_relations=['same', 'opposite', 'unrelated'], tag_lexical_rel=False, tag_entity_rel=False):\n",
    "    def form_As(relations):\n",
    "        return [A_template.format(dt=determiner, ent0=ent0, ent1=ent1, rel=rel, rel_prefix=relation_prefix, rel_suffix=relation_suffix)\n",
    "              for ent0, ent1, rel in [entities + relations[:1], reverse(entities) + reverse(relations)[:1]]]\n",
    "\n",
    "    As = []\n",
    "    for rel0 in relation_group[0]:\n",
    "        for rel1 in relation_group[1]:\n",
    "            relations = [\"is %s:%d than\" % (get_comparative(rel), i) for i, rel in enumerate([rel0, rel1])]\n",
    "            As += form_As(relations)\n",
    "    As = list(set(As))\n",
    "    negAs = join_lists([negate_sent(A)[:1] for A in As]) if has_negA else []\n",
    "\n",
    "    def form_Bs(predicates): \n",
    "        f = mask if 'entity' in mask_types else (lambda x: x)\n",
    "        return [B_template.format(dt=determiner, ent=f(ent), pred=pred, pred_prefix=predicate_prefix)\n",
    "              for ent, pred in zip(entities, predicates)]\n",
    "\n",
    "    Bs, negBs = {'orig': [], 'rand': []}, {}\n",
    "    for k, group in zip(['orig', 'rand'], [relation_group, rand_relation_group]):\n",
    "        for rel0 in group[0]:\n",
    "            for rel1 in group[1]:\n",
    "                predicates = [\"is %s:%d\" % (get_comparative(rel), i) for i, rel in enumerate([rel0, rel1])]\n",
    "                Bs[k] += form_Bs(predicates)\n",
    "    for k in Bs:\n",
    "        Bs[k] = list(set(Bs[k]))\n",
    "        if has_negB:\n",
    "            negBs[k] = join_lists([negate_sent(B)[:1] for B in Bs[k]])\n",
    "            Bs[k], negBs[k] = Bs[k] + [swap_entities(negB) for negB in negBs[k]], negBs[k] + [swap_entities(B) for B in Bs[k]]\n",
    "        else:\n",
    "            negBs[k] = [swap_entities(B) for B in Bs[k]]\n",
    "\n",
    "    def form_sentences(sentence_template, As, Bs, conj):\n",
    "        def extract_rel_id(s): return int(s[s.index(':') + 1])\n",
    "        def get_lexical_rel(rel_id_A, rel_id_B): return 'same' if rel_id_A == rel_id_B else 'opposite'\n",
    "        def strip_rel_id(s, lexical_rel=''):\n",
    "            rel_id_span = s[s.index(':'): s.index(':') + 2]\n",
    "            if lexical_rel != '':\n",
    "                if 'lexical_rel' in mask_types: lexical_rel = mask(lexical_rel)\n",
    "                lexical_rel = ' ( ' + lexical_rel + ' )'\n",
    "            return s.replace(rel_id_span, lexical_rel)\n",
    "        def compare_and_tag_entity(B, A):\n",
    "            entity = [e for e in entities if e in B][0]\n",
    "            entity_rel = 'former' if A.strip().startswith(entity) else 'latter'\n",
    "            if 'entity_rel' in mask_types: entity_rel = mask(entity_rel)\n",
    "            return B.replace(entity, entity + ' ( ' + entity_rel + ' )')                    \n",
    "        \n",
    "        if 'sent_rel' in mask_types: conj = mask(conj)\n",
    "        As_with_rel_ids = [(A, extract_rel_id(A)) for A in As]\n",
    "        Bs_with_rel_ids = [(B, extract_rel_id(B)) for B in Bs]\n",
    "            \n",
    "        sentences = []\n",
    "        for (A, rel_id_A), (B, rel_id_B) in product(As_with_rel_ids, Bs_with_rel_ids):\n",
    "            lexical_rel = 'unrelated' if 'Maybe' in conj else get_lexical_rel(rel_id_A, rel_id_B)\n",
    "            if lexical_rel in lexical_relations:\n",
    "                if tag_entity_rel: B = compare_and_tag_entity(B, A)\n",
    "                if not tag_lexical_rel: lexical_rel = ''\n",
    "                sent = sentence_template.format(A=strip_rel_id(A), B=strip_rel_id(B, lexical_rel), conj=conj)\n",
    "                sent = \" \" + \" \".join(sent.split())\n",
    "                sentences.append(sent)\n",
    "        return sentences\n",
    "\n",
    "    sentences = defaultdict(list)\n",
    "    for entailment_template in entailment_templates[-1:]:\n",
    "        for A, B, conj in [(As, Bs['orig'], 'Right'), \n",
    "                           (negAs, negBs['orig'], 'Right'), \n",
    "                           (As, negBs['orig'], 'Wrong'), \n",
    "                           (negAs, Bs['orig'], 'Wrong'),\n",
    "                           (As, Bs['rand'], 'Maybe'), \n",
    "                           (negAs, negBs['rand'], 'Maybe'), \n",
    "                           (As, negBs['rand'], 'Maybe'), \n",
    "                           (negAs, Bs['rand'], 'Maybe'),\n",
    "                          ]:\n",
    "            sentences[conj] += form_sentences(entailment_template, A, B, conj)\n",
    "    assert len(sentences['Right']) == len(sentences['Wrong']), '%d %d' % (len(sentences['Right']), len(sentences['Wrong']))\n",
    "    if has_neutral: sentences['Maybe'] = random.sample(sentences['Maybe'], len(sentences['Right']))\n",
    "    sentences = join_lists(sentences[k] for k in (sentences.keys() if has_neutral else ['Right', 'Wrong']))\n",
    "    \n",
    "    substituted_sent_groups = []\n",
    "    for sent in sentences:\n",
    "        sent_group = []\n",
    "        for _ in range(n_entity_trials):\n",
    "            e0, e1 = random.sample(entity_set, 2)\n",
    "            sent_group.append(sent.replace(entities[0], e0).replace(entities[1], e1))\n",
    "        substituted_sent_groups.append(sent_group)\n",
    "    return sentences, substituted_sent_groups\n",
    "\n",
    "make_sentences(has_negA=True, has_negB=True, has_neutral=True, tag_lexical_rel=True, tag_entity_rel=True, \n",
    "               mask_types=['sent_rel', 'lexical_rel', 'entity_rel'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, shortcut = RobertaForMaskedLM, RobertaTokenizer, 'roberta-large'\n",
    "model, tokenizer = None, tokenizer_class.from_pretrained(shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in convert_example_to_features: features.labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5483, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "in convert_example_to_features: features.labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 276, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "nTrain = 16524, nValid = 6156\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(frames)\n",
    "all_lines = [make_sentences(relation_group=rg, rand_relation_group=frames[(i + 1) % len(frames)], n_entity_trials=3, \n",
    "                            has_negA=True, has_negB=True, tag_lexical_rel=True, tag_entity_rel=False,\n",
    "                            has_neutral=True, mask_types=['lexical_rel'])[1] \n",
    "             for i, rg in enumerate(frames)]\n",
    "# all_lines = join_lists(all_lines)\n",
    "# all_lines = join_lists(all_lines)\n",
    "for k in CHILDDataset.all_lines: CHILDDataset.all_lines[k] = None\n",
    "train_dataset = CHILDDataset(all_lines, tokenizer, max_noise_len=0, split_pct=[0.7, 0.3, 0.0], mode='train')\n",
    "eval_dataset = CHILDDataset(all_lines, tokenizer, max_noise_len=0, split_pct=[0.7, 0.3, 0.0], mode='dev')\n",
    "print('nTrain = %d, nValid = %d' % (len(train_dataset), len(eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5731bc65e7fc4fb7921a0ffcd062745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99600ed2a77e4c2cb9baf9d4b0496031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=517.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.531, 'learning_rate': 1.8710509348807224e-05, 'epoch': 0.193, 'step': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cdab553c7445deb2702fd619a0a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.236, 'eval_acc0': 0.951, 'eval_stat0': 'Ä same 0.38 0.99, Ä opposite 0.29 0.99, Ä unrelated 0.33 1.00, ', 'epoch': 0.193, 'step': 100}\n",
      "{'loss': 0.045, 'learning_rate': 1.7421018697614445e-05, 'epoch': 0.387, 'step': 200}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4c31f7a8f7458cadac0b9626b91d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.434, 'eval_acc0': 0.967, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 0.387, 'step': 200}\n",
      "{'loss': 0.012, 'learning_rate': 1.6131528046421664e-05, 'epoch': 0.58, 'step': 300}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a38be5ea3cf4f0c8862d2b2d6ade9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.283, 'eval_acc0': 0.97, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 0.58, 'step': 300}\n",
      "{'loss': 0.01, 'learning_rate': 1.4842037395228886e-05, 'epoch': 0.774, 'step': 400}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05dc53ff9b044f78b740c020ed77408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.549, 'eval_acc0': 0.961, 'eval_stat0': 'Ä same 0.35 1.00, Ä opposite 0.32 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 0.774, 'step': 400}\n",
      "{'loss': 0.01, 'learning_rate': 1.3552546744036106e-05, 'epoch': 0.967, 'step': 500}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64586ddc40c94d9a9a4779285ac05f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.514, 'eval_acc0': 0.942, 'eval_stat0': 'Ä same 0.37 0.99, Ä opposite 0.30 0.99, Ä unrelated 0.33 1.00, ', 'epoch': 0.967, 'step': 500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd29e63636e4a39ba4c9377332e38c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=517.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'learning_rate': 1.2263056092843328e-05, 'epoch': 1.161, 'step': 600}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b7bf86fca845bcad37a4986eba9041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.331, 'eval_acc0': 0.976, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 1.161, 'step': 600}\n",
      "{'loss': 0.005, 'learning_rate': 1.0973565441650548e-05, 'epoch': 1.354, 'step': 700}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555ec4b8e72f490c83834188c1cf24b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.487, 'eval_acc0': 0.944, 'eval_stat0': 'Ä same 0.36 0.99, Ä opposite 0.30 0.99, Ä unrelated 0.33 1.00, ', 'epoch': 1.354, 'step': 700}\n",
      "{'loss': 0.001, 'learning_rate': 9.68407479045777e-06, 'epoch': 1.547, 'step': 800}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746cd9428ac9458ab4b5330887599cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.217, 'eval_acc0': 0.976, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.32 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 1.547, 'step': 800}\n",
      "{'loss': 0.0, 'learning_rate': 8.39458413926499e-06, 'epoch': 1.741, 'step': 900}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3976796231fd49d9bdbf050bc8413d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.259, 'eval_acc0': 0.978, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 1.741, 'step': 900}\n",
      "{'loss': 0.003, 'learning_rate': 7.105093488072212e-06, 'epoch': 1.934, 'step': 1000}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1060255ba356457bb6aca3faa619812c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.211, 'eval_acc0': 0.98, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 1.934, 'step': 1000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d26beccc96f4fee83bda20ddaaea02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=517.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 5.815602836879432e-06, 'epoch': 2.128, 'step': 1100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2f301ef09245eeabfa02d8d3c3df8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.215, 'eval_acc0': 0.98, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 2.128, 'step': 1100}\n",
      "{'loss': 0.0, 'learning_rate': 4.526112185686654e-06, 'epoch': 2.321, 'step': 1200}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf8e9acaa994c54bb56baf4b7f528f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.221, 'eval_acc0': 0.979, 'eval_stat0': 'Ä same 0.34 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 2.321, 'step': 1200}\n",
      "{'loss': 0.002, 'learning_rate': 3.236621534493875e-06, 'epoch': 2.515, 'step': 1300}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ead5562eec4b4c9f5b65542aed4061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.337, 'eval_acc0': 0.975, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 2.515, 'step': 1300}\n",
      "{'loss': 0.0, 'learning_rate': 1.9471308833010964e-06, 'epoch': 2.708, 'step': 1400}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982b8ccae911435a844108f03b044e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.326, 'eval_acc0': 0.977, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.34 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 2.708, 'step': 1400}\n",
      "{'loss': 0.0, 'learning_rate': 6.576402321083172e-07, 'epoch': 2.901, 'step': 1500}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd1b9b6872d484a9ea95478b9217973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.328, 'eval_acc0': 0.976, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 2.901, 'step': 1500}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fe5d5ae75a4f678d17b9d2678ac263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=97.0, style=ProgressStyle(description_wiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.328, 'eval_acc0': 0.976, 'eval_stat0': 'Ä same 0.33 1.00, Ä opposite 0.33 1.00, Ä unrelated 0.33 1.00, ', 'epoch': 3.0, 'step': 1551}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1551, training_loss=0.040174619352948214)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_class.from_pretrained('roberta-base', model=model)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=32, per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5, num_train_epochs=3,\n",
    "    logging_steps=100, eval_steps=100, save_steps=0,\n",
    "    evaluate_during_training=True,\n",
    ")\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "trainer.tokenizer = tokenizer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataloader = trainer.get_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in dataloader: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 387,  16,  ...,   2,   0,   0],\n",
       "         [  0, 534,  16,  ...,   2,   0,   0],\n",
       "         [  0, 975,  16,  ...,   2,   0,   0],\n",
       "         ...,\n",
       "         [  0, 487,  16,  ..., 479,   2,   0],\n",
       "         [  0, 673,  16,  ..., 479,   2,   0],\n",
       "         [  0, 725,  16,  ..., 479,   2,   0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  ..., 22,  0,  0],\n",
       "         [ 0,  1,  2,  ..., 22,  0,  0],\n",
       "         [ 0,  1,  2,  ..., 22,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  1,  2,  ..., 22, 23,  0],\n",
       "         [ 0,  1,  2,  ..., 22, 23,  0],\n",
       "         [ 0,  1,  2,  ..., 22, 23,  0]])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer._prepare_inputs(inputs, model)\n",
    "loss, logits = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax((logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz, seq_len, vocab_size = (logits).size()\n",
    "masks = (inputs['labels'] != -100)\n",
    "n_mask = masks.sum(dim=-1)[0].item()\n",
    "pred_labels = logits.masked_select(mask.unsqueeze(-1)).view(bsz, n_mask, vocab_size).argmax(dim=-1)\n",
    "labels = inputs['labels'].masked_select(masks).view(bsz, n_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5442,\n",
       "        -100, -100, -100, -100, -100, 5483, -100, -100, 5143, -100, -100, -100,\n",
       "        -100])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = (logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1).argmax(dim=-1)\n",
    "\n",
    "labels = (inputs['labels'] * (inputs['labels'] != -100)).sum(dim=-1)\n",
    "\n",
    "(pred_labels == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.4\n"
     ]
    }
   ],
   "source": [
    "print('%.1f' % 94.433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä Right 0.6260414719581604\n",
      "Ä Right 0.6174842119216919\n",
      "Ä Right 0.627226710319519\n",
      "Ä Right 0.6074517965316772\n",
      "Ä Right 0.6034893989562988\n",
      "Ä Right 0.5928120613098145\n",
      "Ä Maybe 0.9999963045120239\n",
      "Ä Maybe 0.9999955892562866\n",
      "Ä Maybe 0.9999963045120239\n",
      "Ä Right 0.6209385395050049\n",
      "Ä Right 0.6153814196586609\n",
      "Ä Right 0.6189284324645996\n",
      "Ä Right 0.6173365712165833\n",
      "Ä Right 0.6170286536216736\n",
      "Ä Right 0.6148619651794434\n",
      "Ä Maybe 0.9999971389770508\n",
      "Ä Maybe 0.9999972581863403\n",
      "Ä Maybe 0.9999969005584717\n",
      "Ä Right 0.6081119179725647\n",
      "Ä Right 0.6188942790031433\n",
      "Ä Right 0.6106655597686768\n",
      "Ä Right 0.6149485111236572\n",
      "Ä Right 0.6151918172836304\n",
      "Ä Right 0.6127419471740723\n",
      "Ä Right 0.6444377303123474\n",
      "Ä Right 0.6497629284858704\n",
      "Ä Right 0.6418505311012268\n",
      "Ä Right 0.6264133453369141\n",
      "Ä Right 0.6273400783538818\n",
      "Ä Right 0.6344654560089111\n",
      "Ä Right 0.6158509254455566\n",
      "Ä Right 0.6200970411300659\n",
      "Ä Right 0.6208559274673462\n",
      "Ä Maybe 0.9999988079071045\n",
      "Ä Maybe 0.9999986886978149\n",
      "Ä Maybe 0.9999988079071045\n",
      "Ä Maybe 0.9999974966049194\n",
      "Ä Maybe 0.9999971389770508\n",
      "Ä Maybe 0.9999974966049194\n",
      "Ä Maybe 0.9999963045120239\n",
      "Ä Maybe 0.999996542930603\n",
      "Ä Maybe 0.9999966621398926\n",
      "Ä Right 0.623246967792511\n",
      "Ä Right 0.6270768046379089\n",
      "Ä Right 0.618299663066864\n",
      "Ä Maybe 0.9999973773956299\n",
      "Ä Maybe 0.9999972581863403\n",
      "Ä Maybe 0.9999977350234985\n",
      "Ä Maybe 0.9999961853027344\n",
      "Ä Maybe 0.9999959468841553\n",
      "Ä Maybe 0.9999960660934448\n",
      "Ä Right 0.6204316020011902\n",
      "Ä Right 0.6157919764518738\n",
      "Ä Right 0.6199951171875\n",
      "Ä Maybe 0.9999948740005493\n",
      "Ä Maybe 0.9999945163726807\n",
      "Ä Maybe 0.9999958276748657\n",
      "Ä Right 0.6182829141616821\n",
      "Ä Right 0.6187686324119568\n",
      "Ä Right 0.6175054907798767\n",
      "Ä Right 0.6209660768508911\n",
      "Ä Right 0.6262878775596619\n",
      "Ä Right 0.6350439786911011\n",
      "Ä Right 0.6232182383537292\n"
     ]
    }
   ],
   "source": [
    "for i, label_id in enumerate(pred_labels):\n",
    "    print(tokenizer._convert_id_to_token(label_id.item()), probs[i, label_id].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([40, 24,  0], device='cuda:0'),\n",
       "indices=tensor([5143, 5359,    0], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels.bincount().topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([24, 22, 18], device='cuda:0'),\n",
       "indices=tensor([ 5359,  5143, 31273], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.bincount().topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ä Maybe'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(5359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = probs.topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5143, 31273, 13984, 235, 37234]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä Correct']\n",
      "[5143, 31273, 235, 13984, 10039]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä Left']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['Ä Wrong', 'Ä Right', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['Ä Wrong', 'Ä Right', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä wrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä RIGHT']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä Correct']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä Correct']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä RIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['Ä Right', 'Ä Wrong', 'Ä right', 'Right', 'Ä wrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä RIGHT']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['Ä Right', 'Ä Wrong', 'Right', 'Ä right', 'Ä RIGHT']\n"
     ]
    }
   ],
   "source": [
    "for top_idx in indices:\n",
    "    print(top_idx.tolist())\n",
    "    print(tokenizer.convert_ids_to_tokens(top_idx.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
