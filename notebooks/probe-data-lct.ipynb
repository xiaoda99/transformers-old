{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afraid\tterrified/horrified/scared/stiff/petrified/fearful/panicky\n",
      "angry\tfurious/enraged/outraged/aggrivated/irate/seething\n",
      "ask\tbeg/implore/pray/entreat/supplicate/insist\n",
      "bad\tawful/atrocious/abominable/dreadful/painful/terrible/unspeakable\n",
      "boring\ttedious/deadening/dull/ho-hum/irksome/tiresome/wearisome\n",
      "cat\tlion/tiger/leopard/panther/jaguar\n",
      "chuckle\tlaugh/guffaw/chortle/guffaw/snicker/snigger/titter/roar\n",
      "confused\tlost/bewildered/trapped/desperate\n",
      "creative\tingenious/inventive/demiurgic/deviceful/innovational/innovative/innovatory/original/originative\n",
      "cry\tscream/shriek/screech/screeching\n",
      "damp\tdrenched/dripping/saturated/soaked/soaking/sodden/sopping/soppy/soused/wringing-wet\n",
      "dinner\tfeast/banquet/fiesta\n",
      "dislike\thate/abhor/detest/loathe/abominate/execrate/contemn/despise/scorn/disdain\n",
      "doze\tsleep/slumber\n",
      "drizzle\train/shower/raifall/deluge\n",
      "excited\tagitated/nervous\n",
      "faith\tfanatism/fanaticism/zealotry\n",
      "giggle\tlaugh/guffaw/chortle/guffaw/snicker/snigger/titter/roar\n",
      "guilty\tremorseful/sorrowful/unworthy\n",
      "happy\tecstatic/enraptured/rapturous/rapt/rhapsodic\n",
      "house\tpalace/castle\n",
      "hungry\tstarving/famished/peckish/ravenous/starved\n",
      "indulge\tpamper/spoil/coddle\n",
      "interesting\texciting/exhilarant/exhilarating/exhilarative/eye-popping/inspiring/intoxicating/rousing/stimulating/stirring/arresting/interesting/intriguing/moving/provocative/heady/thrilling\n",
      "irritate\tenrage/incense/infuriate/ire/mad/madden/steam/umbrage\n",
      "jog\trun/scarper/flee/fly\n",
      "lake\tsea/ocean\n",
      "like\tlove/care/fond/crush/infatuate\n",
      "love\tadore/fetishize/idolize/idolise/worship/hero-worship/revere\n",
      "monkey\tgorilla\n",
      "nap\tsleep/slumber\n",
      "necessary\tessential/indispensable/vital/required\n",
      "opposed\taverse/antipathetic/antipathetical/indisposed/loath/loth\n",
      "pain\ttorment/torture/agony\n",
      "pony\thorse\n",
      "poorly\tafflicted/sick/ill/stricken\n",
      "rain\tdeluge/shower\n",
      "sad\tdesparate/despondent/despairing/desponding/forlorn/hopeless/melancholy\n",
      "sea\tocean\n",
      "snack\tmeal/eat\n",
      "sniffles\tpneumonia\n",
      "soon\timmediately/promptly/straightaway\n",
      "strong\tpowerful/forceful/super/potent\n",
      "tasty\tdelicious/delectable/luscious/pleasant-tasting/scrumptious/toothsome/yummy/mouth-watering/ambrosial/heavenly\n",
      "tired\texhausted/drained\n",
      "unfortunate\ttragic/woeful/grievous/wretched/miserable/awful/lamentable/regrettable/desperate/hopeless/disastrous\n",
      "unhappy\tmiserable/suffering/wretched\n",
      "want\tcrave/hunger/thirst/starve/lust\n",
      "warm\thot/fiery/flaming/heated/red-hot/sizzling/sensual/sultry/torrid/white-hot\n",
      "well\tflourishing/robust/booming/prospering/prosperous/thriving\n",
      "airplane\taeroplane/plane\n",
      "auto\tcar/automobile/motorcar\n",
      "baby\tinfant/babe\n",
      "bicycle\tbike/wheel/cycle\n",
      "child\tkid/youngster/minor/shaver/nipper/small_fry/tiddler/tike/tyke/fry/nestling\n",
      "cloth\tfabric/material/textile\n",
      "clothes\tclothing/apparel/dress\n",
      "confused\tbaffled/befuddled/bemused/bewildered/confounded/lost/mazed/mixed-up\n",
      "dollars\tbucks\n",
      "emphasis\taccent/accentuaion/importance/stress/significance\n",
      "father\tdad/daddy\n",
      "flower\tblossom/bloom\n",
      "harbor\tseaport/haven/harbour\n",
      "help\taid/assist\n",
      "hieroglyph\thieroglyphic/pictogram\n",
      "homogeneous\tuniform/unvarying\n",
      "honest\tsincere/ingenuous/true/direct/truthful\n",
      "identical\tsame/indistinguishable\n",
      "incorrect\twrong/counterfactual/erroneous/inaccurate/specious/unsound/untrue/false\n",
      "intelligent\tclever/smart\n",
      "jewel\tgem/stone\n",
      "lad\tchap/fellow/feller/fella/gent/blighter/cuss/bloke\n",
      "lady\tmadam/dame/ma'am/gentlewoman/madame/woman\n",
      "lazy\tindolent/faineant/otiose/slothful/work-shy\n",
      "list\tlisting/enumeration\n",
      "loyal\tfaithful/true/dependable/devoted/reliable\n",
      "market\tmarketplace/mart/bazaar\n",
      "mend\trepair/fix/fixing/fixture/mending/reparation/patch/darn/darning\n",
      "mesh\tgauze/netting/veiling/hairnet/reseau/net/save-all/snood/sparker/tulle/wirework/grillwork\n",
      "monument\tmemorial\n",
      "mother\tmom/mommy/mum\n",
      "murder\tslaying/slay/execution\n",
      "new\tmodern/recent\n",
      "obsolete\toutdated/out-of-date/superannuated\n",
      "organized\tarranged/configured/corporate/incorporated/re-formed/reorganized/reorganised\n",
      "package\tparcel/pack/packet/bundle\n",
      "phone\ttelephone/cell/cellphone/smartphone\n",
      "portion\tpart/component_part/component/constituent\n",
      "railway\trailroad\n",
      "rational\tlogical/coherent/reasonable/sane\n",
      "reasonable\tsensible\n",
      "rock\tstone\n",
      "shore\tcoast/strand/bank/beach/\n",
      "snake\tserpent/ophidian\n",
      "sofa\tcouch/lounge\n",
      "spouse\tpartner/mate/better_half\n",
      "style\tmanner/mode/fashion/way\n",
      "sweets\tconfectionery/dessert/confection\n",
      "villain\tscoundrel/rascal\n",
      "vocabulary\tlexicon/wordbook/dictionary/glossary\n"
     ]
    }
   ],
   "source": [
    "file_synonyms_intensity = open('synonyms_intensity.txt')\n",
    "file_synonyms_exact = open('synonyms_exact.txt')\n",
    "\n",
    "synonyms_intensity = file_synonyms_intensity.read()\n",
    "synonyms_exact = file_synonyms_exact.read()\n",
    "\n",
    "print(synonyms_intensity)\n",
    "print(synonyms_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('afraid',\n",
       "              ['terrified',\n",
       "               'horrified',\n",
       "               'scared',\n",
       "               'stiff',\n",
       "               'petrified',\n",
       "               'fearful',\n",
       "               'panicky']),\n",
       "             ('airplane', ['aeroplane', 'plane']),\n",
       "             ('angry',\n",
       "              ['furious',\n",
       "               'enraged',\n",
       "               'outraged',\n",
       "               'aggrivated',\n",
       "               'irate',\n",
       "               'seething']),\n",
       "             ('ask',\n",
       "              ['beg', 'implore', 'pray', 'entreat', 'supplicate', 'insist']),\n",
       "             ('auto', ['car', 'automobile', 'motorcar']),\n",
       "             ('baby', ['infant', 'babe']),\n",
       "             ('bad',\n",
       "              ['awful',\n",
       "               'atrocious',\n",
       "               'abominable',\n",
       "               'dreadful',\n",
       "               'painful',\n",
       "               'terrible',\n",
       "               'unspeakable']),\n",
       "             ('bicycle', ['bike', 'wheel', 'cycle']),\n",
       "             ('boring',\n",
       "              ['tedious',\n",
       "               'deadening',\n",
       "               'dull',\n",
       "               'ho-hum',\n",
       "               'irksome',\n",
       "               'tiresome',\n",
       "               'wearisome']),\n",
       "             ('cat', ['lion', 'tiger', 'leopard', 'panther', 'jaguar']),\n",
       "             ('child',\n",
       "              ['kid',\n",
       "               'youngster',\n",
       "               'minor',\n",
       "               'shaver',\n",
       "               'nipper',\n",
       "               'small_fry',\n",
       "               'tiddler',\n",
       "               'tike',\n",
       "               'tyke',\n",
       "               'fry',\n",
       "               'nestling']),\n",
       "             ('chuckle',\n",
       "              ['laugh',\n",
       "               'guffaw',\n",
       "               'chortle',\n",
       "               'guffaw',\n",
       "               'snicker',\n",
       "               'snigger',\n",
       "               'titter',\n",
       "               'roar']),\n",
       "             ('cloth', ['fabric', 'material', 'textile']),\n",
       "             ('clothes', ['clothing', 'apparel', 'dress']),\n",
       "             ('confused', ['lost', 'bewildered', 'trapped', 'desperate']),\n",
       "             ('creative',\n",
       "              ['ingenious',\n",
       "               'inventive',\n",
       "               'demiurgic',\n",
       "               'deviceful',\n",
       "               'innovational',\n",
       "               'innovative',\n",
       "               'innovatory',\n",
       "               'original',\n",
       "               'originative']),\n",
       "             ('cry', ['scream', 'shriek', 'screech', 'screeching']),\n",
       "             ('damp',\n",
       "              ['drenched',\n",
       "               'dripping',\n",
       "               'saturated',\n",
       "               'soaked',\n",
       "               'soaking',\n",
       "               'sodden',\n",
       "               'sopping',\n",
       "               'soppy',\n",
       "               'soused',\n",
       "               'wringing-wet']),\n",
       "             ('dinner', ['feast', 'banquet', 'fiesta']),\n",
       "             ('dislike',\n",
       "              ['hate',\n",
       "               'abhor',\n",
       "               'detest',\n",
       "               'loathe',\n",
       "               'abominate',\n",
       "               'execrate',\n",
       "               'contemn',\n",
       "               'despise',\n",
       "               'scorn',\n",
       "               'disdain']),\n",
       "             ('dollars', ['bucks']),\n",
       "             ('doze', ['sleep', 'slumber']),\n",
       "             ('drizzle', ['rain', 'shower', 'raifall', 'deluge']),\n",
       "             ('emphasis',\n",
       "              ['accent',\n",
       "               'accentuaion',\n",
       "               'importance',\n",
       "               'stress',\n",
       "               'significance']),\n",
       "             ('excited', ['agitated', 'nervous']),\n",
       "             ('faith', ['fanatism', 'fanaticism', 'zealotry']),\n",
       "             ('father', ['dad', 'daddy']),\n",
       "             ('flower', ['blossom', 'bloom']),\n",
       "             ('giggle',\n",
       "              ['laugh',\n",
       "               'guffaw',\n",
       "               'chortle',\n",
       "               'guffaw',\n",
       "               'snicker',\n",
       "               'snigger',\n",
       "               'titter',\n",
       "               'roar']),\n",
       "             ('guilty', ['remorseful', 'sorrowful', 'unworthy']),\n",
       "             ('happy',\n",
       "              ['ecstatic', 'enraptured', 'rapturous', 'rapt', 'rhapsodic']),\n",
       "             ('harbor', ['seaport', 'haven', 'harbour']),\n",
       "             ('help', ['aid', 'assist']),\n",
       "             ('hieroglyph', ['hieroglyphic', 'pictogram']),\n",
       "             ('homogeneous', ['uniform', 'unvarying']),\n",
       "             ('honest',\n",
       "              ['sincere', 'ingenuous', 'true', 'direct', 'truthful']),\n",
       "             ('house', ['palace', 'castle']),\n",
       "             ('hungry',\n",
       "              ['starving', 'famished', 'peckish', 'ravenous', 'starved']),\n",
       "             ('identical', ['same', 'indistinguishable']),\n",
       "             ('incorrect',\n",
       "              ['wrong',\n",
       "               'counterfactual',\n",
       "               'erroneous',\n",
       "               'inaccurate',\n",
       "               'specious',\n",
       "               'unsound',\n",
       "               'untrue',\n",
       "               'false']),\n",
       "             ('indulge', ['pamper', 'spoil', 'coddle']),\n",
       "             ('intelligent', ['clever', 'smart']),\n",
       "             ('interesting',\n",
       "              ['exciting',\n",
       "               'exhilarant',\n",
       "               'exhilarating',\n",
       "               'exhilarative',\n",
       "               'eye-popping',\n",
       "               'inspiring',\n",
       "               'intoxicating',\n",
       "               'rousing',\n",
       "               'stimulating',\n",
       "               'stirring',\n",
       "               'arresting',\n",
       "               'interesting',\n",
       "               'intriguing',\n",
       "               'moving',\n",
       "               'provocative',\n",
       "               'heady',\n",
       "               'thrilling']),\n",
       "             ('irritate',\n",
       "              ['enrage',\n",
       "               'incense',\n",
       "               'infuriate',\n",
       "               'ire',\n",
       "               'mad',\n",
       "               'madden',\n",
       "               'steam',\n",
       "               'umbrage']),\n",
       "             ('jewel', ['gem', 'stone']),\n",
       "             ('jog', ['run', 'scarper', 'flee', 'fly']),\n",
       "             ('lad',\n",
       "              ['chap',\n",
       "               'fellow',\n",
       "               'feller',\n",
       "               'fella',\n",
       "               'gent',\n",
       "               'blighter',\n",
       "               'cuss',\n",
       "               'bloke']),\n",
       "             ('lady',\n",
       "              ['madam', 'dame', \"ma'am\", 'gentlewoman', 'madame', 'woman']),\n",
       "             ('lake', ['sea', 'ocean']),\n",
       "             ('lazy',\n",
       "              ['indolent', 'faineant', 'otiose', 'slothful', 'work-shy']),\n",
       "             ('like', ['love', 'care', 'fond', 'crush', 'infatuate']),\n",
       "             ('list', ['listing', 'enumeration']),\n",
       "             ('love',\n",
       "              ['adore',\n",
       "               'fetishize',\n",
       "               'idolize',\n",
       "               'idolise',\n",
       "               'worship',\n",
       "               'hero-worship',\n",
       "               'revere']),\n",
       "             ('loyal',\n",
       "              ['faithful', 'true', 'dependable', 'devoted', 'reliable']),\n",
       "             ('market', ['marketplace', 'mart', 'bazaar']),\n",
       "             ('mend',\n",
       "              ['repair',\n",
       "               'fix',\n",
       "               'fixing',\n",
       "               'fixture',\n",
       "               'mending',\n",
       "               'reparation',\n",
       "               'patch',\n",
       "               'darn',\n",
       "               'darning']),\n",
       "             ('mesh',\n",
       "              ['gauze',\n",
       "               'netting',\n",
       "               'veiling',\n",
       "               'hairnet',\n",
       "               'reseau',\n",
       "               'net',\n",
       "               'save-all',\n",
       "               'snood',\n",
       "               'sparker',\n",
       "               'tulle',\n",
       "               'wirework',\n",
       "               'grillwork']),\n",
       "             ('monkey', ['gorilla']),\n",
       "             ('monument', ['memorial']),\n",
       "             ('mother', ['mom', 'mommy', 'mum']),\n",
       "             ('murder', ['slaying', 'slay', 'execution']),\n",
       "             ('nap', ['sleep', 'slumber']),\n",
       "             ('necessary',\n",
       "              ['essential', 'indispensable', 'vital', 'required']),\n",
       "             ('new', ['modern', 'recent']),\n",
       "             ('obsolete', ['outdated', 'out-of-date', 'superannuated']),\n",
       "             ('opposed',\n",
       "              ['averse',\n",
       "               'antipathetic',\n",
       "               'antipathetical',\n",
       "               'indisposed',\n",
       "               'loath',\n",
       "               'loth']),\n",
       "             ('organized',\n",
       "              ['arranged',\n",
       "               'configured',\n",
       "               'corporate',\n",
       "               'incorporated',\n",
       "               're-formed',\n",
       "               'reorganized',\n",
       "               'reorganised']),\n",
       "             ('package', ['parcel', 'pack', 'packet', 'bundle']),\n",
       "             ('pain', ['torment', 'torture', 'agony']),\n",
       "             ('phone', ['telephone', 'cell', 'cellphone', 'smartphone']),\n",
       "             ('pony', ['horse']),\n",
       "             ('poorly', ['afflicted', 'sick', 'ill', 'stricken']),\n",
       "             ('portion',\n",
       "              ['part', 'component_part', 'component', 'constituent']),\n",
       "             ('railway', ['railroad']),\n",
       "             ('rain', ['deluge', 'shower']),\n",
       "             ('rational', ['logical', 'coherent', 'reasonable', 'sane']),\n",
       "             ('reasonable', ['sensible']),\n",
       "             ('rock', ['stone']),\n",
       "             ('sad',\n",
       "              ['desparate',\n",
       "               'despondent',\n",
       "               'despairing',\n",
       "               'desponding',\n",
       "               'forlorn',\n",
       "               'hopeless',\n",
       "               'melancholy']),\n",
       "             ('sea', ['ocean']),\n",
       "             ('shore', ['coast', 'strand', 'bank', 'beach', '']),\n",
       "             ('snack', ['meal', 'eat']),\n",
       "             ('snake', ['serpent', 'ophidian']),\n",
       "             ('sniffles', ['pneumonia']),\n",
       "             ('sofa', ['couch', 'lounge']),\n",
       "             ('soon', ['immediately', 'promptly', 'straightaway']),\n",
       "             ('spouse', ['partner', 'mate', 'better_half']),\n",
       "             ('strong', ['powerful', 'forceful', 'super', 'potent']),\n",
       "             ('style', ['manner', 'mode', 'fashion', 'way']),\n",
       "             ('sweets', ['confectionery', 'dessert', 'confection']),\n",
       "             ('tasty',\n",
       "              ['delicious',\n",
       "               'delectable',\n",
       "               'luscious',\n",
       "               'pleasant-tasting',\n",
       "               'scrumptious',\n",
       "               'toothsome',\n",
       "               'yummy',\n",
       "               'mouth-watering',\n",
       "               'ambrosial',\n",
       "               'heavenly']),\n",
       "             ('tired', ['exhausted', 'drained']),\n",
       "             ('unfortunate',\n",
       "              ['tragic',\n",
       "               'woeful',\n",
       "               'grievous',\n",
       "               'wretched',\n",
       "               'miserable',\n",
       "               'awful',\n",
       "               'lamentable',\n",
       "               'regrettable',\n",
       "               'desperate',\n",
       "               'hopeless',\n",
       "               'disastrous']),\n",
       "             ('unhappy', ['miserable', 'suffering', 'wretched']),\n",
       "             ('villain', ['scoundrel', 'rascal']),\n",
       "             ('vocabulary', ['lexicon', 'wordbook', 'dictionary', 'glossary']),\n",
       "             ('want', ['crave', 'hunger', 'thirst', 'starve', 'lust']),\n",
       "             ('warm',\n",
       "              ['hot',\n",
       "               'fiery',\n",
       "               'flaming',\n",
       "               'heated',\n",
       "               'red-hot',\n",
       "               'sizzling',\n",
       "               'sensual',\n",
       "               'sultry',\n",
       "               'torrid',\n",
       "               'white-hot']),\n",
       "             ('well',\n",
       "              ['flourishing',\n",
       "               'robust',\n",
       "               'booming',\n",
       "               'prospering',\n",
       "               'prosperous',\n",
       "               'thriving'])])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "synonyms_dict = {}\n",
    "files = [file_synonyms_intensity,file_synonyms_exact]\n",
    "\n",
    "for each in files:\n",
    "    \n",
    "    # put the pos to the start of file\n",
    "    each.seek(0, os.SEEK_SET)\n",
    "\n",
    "    for line in each.read().split(\"\\n\"):\n",
    "        line_split = line.split()\n",
    "        master = line_split[0]\n",
    "        slave = line_split[1].split('/')\n",
    "        if not master in synonyms_dict:\n",
    "            synonyms_dict[master] = slave\n",
    "            \n",
    "ordered_synonyms_dict = collections.OrderedDict(sorted(synonyms_dict.items()))\n",
    "ordered_synonyms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/xd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('warm', 'NN')]\n",
      "[('good', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "document = \"warm\"\n",
    "print(nltk.pos_tag([document]))\n",
    "print(nltk.pos_tag(['good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': ['furious', 'enraged', 'outraged', 'aggrivated', 'irate', 'seething'], 'bad': ['awful', 'atrocious', 'abominable', 'dreadful', 'painful', 'terrible', 'unspeakable'], 'creative': ['ingenious', 'inventive', 'demiurgic', 'deviceful', 'innovational', 'innovative', 'innovatory', 'original', 'originative'], 'guilty': ['remorseful', 'sorrowful', 'unworthy'], 'happy': ['ecstatic', 'enraptured', 'rapturous', 'rapt', 'rhapsodic'], 'homogeneous': ['uniform', 'unvarying'], 'identical': ['same', 'indistinguishable'], 'necessary': ['essential', 'indispensable', 'vital', 'required'], 'new': ['modern', 'recent'], 'rational': ['logical', 'coherent', 'reasonable', 'sane'], 'reasonable': ['sensible'], 'strong': ['powerful', 'forceful', 'super', 'potent'], 'unhappy': ['miserable', 'suffering', 'wretched']}\n",
      "============================================\n",
      "{'afraid': 'NN', 'airplane': 'NN', 'ask': 'NN', 'auto': 'NN', 'baby': 'NN', 'bicycle': 'NN', 'boring': 'NN', 'cat': 'NN', 'child': 'NN', 'chuckle': 'NN', 'cloth': 'NN', 'clothes': 'NNS', 'confused': 'VBN', 'cry': 'NN', 'damp': 'NN', 'dinner': 'NN', 'dislike': 'NN', 'dollars': 'NNS', 'doze': 'NN', 'drizzle': 'NN', 'emphasis': 'NN', 'excited': 'VBN', 'faith': 'NN', 'father': 'NN', 'flower': 'NN', 'giggle': 'NN', 'harbor': 'NN', 'help': 'NN', 'hieroglyph': 'NN', 'honest': 'NN', 'house': 'NN', 'hungry': 'NN', 'incorrect': 'NN', 'indulge': 'NN', 'intelligent': 'NN', 'interesting': 'VBG', 'irritate': 'NN', 'jewel': 'NN', 'jog': 'NN', 'lad': 'NN', 'lady': 'NN', 'lake': 'NN', 'lazy': 'NN', 'like': 'IN', 'list': 'NN', 'love': 'NN', 'loyal': 'NN', 'market': 'NN', 'mend': 'NN', 'mesh': 'NN', 'monkey': 'NN', 'monument': 'NN', 'mother': 'NN', 'murder': 'NN', 'nap': 'NN', 'obsolete': 'NN', 'opposed': 'VBN', 'organized': 'VBN', 'package': 'NN', 'pain': 'NN', 'phone': 'NN', 'pony': 'NN', 'poorly': 'RB', 'portion': 'NN', 'railway': 'NN', 'rain': 'NN', 'rock': 'NN', 'sad': 'NN', 'sea': 'NN', 'shore': 'NN', 'snack': 'NN', 'snake': 'NN', 'sniffles': 'NNS', 'sofa': 'NN', 'soon': 'RB', 'spouse': 'NN', 'style': 'NN', 'sweets': 'NNS', 'tasty': 'NN', 'tired': 'VBN', 'unfortunate': 'NN', 'villain': 'NN', 'want': 'NN', 'warm': 'NN', 'well': 'RB'}\n"
     ]
    }
   ],
   "source": [
    "adj_ordered_synonyms_dict = {}\n",
    "not_adj = {}\n",
    "\n",
    "for (key,value) in ordered_synonyms_dict.items():\n",
    "    if nltk.pos_tag([key])[0][1] == \"JJ\":\n",
    "        adj_ordered_synonyms_dict[key] = value\n",
    "    else:\n",
    "        not_adj[key] = nltk.pos_tag([key])[0][1]\n",
    "        \n",
    "adj_ordered_synonyms_dict.pop('vocabulary')\n",
    "print(adj_ordered_synonyms_dict)\n",
    "print('============================================')\n",
    "print(not_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "model_class, tokenizer_class, shortcut, mask_token = RobertaForMaskedLM, RobertaTokenizer, 'roberta-base', '<mask>'\n",
    "model, tokenizer = model_class.from_pretrained(shortcut), tokenizer_class.from_pretrained(shortcut)\n",
    "models[shortcut] = (model, tokenizer, mask_token)\n",
    "\n",
    "model_class, tokenizer_class, shortcut, mask_token = RobertaForMaskedLM, RobertaTokenizer, 'roberta-large', '<mask>'\n",
    "model, tokenizer = model_class.from_pretrained(shortcut), tokenizer_class.from_pretrained(shortcut)\n",
    "models[shortcut] = (model, tokenizer, mask_token)\n",
    "\n",
    "model, tokenizer, mask_token = models['roberta-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    'John is * and Mary is also _ .',\n",
    "    'The synonyms of * is _ .'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pandas import Series,DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "John is angry and Mary is also _ .\n",
      "['angry', 'upset', 'furious', 'mad', 'worried', 'unhappy', 'annoyed', 'angered', 'frustrated', 'sad']\n",
      "tensor([0.7821, 0.0406, 0.0237, 0.0127, 0.0116, 0.0078, 0.0077, 0.0062, 0.0053,\n",
      "        0.0049])\n",
      "aggrivatedis not in list\n",
      "irateis not in list\n",
      "seethingis not in list\n",
      "{'furious': tensor(0.0237), 'enraged': tensor(0.0029), 'outraged': tensor(0.0020)}\n",
      "max_probe is: tensor(0.0237)\n",
      "the position of max probe is: 2\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is bad and Mary is also _ .\n",
      "['bad', 'evil', 'good', 'terrible', 'horrible', 'Bad', 'awful', 'naughty', 'corrupt', 'wicked']\n",
      "tensor([0.8633, 0.0506, 0.0143, 0.0066, 0.0029, 0.0024, 0.0024, 0.0023, 0.0022,\n",
      "        0.0021])\n",
      "atrociousis not in list\n",
      "abominableis not in list\n",
      "unspeakableis not in list\n",
      "{'awful': tensor(0.0024), 'dreadful': tensor(7.2874e-05), 'painful': tensor(1.5679e-05), 'terrible': tensor(0.0066)}\n",
      "max_probe is: tensor(0.0066)\n",
      "the position of max probe is: 3\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is creative and Mary is also _ .\n",
      "['creative', 'artistic', 'talented', 'beautiful', 'imaginative', 'passionate', 'spiritual', 'innovative', '', 'gifted']\n",
      "tensor([0.8772, 0.0193, 0.0054, 0.0045, 0.0036, 0.0034, 0.0023, 0.0022, 0.0019,\n",
      "        0.0016])\n",
      "demiurgicis not in list\n",
      "devicefulis not in list\n",
      "innovationalis not in list\n",
      "innovatoryis not in list\n",
      "originativeis not in list\n",
      "{'ingenious': tensor(4.0205e-05), 'inventive': tensor(0.0009), 'innovative': tensor(0.0022), 'original': tensor(5.5851e-05)}\n",
      "max_probe is: tensor(0.0022)\n",
      "the position of max probe is: 7\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is guilty and Mary is also _ .\n",
      "['guilty', 'involved', 'innocent', 'responsible', 'complicit', 'suspect', 'accused', 'implicated', 'convicted', '']\n",
      "tensor([0.9289, 0.0100, 0.0085, 0.0031, 0.0030, 0.0029, 0.0025, 0.0023, 0.0017,\n",
      "        0.0012])\n",
      "remorsefulis not in list\n",
      "sorrowfulis not in list\n",
      "{'unworthy': tensor(5.7357e-06)}\n",
      "max_probe is: tensor(5.7357e-06)\n",
      "20\n",
      "40\n",
      "80\n",
      "160\n",
      "320\n",
      "640\n",
      "1280\n",
      "the position of max probe is: 751\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is happy and Mary is also _ .\n",
      "['happy', '', 'pleased', 'happier', '.', 'smiling', 'satisfied', 'glad', 'excited', 'delighted']\n",
      "tensor([0.9026, 0.0143, 0.0132, 0.0081, 0.0063, 0.0060, 0.0041, 0.0033, 0.0015,\n",
      "        0.0015])\n",
      "enrapturedis not in list\n",
      "rapturousis not in list\n",
      "rhapsodicis not in list\n",
      "{'ecstatic': tensor(0.0006), 'rapt': tensor(2.9104e-06)}\n",
      "max_probe is: tensor(0.0006)\n",
      "20\n",
      "40\n",
      "the position of max probe is: 21\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is homogeneous and Mary is also _ .\n",
      "['so', '', '', 'hom', 'pure', '.', 'white', 'such', 'spherical', 'one']\n",
      "tensor([0.0756, 0.0463, 0.0329, 0.0287, 0.0279, 0.0252, 0.0221, 0.0175, 0.0165,\n",
      "        0.0164])\n",
      "unvaryingis not in list\n",
      "{'uniform': tensor(0.0039)}\n",
      "max_probe is: tensor(0.0039)\n",
      "20\n",
      "40\n",
      "the position of max probe is: 37\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is identical and Mary is also _ .\n",
      "['identical', 'similar', 'a', 'related', 'twin', 'the', '', 'twins', 'so', 'unique']\n",
      "tensor([0.8979, 0.0050, 0.0037, 0.0034, 0.0028, 0.0025, 0.0019, 0.0018, 0.0018,\n",
      "        0.0017])\n",
      "{'same': tensor(0.0004), 'indistinguishable': tensor(9.2665e-05)}\n",
      "max_probe is: tensor(0.0004)\n",
      "20\n",
      "40\n",
      "the position of max probe is: 38\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is necessary and Mary is also _ .\n",
      "['necessary', 'important', 'needed', 'required', 'indispensable', 'essential', 'desirable', 'good', 'sufficient', 'crucial']\n",
      "tensor([0.6890, 0.0845, 0.0670, 0.0287, 0.0157, 0.0096, 0.0027, 0.0027, 0.0026,\n",
      "        0.0026])\n",
      "{'essential': tensor(0.0096), 'indispensable': tensor(0.0157), 'vital': tensor(0.0015), 'required': tensor(0.0287)}\n",
      "max_probe is: tensor(0.0287)\n",
      "the position of max probe is: 3\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is new and Mary is also _ .\n",
      "['new', 'old', 'born', 'young', 'New', '', 'pregnant', 'alive', 'there', 'dead']\n",
      "tensor([0.8284, 0.0249, 0.0081, 0.0067, 0.0053, 0.0041, 0.0035, 0.0034, 0.0031,\n",
      "        0.0029])\n",
      "{'modern': tensor(0.0001), 'recent': tensor(0.0006)}\n",
      "max_probe is: tensor(0.0006)\n",
      "20\n",
      "40\n",
      "the position of max probe is: 39\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is rational and Mary is also _ .\n",
      "['rational', 'logical', '', 'so', '', 'a', 'intelligent', 'good', 'just', 'such']\n",
      "tensor([0.8579, 0.0133, 0.0041, 0.0039, 0.0039, 0.0039, 0.0031, 0.0028, 0.0028,\n",
      "        0.0027])\n",
      "{'logical': tensor(0.0133), 'coherent': tensor(6.8406e-05), 'reasonable': tensor(0.0021), 'sane': tensor(0.0016)}\n",
      "max_probe is: tensor(0.0133)\n",
      "the position of max probe is: 1\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is reasonable and Mary is also _ .\n",
      "['reasonable', 'sensible', 'rational', 'unreasonable', 'logical', 'respectful', 'good', 'thoughtful', 'fair', '']\n",
      "tensor([0.7699, 0.0152, 0.0059, 0.0057, 0.0055, 0.0052, 0.0044, 0.0042, 0.0040,\n",
      "        0.0038])\n",
      "{'sensible': tensor(0.0152)}\n",
      "max_probe is: tensor(0.0152)\n",
      "the position of max probe is: 1\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is strong and Mary is also _ .\n",
      "['strong', 'powerful', 'beautiful', '', 'good', 'stronger', 'courageous', 'weak', 'brave', 'active']\n",
      "tensor([0.8674, 0.0103, 0.0079, 0.0056, 0.0040, 0.0031, 0.0029, 0.0021, 0.0019,\n",
      "        0.0018])\n",
      "{'powerful': tensor(0.0103), 'forceful': tensor(0.0002), 'super': tensor(3.0289e-05), 'potent': tensor(9.8781e-06)}\n",
      "max_probe is: tensor(0.0103)\n",
      "the position of max probe is: 1\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "==========================================================\n",
      "John is unhappy and Mary is also _ .\n",
      "['unhappy', 'sad', 'upset', 'angry', 'miserable', 'depressed', 'disappointed', 'happy', 'worried', 'dissatisfied']\n",
      "tensor([0.7870, 0.0198, 0.0161, 0.0147, 0.0121, 0.0115, 0.0097, 0.0084, 0.0073,\n",
      "        0.0056])\n",
      "{'miserable': tensor(0.0121), 'suffering': tensor(0.0045), 'wretched': tensor(6.9012e-06)}\n",
      "max_probe is: tensor(0.0121)\n",
      "the position of max probe is: 4\n",
      "-----------------------------------------------------\n",
      "*****************************************************\n",
      "   count  position\n",
      "8      3         1\n",
      "0      1         2\n",
      "1      2         3\n",
      "9      1         4\n",
      "2      1         7\n",
      "4      1        21\n",
      "5      1        37\n",
      "6      1        38\n",
      "7      1        39\n",
      "3      1       751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='position'>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_dict = {}\n",
    "\n",
    "for (key,value) in adj_ordered_synonyms_dict.items():\n",
    "    for temp_num in range(2):\n",
    "        no_predict = 0\n",
    "        print(\"==========================================================\")\n",
    "        text = templates[0].replace('*',key)\n",
    "        print(text)\n",
    "\n",
    "        # module\n",
    "        if mask_token is not None:\n",
    "            text = text.replace(' _ ', ' %s ' % mask_token)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text, add_special_tokens=True))\n",
    "        tokens = ['*' if token in ['*', 'Ġ*'] else token for token in tokens]\n",
    "        marker = '*'\n",
    "        if marker in tokens:\n",
    "            assert tokens.count(marker) == 2, str(tokens)\n",
    "            p, h = [i for i, token in enumerate(tokens) if token == marker]\n",
    "            tokens = [token for token in tokens if token != marker]\n",
    "            h -= 1\n",
    "            print(tokens[p], tokens[h])\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        pred_idx = [i for i, token in enumerate(tokens) if token == mask_token] if mask_token is not None else [-1]\n",
    "        tokens = ['@' + token if not token.startswith('Ġ') and token not in ['<s>', '</s>', '<mask>'] else token.replace('Ġ', '') \n",
    "                  for token in tokens] \n",
    "        #print(tokens)\n",
    "\n",
    "        input_ids = torch.tensor([token_ids])\n",
    "        with torch.no_grad():\n",
    "            logits, attns = model(input_ids, output_attentions=True)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        for i in pred_idx:\n",
    "            top_probs, top_indexes = probs[0][i].topk(10)\n",
    "            top_tokens = tokenizer.convert_ids_to_tokens(top_indexes) \n",
    "\n",
    "\n",
    "            # remove G\n",
    "            ans = []\n",
    "            for word in top_tokens:\n",
    "                ans.append(word[1:])\n",
    "            print(ans)\n",
    "            print(top_probs)\n",
    "\n",
    "\n",
    "            # 找到每个同义词位于预测的第几个位置，先找到概率最大的，再去找最大的是预测的第几个\n",
    "            # 找到每个同义词对应的概率\n",
    "            word_probe = {}\n",
    "            for word in value:\n",
    "                word_id = tokenizer._convert_token_to_id('Ġ'+ word)\n",
    "                if (word_id != 3):\n",
    "                    word_probe[word] = probs[0][i][word_id]\n",
    "                else:\n",
    "                    print(word + 'is not in list')\n",
    "\n",
    "            print(word_probe)\n",
    "            \n",
    "            # 没有反义词在预测的结果里\n",
    "            # 保存计算结果的json文件中写为 -1\n",
    "            if (len(word_probe)==0):\n",
    "                no_predict = 1\n",
    "                break\n",
    "                \n",
    "            \n",
    "\n",
    "            # 找到同义词中概率最大的词和值\n",
    "            max_word = max(word_probe, key=word_probe.get)\n",
    "            max_probe = word_probe[max_word]\n",
    "\n",
    "            print('max_probe is: ',end='')\n",
    "            print(max_probe)\n",
    "\n",
    "            # 判断概率最大的同义词是预测的第几个\n",
    "            # 不要陷入死循环\n",
    "            # k的值不要超过 tensor 的长度\n",
    "            k = 10\n",
    "            probe_get = 0\n",
    "            while(probe_get == 0):\n",
    "                top_probs, top_indexes = probs[0][i].topk(k)\n",
    "                #print(top_probs)\n",
    "                if max_probe in top_probs:\n",
    "                    print('the position of max probe is: ',end='')\n",
    "                    print((top_probs == max_probe).nonzero().item())\n",
    "                    print('-----------------------------------------------------')\n",
    "\n",
    "                    probe_get = 1\n",
    "                else:\n",
    "                    k=k*2\n",
    "                    print(k)\n",
    "                    if (k >= list(probs[0][i].size())[0]):\n",
    "                        top_probs, top_indexes = probs[0][i].topk(probs[0][i].size())\n",
    "                        print(top_probs)\n",
    "                        print('the position of max probe is: ',end='')\n",
    "                        print((top_probs == max_probe).nonzero().item())\n",
    "                        print('-----------------------------------------------------')\n",
    "                        probe_get = 1\n",
    "\n",
    "\n",
    "            print('*****************************************************')\n",
    "\n",
    "\n",
    "            \n",
    "        # 没有预测结果\n",
    "        if(no_predict == 1):\n",
    "            # 写入结果文件\n",
    "            if (temp_num == 0):\n",
    "                result_dict[key] = [result_dict[key],-1]\n",
    "            else:\n",
    "                result_dict[key].append(-1)\n",
    "            break\n",
    "            \n",
    "            \n",
    "        #break\n",
    "        position = (top_probs == max_probe).nonzero().item()\n",
    "        if position not in count_dict:\n",
    "            count_dict[position] = 1\n",
    "        else:\n",
    "            count_dict[position]+=1\n",
    "        \n",
    "\n",
    "\n",
    "# 画图\n",
    "\n",
    "count_dict_keys = count_dict.keys()\n",
    "count_dict_values = count_dict.values()\n",
    "\n",
    "data = {\n",
    "    'count':Series(count_dict_values),\n",
    "    'position':Series(count_dict_keys)\n",
    "}\n",
    "\n",
    "df = DataFrame(data)\n",
    "df.sort_values(\"position\",inplace=True)\n",
    "print(df)\n",
    "# plt.bar(count_dict_keys,count_dict_values)\n",
    "df.plot(x='position',y='count',kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
