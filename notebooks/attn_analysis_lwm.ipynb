{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file_utils.py: default_cache_path = /raid/xd/.cache/torch/transformers\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "# from transformers.trainer_utils import EvaluationStrategy\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/roberta-large/resolve/main/config.json\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/roberta-large-config.json\n",
      "In cached_path: url_or_filename = https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/roberta-large-pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('roberta-large', cache_dir=cache_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', cache_dir=cache_dir)\n",
    "models['roberta-large'] = (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 't5-11b'\n",
    "proxies = {'http': '192.168.50.1:1081'}\n",
    "model = model11b = T5ForConditionalGeneration.from_pretrained(model_name, proxies=proxies)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-11b')\n",
    "tokenizer.decode_strip_special_tokens = MethodType(decode_strip_special_tokens, tokenizer)\n",
    "tokenizer.decode_old = MethodType(decode_old, tokenizer)\n",
    "\n",
    "models['t5-11b'] = model, tokenizer\n",
    "\n",
    "device_map = {0: list(range(0, 6)), 1: list(range(6, 15)), 2: list(range(15, 24))}\n",
    "model.parallelize(device_map)\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-xl'  # medium / large / xl\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPTNeoForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e885a76c2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"EleutherAI/gpt-neo-2.7B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTNeoForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPTNeoForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'roberta-large'\n",
    "# model_name = 'gpt2-xl'\n",
    "model_name = 'EleutherAI/gpt-neo-2.7B'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "model, tokenizer = models[model_name]\n",
    "\n",
    "masked_lm = tokenizer.mask_token is not None and len(tokenizer.additional_special_tokens) == 0\n",
    "if masked_lm:\n",
    "    mask_token = tokenizer.mask_token  # '<mask>' for roberta\n",
    "elif len(tokenizer.additional_special_tokens) > 0:\n",
    "    mask_token = tokenizer.additional_special_tokens[0]  # '<sxtra_id_0>' for t5\n",
    "else:\n",
    "    mask_token = ''  # for gpt2\n",
    "if masked_lm: nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59cd80813dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#vocab = list(string.ascii_uppercase) #+ ['_'] * 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#query_vocab = list(string.ascii_uppercase)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;31m#nrow是生成的行数，ncol是每组的字母数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "from random import choice, choices, shuffle, sample\n",
    "\n",
    "#vocab = list(string.ascii_uppercase) #+ ['_'] * 16\n",
    "vocab = list(string.digits)[1:]\n",
    "#query_vocab = list(string.ascii_uppercase)\n",
    "nrows, ncols = 8, 4 #nrow是生成的行数，ncol是每组的字母数\n",
    "has_query = False\n",
    "has_output = True\n",
    "def map_fn(x): return x.lower()\n",
    "\n",
    "for _ in range(nrows):\n",
    "    input_tokens = sample(vocab, ncols) #sample函数随机从vocab里选n个\n",
    "    #input_tokens = [ map_fn(choice(vocab)) for _ in range(ncols)]\n",
    "    i = random.randint(0, len(input_tokens) - 1)\n",
    "    input_tokens[i] = input_tokens[i] + '0'\n",
    "#     input_tokens[-1] = input_tokens[0]\n",
    "#    special = choice(vocab)\n",
    "#     input_tokens = [choice(vocab).lower()] * (ncols - 1) + [special]\n",
    "#     shuffle(input_tokens)\n",
    "    print(' '.join(input_tokens), end='')\n",
    "    if has_query:\n",
    "#         query_tokens = sample(input_tokens, ncols - 1)\n",
    "        query_tokens = input_tokens.copy()\n",
    "        i = random.randint(0, len(input_tokens) - 1)\n",
    "#         query_token = input_tokens[i]\n",
    "        query_tokens[i] = choice(vocab)\n",
    "#         query_tokens = [t.lower() for t in query_tokens]\n",
    "        print(',', ' '.join(query_tokens), end='')\n",
    "    print(' -> ', end='')\n",
    "    if has_output:\n",
    "        #output_tokens = input_tokens[i][:-1] #.upper()\n",
    "#        output_tokens = special.lower()\n",
    "#         output_tokens = choice(input_tokens)\n",
    "#         output_tokens = list(set(input_tokens) - set(query_tokens))\n",
    "#         output_tokens = map(map_fn, input_tokens)\n",
    "#         output_tokens = reverse(input_tokens)\n",
    "#         output_tokens = query_tokens[i].lower()\n",
    "        print(''.join(output_tokens), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from attattr\n",
    "def scaled_input(emb, num_points, baseline=None):\n",
    "    # shape of emb: (bsz, num_head, seq_len, seq_len)\n",
    "    assert emb.size(0) == 1\n",
    "    if baseline is None: baseline = torch.zeros_like(emb)   \n",
    "    step = (emb - baseline) / num_points\n",
    "    #res = torch.cat([baseline + step * i for i in range(num_points)], dim=0) # orig\n",
    "    res = torch.cat([baseline + step * (i+1) for i in range(num_points)], dim=0)  # revised\n",
    "    return res, step\n",
    "\n",
    "# from https://discuss.pytorch.org/t/get-top-k-indices-values-of-all-rows/89354\n",
    "def unravel_index(index, shape):\n",
    "    out = []\n",
    "    for dim in reversed(shape):\n",
    "        out.append(index % dim)\n",
    "        index = index // dim\n",
    "    r = tuple(reversed(out))\n",
    "    return torch.cat([i.unsqueeze(-1) for i in r], dim=-1).cpu().tolist() if type(index) in [torch.Tensor] else r\n",
    "\n",
    "def numpy(a, decimals=4): return a.detach().cpu().numpy().round(decimals)\n",
    "\n",
    "def h2topk(h, k=4, return_probs=True):\n",
    "    if not hasattr(h2topk, 'ln') or h2topk.ln.normalized_shape[0] != h.size(-1):\n",
    "        h2topk.ln = nn.LayerNorm(h.size(-1))\n",
    "#     r = model.lm_head(h2topk.ln(h))\n",
    "    r = model.lm_head(h)\n",
    "    if return_probs: r = r.softmax(-1)\n",
    "    return r.topk(k, dim=-1) if k > 0 else r\n",
    "\n",
    "def globalize(tensor):\n",
    "    if tensor.dim() == 4: return tensor  # global attention\n",
    "    assert tensor.dim() == 5, str(tensor.dim())\n",
    "    assert tensor.size(1) == 1, str(tensor.size(1))  # num_blocks\n",
    "    seq_len = tensor.size(3)\n",
    "    return tensor.squeeze(1)[:, :, :, -seq_len:]  # (bsz, num_blocks, H, seq_len, block_len) -> (bsz, H, seq_len, seq_len)\n",
    "\n",
    "def show_topk(values, indices, values_fn=numpy, indices_fn=numpy):\n",
    "    return dict(OrderedDict(zip(indices_fn(indices), values_fn(values))))\n",
    "\n",
    "def append_tokens_to_positions(position_tensor):\n",
    "    positions = numpy(position_tensor)\n",
    "    return ['%d %s' % (p, tokens[p]) for p in positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    " '''\n",
    "A H S -> A\n",
    "S D N -> S\n",
    "U D B -> U\n",
    "Z G M ->''',\n",
    " '''\n",
    "A H S H -> a\n",
    "S D N F -> s\n",
    "U D B S -> u\n",
    "Z G M E ->''',  # [(22, 15, 0.223), (21, 13, 0.197), (25, 0, 0.16), (23, 11, 0.144), (20, 19, 0.14)]\n",
    " '''\n",
    "A H S H -> A\n",
    "S D N F -> S\n",
    "U D B S -> U\n",
    "Z G M E ->''',  # [(22, 15, 0.188), (21, 13, 0.113), (23, 11, 0.106), (21, 19, 0.09), (24, 9, 0.072)]\n",
    "'''\n",
    "o t j -> O\n",
    "r n k -> R\n",
    "n m c -> N\n",
    "m g d -> M\n",
    "g c j -> G\n",
    "x z o -> X\n",
    "i c p -> I\n",
    "u a o ->''',  # [(22, 15, 0.119), (23, 11, 0.109), (21, 13, 0.101), (29, 0, 0.062), (24, 11, 0.044)]\n",
    " '''\n",
    "A L A -> l\n",
    "F B F -> b\n",
    "M A M -> a\n",
    "O W O -> w\n",
    "W Y W -> y\n",
    "D G D ->''',  # [(26, 16, 0.109), (22, 24, 0.07), (25, 0, 0.063), (22, 15, 0.052), (21, 16, 0.045)]\n",
    " '''\n",
    "N S N -> S\n",
    "N K N -> K\n",
    "O M O -> M\n",
    "T V T ->''',  # [(26, 16, 0.062), (21, 13, 0.052), (22, 24, 0.049), (25, 0, 0.044), (22, 15, 0.041)]\n",
    " '''\n",
    "n s n -> S\n",
    "n k n -> K\n",
    "o m o -> M\n",
    "f b f -> B\n",
    "m a m -> A\n",
    "t v t ->''',  # [(26, 14, 0.078), (21, 4, 0.044), (17, 1, 0.042), (21, 13, 0.041), (25, 15, 0.03)] *******\n",
    " '''\n",
    "n s n -> s\n",
    "n k n -> k\n",
    "o m o -> m\n",
    "f b f -> b\n",
    "m a m -> a\n",
    "t v t ->''',  # [(22, 24, 0.068), (24, 9, 0.051), (17, 1, 0.04), (26, 16, 0.037), (22, 15, 0.032)]\n",
    "'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E\n",
    "N I B -> I\n",
    "K U I ->''',  # [(21, 4, 0.046), (26, 16, 0.043), (22, 24, 0.03), (16, 24, 0.029), (29, 2, 0.029)]\n",
    "             #  [(21, 4, 0.056), (26, 16, 0.05), (22, 24, 0.049), (29, 2, 0.03), (16, 24, 0.029)]\n",
    "'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H ->''',\n",
    "'''\n",
    "Q N J P, Q J J P -> J\n",
    "B V I F, B V I W -> W\n",
    "J C B T, J H B T -> H\n",
    "Q I M G, Q M M G -> M\n",
    "J D Q K, J U Q K -> U\n",
    "Q M A S, Q Q A S -> Q\n",
    "J E E J, J E V J -> V\n",
    "V T H V, V T S V -> S\n",
    "Z R N C, Z R N C -> N\n",
    "A H Z G, A H O G ->''',\n",
    "'''\n",
    "n d d d -> n\n",
    "f f d f -> d\n",
    "e b e e -> b\n",
    "s q s s -> q\n",
    "d d d o -> o\n",
    "e c e e ->'''\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {\n",
    "    'A B C -> B':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H ->''',\n",
    "    'A B C -> A':'''\n",
    "M T L -> M\n",
    "X I J -> X\n",
    "T D U -> T\n",
    "K L H -> K\n",
    "L C V -> L\n",
    "J Y D -> J\n",
    "A K G -> A\n",
    "V E H ->''',\n",
    "    'A B A -> B': '''\n",
    "N S N -> S\n",
    "N K N -> K\n",
    "O M O -> M\n",
    "T V T ->''',\n",
    "    'A B C -> C':'''\n",
    "Z J V -> V\n",
    "H V G -> G\n",
    "H X L -> L\n",
    "S X M -> M\n",
    "R N Y -> Y\n",
    "Z D A -> A\n",
    "V W M -> M\n",
    "D T C ->''',\n",
    "    \n",
    "    'A B C D -> A': '''\n",
    "A H S H -> A\n",
    "S D N F -> S\n",
    "U D B S -> U\n",
    "Z G M E ->''',\n",
    "    'A B C D -> a': '''\n",
    "A H S H -> a\n",
    "S D N F -> s\n",
    "U D B S -> u\n",
    "Z G M E ->''',\n",
    "    'a b c -> A':'''\n",
    "o t j -> O\n",
    "r n k -> R\n",
    "n m c -> N\n",
    "m g d -> M\n",
    "g c j -> G\n",
    "x z o -> X\n",
    "i c p -> I\n",
    "u a o ->''',\n",
    "    'A B C -> a':'''\n",
    "D Q H -> d\n",
    "I J L -> i\n",
    "O V R -> o\n",
    "T F E -> t\n",
    "Q N R -> q\n",
    "W Q B -> w\n",
    "Z H J -> z\n",
    "W V Y ->''',\n",
    "    'A B C -> b':'''\n",
    "T N X -> n\n",
    "D E U -> e\n",
    "U R Q -> r\n",
    "C T B -> t\n",
    "X B F -> b\n",
    "Q G V ->''',\n",
    "    'A B C -> c':'''\n",
    "P Q Z -> z\n",
    "C E E -> e\n",
    "U G H -> h\n",
    "X B F -> f\n",
    "K P Y -> y\n",
    "A M A -> a\n",
    "K E T ->''',\n",
    "    'A B A -> b': '''\n",
    "A L A -> l\n",
    "F B F -> b\n",
    "M A M -> a\n",
    "O W O -> w\n",
    "W Y W -> y\n",
    "D G D ->''',\n",
    "    'a b a -> B': '''\n",
    "n s n -> S\n",
    "n k n -> K\n",
    "o m o -> M\n",
    "f b f -> B\n",
    "m a m -> A\n",
    "t v t ->''',\n",
    "    'a b a -> b': '''\n",
    "n s n -> s\n",
    "n k n -> k\n",
    "o m o -> m\n",
    "f b f -> b\n",
    "m a m -> a\n",
    "t v t ->''',\n",
    "    'A B C -> B 2':'''\n",
    "M T L -> T\n",
    "X I J -> I\n",
    "T D U -> D\n",
    "K L H -> L\n",
    "L C V -> C\n",
    "J Y D -> Y\n",
    "A K G -> K\n",
    "V E H -> E\n",
    "N I B -> I\n",
    "K U F ->''',# 答案错误\n",
    "    'Q N J P, Q J J P -> J':'''\n",
    "Q N J P, Q J J P -> J\n",
    "B V I F, B V I W -> W\n",
    "J C B T, J H B T -> H\n",
    "Q I M G, Q M M G -> M\n",
    "J D Q K, J U Q K -> U\n",
    "Q M A S, Q Q A S -> Q\n",
    "J E E J, J E V J -> V\n",
    "V T H V, V T S V -> S\n",
    "Z R N C, Z R N C -> N\n",
    "A H Z G, A H O G ->''',\n",
    "    'a b b b -> a':'''\n",
    "n d d d -> n\n",
    "f f d f -> d\n",
    "e b e e -> b\n",
    "s q s s -> q\n",
    "d d d o -> o\n",
    "e c e e ->''',\n",
    "    'N J , J J  -> J':'''\n",
    "I F, I W -> W\n",
    "C B, H B -> H\n",
    "A G, M G -> M\n",
    "J D, J U -> U\n",
    "M Q, K Q -> K\n",
    "E C, L C ->''', \n",
    "    'Q N J , Q J J -> J':'''\n",
    "Q N J, Q J J -> J\n",
    "V I F, V I W -> W\n",
    "J C B, J H B -> H\n",
    "Q I M, Q M M -> M\n",
    "D Q K, U Q K -> U\n",
    "Q M A, Q Q A -> Q\n",
    "J E E, J E V -> V\n",
    "H Z G, H O G ->''',\n",
    "    'G L C, G L -> C':'''\n",
    "G L C, G L -> C\n",
    "Y P J, P Y -> J\n",
    "E S A, A S -> E\n",
    "U P W, P U -> W\n",
    "Z Q J, Z J -> Q\n",
    "C K Z, Z C -> K\n",
    "B L M, L M ->''',\n",
    "    'Z Y, y -> z':'''\n",
    "Z Y, y -> z\n",
    "K B, b -> k\n",
    "N E, e -> n\n",
    "J S, j -> s\n",
    "O W, o -> w\n",
    "F R, f -> r\n",
    "J S, s -> j\n",
    "N O, o -> n\n",
    "P R, p ->''',\n",
    "    '1 70 4 -> 7':'''\n",
    "1 70 4 -> 7\n",
    "4 7 20 -> 2\n",
    "60 4 8 -> 6\n",
    "50 9 3 -> 5\n",
    "8 30 6 -> 3\n",
    "7 6 90 -> 9\n",
    "5 2 10 -> 1\n",
    "1 80 3 ->''', #答案错误\n",
    "    '10 5 6 -> 1':'''\n",
    "10 5 6 -> 1\n",
    "4 5 90 -> 9\n",
    "40 9 5 -> 4\n",
    "4 7 80 -> 8\n",
    "3 4 60 -> 6\n",
    "3 4 80 -> 8\n",
    "6 70 4 -> 7\n",
    "5 2 90 ->''',\n",
    "    '6 7 30 1 -> 3':'''\n",
    "4 5 9 70 -> 7\n",
    "50 1 7 9 -> 5\n",
    "2 90 5 4 -> 9\n",
    "10 9 7 5 -> 1\n",
    "5 9 60 1 -> 6\n",
    "30 8 7 2 -> 3\n",
    "1 90 2 7 -> 9\n",
    "2 5 9 40 ->''',#此类答案均错误\n",
    "    'b 1 g t -> 1':'''\n",
    "b 1 g t -> 1\n",
    "v p 3 y -> 3\n",
    "u 2 a h -> 2\n",
    "m d j 5 -> 5\n",
    "t o s 6 -> 6\n",
    "9 v i q -> 9\n",
    "m 5 p w ->''',\n",
    "    'b k d e -> k':'''\n",
    "a 1 c d -> 1\n",
    "b k d e -> k\n",
    "d e q g -> q\n",
    "f g h p -> p\n",
    "o p t r -> t\n",
    "h y j k ->''',\n",
    "}\n",
    "# ABC->A\n",
    "# ABC->B\n",
    "# ABC->C\n",
    "# ABC->a\n",
    "# ABC->b\n",
    "# ABC->c\n",
    "# a 1 c d -> 1\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7d53e7a849e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a b a -> b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_token' is not defined"
     ]
    }
   ],
   "source": [
    "task_name = 'a b a -> b'\n",
    "text = texts[task_name]\n",
    "_text = text.replace('_', mask_token).rstrip()\n",
    "print(_text)\n",
    "\n",
    "if masked_lm:\n",
    "    print(_text, ['%s %.3f' % (i['token_str'], i['score']) for i in nlp(_text)])\n",
    "    print(tokenizer.tokenize(_text))\n",
    "else:\n",
    "    inputs = tokenizer.encode_plus(_text, return_tensors='pt')\n",
    "    inputs = prepare_inputs(inputs, model.device)\n",
    "#     max_length = 1 + (inputs['input_ids'].size(1) if mask_token == '' else 0)\n",
    "#     with torch.no_grad(): outputs = model.generate(**inputs, max_length=max_length, top_k=1)\n",
    "#     print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "    for block in model.transformer.h:\n",
    "        block.h_in, block.attn_out, block.h_mid = None, None, None\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_hidden_states=True)\n",
    "    logits = outputs.logits #if hasattr(outputs, 'logits') else outputs[0]\n",
    "    values, indices = logits[0, -1].softmax(-1).topk(5)\n",
    "    dict(OrderedDict(zip(tokenizer.convert_ids_to_tokens(indices), numpy(values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_heads, all_attrs, all_attns = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_heads = {}#http://octa:8890/notebooks/notebooks/attn_analysis_lwm.ipynb#{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.0962e-05, 9.1244e-04, 1.9195e-04, 1.3151e-02, 6.0786e-01, 7.4873e-01,\n",
       "        8.6274e-01, 9.4092e-01, 9.7023e-01, 9.7745e-01], device='cuda:1',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 1/22 [00:00<00:11,  1.90it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.3904e-06, 5.7904e-03, 1.9582e-02, 5.6671e-02, 1.0602e-01, 8.9333e-01,\n",
       "        9.8516e-01, 9.7900e-01, 9.7842e-01, 9.7745e-01], device='cuda:1',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 2/22 [00:01<00:12,  1.55it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9793, 0.9809, 0.9778, 0.9762, 0.9749, 0.9753, 0.9762, 0.9770, 0.9774,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▎        | 3/22 [00:01<00:12,  1.48it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8888, 0.9551, 0.9770, 0.9857, 0.9874, 0.9869, 0.9852, 0.9829, 0.9807,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 4/22 [00:02<00:12,  1.47it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9453, 0.9590, 0.9658, 0.9698, 0.9725, 0.9743, 0.9756, 0.9764, 0.9770,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 5/22 [00:03<00:11,  1.47it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9800, 0.9809, 0.9814, 0.9817, 0.9816, 0.9813, 0.9807, 0.9799, 0.9788,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 6/22 [00:03<00:10,  1.50it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9123, 0.9294, 0.9433, 0.9542, 0.9624, 0.9684, 0.9725, 0.9752, 0.9768,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 7/22 [00:04<00:09,  1.53it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8757, 0.9141, 0.9396, 0.9552, 0.9646, 0.9701, 0.9731, 0.9755, 0.9770,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 8/22 [00:05<00:08,  1.56it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8690, 0.9077, 0.9308, 0.9450, 0.9545, 0.9612, 0.9663, 0.9705, 0.9737,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 9/22 [00:05<00:08,  1.60it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8984, 0.9335, 0.9519, 0.9623, 0.9685, 0.9721, 0.9742, 0.9753, 0.9766,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 10/22 [00:06<00:07,  1.65it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6764, 0.8060, 0.8451, 0.8628, 0.8817, 0.9033, 0.9270, 0.9505, 0.9675,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 11/22 [00:06<00:06,  1.71it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7747, 0.8513, 0.9023, 0.9326, 0.9503, 0.9609, 0.9676, 0.9721, 0.9753,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 12/22 [00:07<00:05,  1.76it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2479, 0.3971, 0.5571, 0.7034, 0.8181, 0.8953, 0.9393, 0.9612, 0.9717,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 13/22 [00:07<00:04,  1.83it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6850, 0.7715, 0.8371, 0.8856, 0.9202, 0.9435, 0.9585, 0.9679, 0.9738,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 14/22 [00:08<00:04,  1.90it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9074, 0.9206, 0.9321, 0.9422, 0.9509, 0.9584, 0.9646, 0.9697, 0.9740,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 15/22 [00:08<00:03,  1.97it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9531, 0.9574, 0.9612, 0.9646, 0.9675, 0.9701, 0.9724, 0.9743, 0.9760,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 16/22 [00:09<00:02,  2.06it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9573, 0.9602, 0.9630, 0.9656, 0.9680, 0.9702, 0.9723, 0.9742, 0.9759,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 17/22 [00:09<00:02,  2.12it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9477, 0.9536, 0.9585, 0.9627, 0.9662, 0.9691, 0.9717, 0.9739, 0.9758,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 18/22 [00:10<00:01,  2.21it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9738, 0.9743, 0.9748, 0.9753, 0.9757, 0.9761, 0.9765, 0.9768, 0.9772,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 19/22 [00:10<00:01,  2.30it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9727, 0.9733, 0.9739, 0.9745, 0.9750, 0.9756, 0.9761, 0.9765, 0.9770,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 20/22 [00:10<00:00,  2.42it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9752, 0.9756, 0.9759, 0.9763, 0.9766, 0.9768, 0.9771, 0.9773, 0.9774,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 21/22 [00:11<00:00,  2.54it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9788, 0.9786, 0.9784, 0.9783, 0.9781, 0.9780, 0.9779, 0.9777, 0.9776,\n",
       "        0.9774], device='cuda:1', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:11<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(_text, return_tensors='pt')\n",
    "inputs = prepare_inputs(inputs, model.device)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions #if hasattr(outputs, 'attentions') else outputs[-1]\n",
    "L, H = len(attentions), attentions[0].size(1)\n",
    "logits = outputs.logits #if hasattr(outputs, 'logits') else outputs[0]\n",
    "pred_label = logits[0, -1].argmax().item()\n",
    "attns = torch.cat([globalize(a) for a in attentions])\n",
    "\n",
    "# layer_range = (15, 41)  # gpt2-xl, 48L\n",
    "# layer_range = (10, 28)  # gpt-neo-2.7B, 32L\n",
    "# layer_range = (8, 21)  # gpt-neo-1.3B, 24L\n",
    "layer_range = (0, 22)\n",
    "attrs = []\n",
    "for i in tqdm(range(*layer_range)):\n",
    "    attn = attentions[i]\n",
    "    scaled_attn, step = scaled_input(attn, 10) # 5\n",
    "    _ = scaled_attn.requires_grad_(True)\n",
    "    \n",
    "    attn_module = model.transformer.h[i].attn\n",
    "    if hasattr(attn_module, 'attention'): attn_module = attn_module.attention  # for gpt-neo\n",
    "    attn_module.w = scaled_attn\n",
    "    try: outputs = model(**inputs)\n",
    "    finally: attn_module.w = None\n",
    "        \n",
    "#     model.transformer.h[layer].exit = True\n",
    "#     try:\n",
    "#         attention = model.transformer(**inputs)\n",
    "#     finally:\n",
    "#         attn_module.w = None\n",
    "#         model.transformer.h[layer].exit = None\n",
    "#     y = globalize(attention)[:, head, src, tgt]\n",
    "    \n",
    "    logits = outputs.logits #if hasattr(outputs, 'logits') else outputs[0]\n",
    "    #y = logits[:, -1, pred_label]  #　orig\n",
    "    y = logits.softmax(-1)[:, -1, pred_label]  # revised\n",
    "    y\n",
    "    attn_grad = torch.autograd.grad(torch.unbind(y), scaled_attn)[0]\n",
    "    attn_grad = attn_grad.sum(dim=0, keepdim=True) # (bsz, H, qlen, klen) -> (1, H, qlen, klen)\n",
    "    attr = attn_grad * step\n",
    "    attrs.append(attr.data)\n",
    "attrs = torch.cat([globalize(a) for a in attrs])\n",
    "all_attns[task_name] = attns\n",
    "all_attrs[task_name] = attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1, 'Z') (2, 'J') (3, 'V') (4, '->') (5, 'V') \n",
      "(7, 'H') (8, 'V') (9, 'G') (10, '->') (11, 'G') \n",
      "(13, 'H') (14, 'X') (15, 'L') (16, '->') (17, 'L') \n",
      "(19, 'S') (20, 'X') (21, 'M') (22, '->') (23, 'M') \n",
      "(25, 'R') (26, 'N') (27, 'Y') (28, '->') (29, 'Y') \n",
      "(31, 'Z') (32, 'D') (33, 'A') (34, '->') (35, 'A') \n",
      "(37, 'V') (38, 'W') (39, 'M') (40, '->') (41, 'M') \n",
      "(43, 'D') (44, 'T') (45, 'C') (46, '->') "
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.replace('Ġ', '').replace('Ċ', '^') for token in tokenizer.tokenize(_text)]\n",
    "for i, token in enumerate(tokens):\n",
    "    if token in ['Ċ', '^']: print()\n",
    "    else: print((i, token), end=' ')\n",
    "seq_len = len(tokens)\n",
    "answer = tokens[-1]\n",
    "tgt = [i for i, token in enumerate(tokens[:-1]) if token.lower() == answer.lower()][-1]\n",
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-8\t0.1958\t [([0, 35], 0.1217), ([0, 41], 0.0335), ([0, 17], 0.0149), ([0, 23], 0.0145), ([0, 29], 0.0112)]\n",
      "12-16\t0.1604\t [([0, 35], 0.0651), ([0, 41], 0.0361), ([0, 29], 0.0299), ([0, 17], 0.0148), ([0, 23], 0.0145)]\n",
      "12-18\t0.1286\t [([0, 35], 0.0723), ([0, 5], 0.0206), ([0, 17], 0.015), ([0, 23], 0.0112), ([0, 29], 0.0094)]\n",
      "10-0\t0.1249\t [([0, 46], 0.1226), ([0, 34], 0.0012), ([0, 0], 0.0006), ([0, 16], 0.0002), ([0, 10], 0.0002)]\n",
      "13-2\t0.0900\t [([0, 45], 0.09), ([0, 33], 0.0), ([0, 43], 0.0), ([0, 42], 0.0), ([0, 32], 0.0)]\n",
      "12-1\t0.0679\t [([0, 17], 0.0243), ([0, 29], 0.0136), ([0, 23], 0.0124), ([0, 35], 0.0103), ([0, 11], 0.0073)]\n",
      "13-12\t0.0483\t [([0, 45], 0.0463), ([0, 43], 0.0016), ([0, 37], 1e-04), ([0, 39], 1e-04), ([0, 42], 1e-04)]\n",
      "12-5\t0.0383\t [([0, 35], 0.0107), ([0, 23], 0.0082), ([0, 11], 0.0074), ([0, 17], 0.0065), ([0, 29], 0.0055)]\n",
      "12-2\t0.0258\t [([0, 17], 0.0091), ([0, 35], 0.0085), ([0, 23], 0.0048), ([0, 11], 0.0024), ([0, 5], 0.001)]\n",
      "14-6\t0.0252\t [([0, 11], 0.0065), ([0, 35], 0.0063), ([0, 23], 0.0048), ([0, 17], 0.0039), ([0, 29], 0.0037)]\n",
      "12-6\t0.0250\t [([0, 17], 0.0076), ([0, 41], 0.0062), ([0, 23], 0.0048), ([0, 29], 0.0035), ([0, 11], 0.0029)]\n",
      "13-5\t0.0229\t [([0, 45], 0.0175), ([0, 46], 0.0052), ([0, 39], 1e-04), ([0, 43], 1e-04), ([0, 40], 0.0)]\n",
      "10-1\t0.0208\t [([0, 35], 0.0076), ([0, 0], 0.0043), ([0, 17], 0.0033), ([0, 5], 0.0033), ([0, 11], 0.0024)]\n",
      "13-7\t0.0196\t [([0, 45], 0.01), ([0, 44], 0.0057), ([0, 43], 0.0018), ([0, 38], 0.0011), ([0, 46], 0.0009)]\n",
      "13-18\t0.0193\t [([0, 45], 0.019), ([0, 39], 0.0002), ([0, 41], 0.0), ([0, 43], 0.0), ([0, 38], 0.0)]\n",
      "10-2\t0.0141\t [([0, 17], 0.0052), ([0, 35], 0.0042), ([0, 23], 0.0026), ([0, 29], 0.0011), ([0, 11], 0.001)]\n",
      "13-3\t0.0129\t [([0, 45], 0.0093), ([0, 43], 0.0034), ([0, 41], 1e-04), ([0, 39], 1e-04), ([0, 37], 0.0)]\n",
      "10-16\t0.0120\t [([0, 35], 0.0065), ([0, 17], 0.0025), ([0, 23], 0.002), ([0, 29], 0.0007), ([0, 11], 0.0003)]\n",
      "11-17\t0.0111\t [([0, 45], 0.0055), ([0, 44], 0.0025), ([0, 43], 0.0016), ([0, 38], 0.0008), ([0, 37], 0.0008)]\n",
      "12-4\t0.0109\t [([0, 17], 0.0027), ([0, 35], 0.0026), ([0, 11], 0.0022), ([0, 5], 0.0021), ([0, 23], 0.0013)]\n",
      "11-9\t0.0105\t [([0, 43], 0.006), ([0, 44], 0.0013), ([0, 37], 0.0013), ([0, 31], 0.001), ([0, 45], 0.0009)]\n",
      "12-12\t0.0102\t [([0, 46], 0.0027), ([0, 40], 0.0023), ([0, 34], 0.0019), ([0, 10], 0.0018), ([0, 22], 0.0015)]\n",
      "14-11\t0.0102\t [([0, 11], 0.0035), ([0, 35], 0.0024), ([0, 29], 0.0018), ([0, 17], 0.0016), ([0, 23], 0.0008)]\n",
      "12-15\t0.0100\t [([0, 36], 0.0031), ([0, 18], 0.0022), ([0, 24], 0.0021), ([0, 12], 0.0016), ([0, 30], 0.001)]\n",
      "13-1\t0.0096\t [([0, 41], 0.0043), ([0, 35], 0.0032), ([0, 43], 0.0009), ([0, 23], 0.0007), ([0, 29], 0.0005)]\n",
      "12-13\t0.0094\t [([0, 44], 0.003), ([0, 38], 0.002), ([0, 43], 0.0016), ([0, 19], 0.0014), ([0, 31], 0.0014)]\n",
      "10-13\t0.0092\t [([0, 17], 0.0028), ([0, 5], 0.0021), ([0, 0], 0.0017), ([0, 11], 0.0015), ([0, 35], 0.0011)]\n",
      "6-8\t0.0086\t [([0, 28], 0.002), ([0, 34], 0.0018), ([0, 46], 0.0018), ([0, 10], 0.0015), ([0, 16], 0.0015)]\n",
      "14-0\t0.0084\t [([0, 45], 0.0067), ([0, 46], 0.0012), ([0, 0], 0.0003), ([0, 36], 1e-04), ([0, 35], 1e-04)]\n",
      "20-19\t0.0081\t [([0, 45], 0.0079), ([0, 9], 1e-04), ([0, 32], 1e-04), ([0, 7], 0.0), ([0, 11], 0.0)]\n",
      "12-19\t0.0075\t [([0, 35], 0.0026), ([0, 0], 0.0022), ([0, 36], 0.002), ([0, 46], 0.0003), ([0, 24], 0.0002)]\n",
      "12-9\t0.0073\t [([0, 36], 0.0032), ([0, 0], 0.002), ([0, 12], 0.0015), ([0, 46], 0.0004), ([0, 34], 0.0002)]\n",
      "19-8\t0.0073\t [([0, 45], 0.0073), ([0, 33], 0.0), ([0, 40], 0.0), ([0, 43], 0.0), ([0, 35], 0.0)]\n",
      "15-8\t0.0070\t [([0, 45], 0.0066), ([0, 44], 1e-04), ([0, 39], 1e-04), ([0, 43], 1e-04), ([0, 38], 1e-04)]\n",
      "13-11\t0.0065\t [([0, 45], 0.0032), ([0, 41], 0.0014), ([0, 46], 0.0012), ([0, 40], 0.0005), ([0, 35], 0.0002)]\n",
      "13-8\t0.0060\t [([0, 46], 0.0023), ([0, 41], 0.0015), ([0, 42], 0.0008), ([0, 35], 0.0008), ([0, 34], 0.0005)]\n",
      "17-17\t0.0059\t [([0, 45], 0.0059), ([0, 41], 0.0), ([0, 34], 0.0), ([0, 42], 0.0), ([0, 23], 0.0)]\n",
      "12-7\t0.0059\t [([0, 41], 0.0023), ([0, 35], 0.002), ([0, 29], 0.0007), ([0, 11], 0.0005), ([0, 17], 0.0004)]\n",
      "16-15\t0.0055\t [([0, 45], 0.0045), ([0, 46], 0.0006), ([0, 33], 0.0003), ([0, 32], 1e-04), ([0, 39], 1e-04)]\n"
     ]
    }
   ],
   "source": [
    "ki = [41, 35, 29, 23, 17, 11, 5]\n",
    "tgt_i = 22 # 是要找箭头关注变化的位置的头\n",
    "_attrs = attrs #/ attrs.view(attrs.size(0), -1).norm(dim=1).view(attrs.size(0), 1, 1, 1)\n",
    "_attrs = _attrs[:, :, -1:, :]#tgt_i:tgt_i+1]\n",
    "#values, indices = _attrs.view(_attrs.size(0), H, -1).topk(1, dim=-1)\n",
    "values, indices = _attrs.view(_attrs.size(0), H, -1).topk(5, dim=-1)\n",
    "val, ind = values.sum(dim=-1).view(-1).topk(50)\n",
    "val, ind = numpy(val), unravel_index(ind, values.size()[:-1])\n",
    "\n",
    "top_heads = []\n",
    "# for (l, h), v in zip(ind, val):\n",
    "#     top_heads.append((l, h, v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h])))))\n",
    "#     if l<2: continue\n",
    "#     print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1])\n",
    "    #if l < 2: continue\n",
    "   # print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1])#, attns[l, h, -1, tgt_i].item())\n",
    "for (l, h), v in zip(ind, val):\n",
    "#     top_heads[(l, h)] = (v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h]))))\n",
    "    top_heads.append((l, h, v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h])))))\n",
    "    if l <5: continue\n",
    "    print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1])#, attns[l, h, -1, tgt_i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-2\t0.0900\t [([0, 0], 0.09)] 0.9725266098976135\n",
      "13-12\t0.0463\t [([0, 0], 0.0463)] 0.5863183736801147\n",
      "13-18\t0.0190\t [([0, 0], 0.019)] 0.8204120993614197\n",
      "13-5\t0.0175\t [([0, 0], 0.0175)] 0.8392573595046997\n",
      "13-7\t0.0100\t [([0, 0], 0.01)] 0.31353142857551575\n",
      "13-3\t0.0093\t [([0, 0], 0.0093)] 0.4202425181865692\n",
      "20-19\t0.0079\t [([0, 0], 0.0079)] 0.4849521517753601\n",
      "19-8\t0.0073\t [([0, 0], 0.0073)] 0.9890338182449341\n",
      "14-0\t0.0067\t [([0, 0], 0.0067)] 0.35114535689353943\n",
      "15-8\t0.0066\t [([0, 0], 0.0066)] 0.520453155040741\n",
      "17-17\t0.0059\t [([0, 0], 0.0059)] 0.8330322504043579\n",
      "11-17\t0.0055\t [([0, 0], 0.0055)] 0.2726283669471741\n",
      "20-13\t0.0047\t [([0, 0], 0.0047)] 0.7654220461845398\n",
      "16-15\t0.0045\t [([0, 0], 0.0045)] 0.30768778920173645\n",
      "13-10\t0.0042\t [([0, 0], 0.0042)] 0.8127387762069702\n",
      "17-13\t0.0042\t [([0, 0], 0.0042)] 0.1281743347644806\n",
      "14-13\t0.0035\t [([0, 0], 0.0035)] 0.9591408967971802\n",
      "18-9\t0.0033\t [([0, 0], 0.0033)] 0.7281856536865234\n",
      "13-11\t0.0032\t [([0, 0], 0.0032)] 0.2630724012851715\n",
      "19-9\t0.0030\t [([0, 0], 0.003)] 0.35838571190834045\n",
      "15-16\t0.0023\t [([0, 0], 0.0023)] 0.3455911874771118\n",
      "11-5\t0.0019\t [([0, 0], 0.0019)] 0.5495287775993347\n",
      "13-19\t0.0015\t [([0, 0], 0.0015)] 0.2583109736442566\n",
      "15-10\t0.0013\t [([0, 0], 0.0013)] 0.278567910194397\n",
      "10-10\t0.0013\t [([0, 0], 0.0013)] 0.14574319124221802\n",
      "17-4\t0.0013\t [([0, 0], 0.0013)] 0.3702365756034851\n",
      "8-1\t0.0013\t [([0, 0], 0.0013)] 0.99998939037323\n",
      "15-1\t0.0011\t [([0, 0], 0.0011)] 0.22430424392223358\n",
      "11-19\t0.0011\t [([0, 0], 0.0011)] 0.12413560599088669\n",
      "9-10\t0.0011\t [([0, 0], 0.0011)] 0.29155683517456055\n",
      "7-14\t0.0010\t [([0, 0], 0.001)] 0.16643935441970825\n",
      "15-7\t0.0010\t [([0, 0], 0.001)] 0.49894264340400696\n",
      "11-7\t0.0009\t [([0, 0], 0.0009)] 0.22333280742168427\n",
      "11-9\t0.0009\t [([0, 0], 0.0009)] 0.10902252793312073\n",
      "18-6\t0.0008\t [([0, 0], 0.0008)] 0.08260254561901093\n",
      "11-2\t0.0007\t [([0, 0], 0.0007)] 0.1576467901468277\n",
      "9-12\t0.0007\t [([0, 0], 0.0007)] 0.5680782198905945\n",
      "13-0\t0.0006\t [([0, 0], 0.0006)] 0.11989380419254303\n",
      "7-2\t0.0006\t [([0, 0], 0.0006)] 0.4339902997016907\n",
      "13-17\t0.0005\t [([0, 0], 0.0005)] 0.09812405705451965\n"
     ]
    }
   ],
   "source": [
    "ki = [41, 35, 29, 23, 17, 11, 5]\n",
    "tgt_i = 45 # 是要找箭头关注变化的位置的头\n",
    "_attrs = attrs #/ attrs.view(attrs.size(0), -1).norm(dim=1).view(attrs.size(0), 1, 1, 1)\n",
    "_attrs = _attrs[:, :, -1:, tgt_i:tgt_i+1]\n",
    "values, indices = _attrs.view(_attrs.size(0), H, -1).topk(1, dim=-1)\n",
    "#values, indices = _attrs.view(_attrs.size(0), H, -1).topk(5, dim=-1)\n",
    "val, ind = values.sum(dim=-1).view(-1).topk(50)\n",
    "val, ind = numpy(val), unravel_index(ind, values.size()[:-1])\n",
    "\n",
    "top_heads = []\n",
    "# for (l, h), v in zip(ind, val):\n",
    "#     top_heads.append((l, h, v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h])))))\n",
    "#     if l<2: continue\n",
    "#     print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1])\n",
    "    #if l < 2: continue\n",
    "   # print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1])#, attns[l, h, -1, tgt_i].item())\n",
    "for (l, h), v in zip(ind, val):\n",
    "#     top_heads[(l, h)] = (v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h]))))\n",
    "    top_heads.append((l, h, v, list(zip(unravel_index(indices[l, h], _attrs.size()[-2:]), numpy(values[l, h])))))\n",
    "    if l <7: continue\n",
    "    print('%d-%d\\t%.4f\\t' % (l, h, v), top_heads[-1][-1], attns[l, h, -1, tgt_i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer, head = 12, 4 #\n",
    "# layer, head = \n",
    "layer2, head2 = 13,12# 是\n",
    "src, tgt = 61, 58\n",
    "h = model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output.retain_grad() failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'58 R': 0.1839,\n",
       " '57 P': 0.1824,\n",
       " '60 p': 0.1175,\n",
       " '59 ,': 0.0854,\n",
       " '61 ->': 0.0789}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Ġr': 0.707, 'Ġp': 0.1634, 'Ġq': 0.0661, 'Ġpr': 0.0053, 'Ġo': 0.0051}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = h[layer].attn.attention\n",
    "mask = torch.ones(H, seq_len)\n",
    "mask[:, -1] = 0\n",
    "mask[head, -1] = 1# 0是去掉，1是不去\n",
    "m.attn_mask = mask.unsqueeze(-1)\n",
    "m.attn_out = None\n",
    "try:\n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "finally: m.attn_mask = None\n",
    "attn_out = m.attn_out\n",
    "delattr(m, 'attn_out')\n",
    "\n",
    "attn = globalize(outputs.attentions[layer2])[0, head2, -1]\n",
    "show_topk(*attn.topk(5), indices_fn=append_tokens_to_positions)\n",
    "probs = outputs.logits[0, -1].softmax(-1)\n",
    "show_topk(*probs.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)\n",
    "# 第一个输出是对最后进行结果预测的head的影响，看指向的概率变换，第二个是最终结果的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_heads[task_name] = top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_heads[task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer.encode_plus(_text, return_tensors='pt')\n",
    "# outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "# y = logits[0, -1].max()\n",
    "# attentions = outputs.attentions if hasattr(outputs, 'attentions') else outputs[-1]\n",
    "# for a in attentions: a.retain_grad()\n",
    "# model.zero_grad()\n",
    "# y.backward()\n",
    "\n",
    "# # attns = torch.cat(attentions)\n",
    "# grads = torch.cat([a.grad for a in attentions])\n",
    "# attrs2 = attns * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(top_heads)):\n",
    "    layer, head, v, _ = top_heads[i]\n",
    "#     if layer in [0, 1, ]: continue\n",
    "#     layer, head, v = 30, 10, 1.\n",
    "    fig, axs = plt.subplots(1,2,sharey=False, figsize=(10 * 2, 10))\n",
    "    for i, (a, _ax) in enumerate(zip([attns, attrs], axs)):\n",
    "        a = a[layer][head].detach().cpu()\n",
    "        a, annot = ((a * 100).long(), True) if i == -1 else (a, False)\n",
    "        res = sns.heatmap(a, square=True, cbar=False, annot=annot, fmt='d', linewidths=0.1, linecolor='grey', \n",
    "                          xticklabels=tokens, yticklabels=tokens, ax=_ax)\n",
    "        _ = res.set_xticklabels(res.get_xmajorticklabels(), fontsize=9+3-2, rotation=0)\n",
    "        _ = res.set_yticklabels(res.get_ymajorticklabels(), fontsize=9+3-2, rotation=0)\n",
    "        _ = plt.xlabel('%d-%d    %.4f' % (layer, head, v), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_string(gpt2_tokenizer):\n",
    "    tokens = [gpt2_tokenizer.convert_ids_to_tokens(i) for i in range(120)]\n",
    "    tokens = [token for token in tokens if token not in string.digits + string.ascii_uppercase + string.ascii_lowercase]\n",
    "    tokens = ['Ġ' + token for token in tokens if gpt2_tokenizer._convert_token_to_id('Ġ' + token) != 50256]\n",
    "    token_ids = [gpt2_tokenizer._convert_token_to_id(token) for token in tokens]  # XD\n",
    "    print(tokens, len(tokens))\n",
    "    \n",
    "    sampled_tokens_idx = []\n",
    "    sampled_tokens = []\n",
    "    sampled_token_ids = []  # XD\n",
    "\n",
    "    random.seed(6)  # XD\n",
    "    #get sampled tokens idx\n",
    "    range_ = list(range(len(tokens)))\n",
    "    for i in range(362):\n",
    "        idx = random.choice(range_)\n",
    "        sampled_tokens_idx.append(idx)\n",
    "\n",
    "    for idx in sampled_tokens_idx:\n",
    "        sampled_tokens.append(tokens[idx])\n",
    "        sampled_token_ids.append(token_ids[idx])\n",
    "\n",
    "    text = ''.join(sampled_tokens).replace('Ġ', ' ')  # XD\n",
    "    print(text, len(sampled_token_ids)) # XD\n",
    "    return sampled_token_ids, text  # XD\n",
    "    \n",
    "    # print(\"\".join(sampled_tokens), len(sampled_tokens))\n",
    "    # return \"\".join(sampled_tokens), len(sampled_tokens)\n",
    "\n",
    "token_ids, text = get_random_string(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Big is to small as fast is to _',\n",
    "    'Bread is to eat as gun is to _',\n",
    "    'big: small, fast: _',\n",
    "    'bread: eat, gun: _ .',\n",
    "    'flower: fragrant, fire: hot, bread: delicious, gun: _ ',\n",
    "    'Big and small are _ .',\n",
    "    'What is twice 3? _.',\n",
    "    'What is the half 6? _.',\n",
    "    'There is a sequence: 3, 5, 2, 7. The number immediately precedes 5 is _.',  # :)\n",
    "    'There is a sequence: 3, 5, 2, 7. The number immediately follows 5 is _.',  # :(\n",
    "    'There is a sequence: 3, 5, 2, 7. The number between 5 and 7 is _.',\n",
    "    'There is a sequence of numbers: 3, 5, 2, 4. _ is the first number.',\n",
    "    'There is a sequence of numbers: 3, 5, 2. The reversed sequence is _.',\n",
    "    '''There is a sequence of numbers: 5, 1, 6, 3. The second number is 1.\n",
    "There is a sequence of numbers: 3, 7, 2, 4. The second number is _.''',\n",
    "    '''There is a sequence of letters: e, c, b, a. The last letter is a.\n",
    "There is a sequence of letters: f, d, b, g. The last letter is _.''',\n",
    "    '''The uppercase of c is C. The uppercase of f is _.''',\n",
    "    '''The successor of 3 is 4. The successor of 8 is _.''',\n",
    "    '''The successor of 3 is 4. The successor of _ is 6.''',\n",
    "#     '''The predecessor of 3 is 2. The predecessor of 5 is 4. The predecessor of 6 is _''',\n",
    "#     '''The previous integer of 4 is 3. The previous integer of 3 is _.''',\n",
    "#     '''3 minus 1 equals 2. 5 minus 1 equals _.''',\n",
    "    '''If 2 changes to 3, 5 changes to 6, then _ changes to 9''',\n",
    "    '''If 2 changes to 20, 3 changes to 30, then 5 changes to _''',\n",
    "    '''2 -> 3, 4 -> 5, 5 -> 6, 9 -> _.''',\n",
    "    '''3 -> 2, 5 -> 4, 6 -> 5, 9 -> _''',\n",
    "    '''9 -> 8, 7 -> 6, 6 -> 5, 2 -> _.''',\n",
    "    '''3 is to _ as 4 is to 8 and 5 is to 10.''',\n",
    "#     '''6 : _ :: 5 : 10 :: 7 : 14 :: 8 : 16.''',\n",
    "#     '''a is to _ as f is to g, h to i, i to j, s to t.''',\n",
    "#     '''c is to _ as f is to e, h to g, j to i.''',\n",
    "    '''c is to _ as j is to i, h to g, f to e.''',\n",
    "#     '''Twice 3 is 6, twice 4 is _.''',\n",
    "#     '''Half of 4 is 2, half of 6 is _.''',\n",
    "\n",
    "# '''Shall I compare thee to a summer's day?\n",
    "# Thou''',\n",
    "# '''Do not go gentle into that good night,\n",
    "# Old age should burn and rave at close of day;\n",
    "# Rage'''\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
