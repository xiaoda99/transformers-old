{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from itertools import product, chain\n",
    "import numpy as np\n",
    "from pattern.en import comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from child_frames import frames\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src/transformers')\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers.data.datasets.glue import Split\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A_template = \"{rel_prefix} {dt} {ent0} {rel} {dt} {ent1} {rel_suffix}\"\n",
    "B_templates = [\"{pred_prefix} {dt} {ent} {pred}\", \"{pred_prefix} {pred} {dt} {ent}\"]\n",
    "B_template = B_templates[0]\n",
    "entailment_templates = [\n",
    "    \"{A} ? {conj} , {B} .\",  # yes/no/maybe\n",
    "    \"{A} , so {B} ? {conj} .\",\n",
    "]\n",
    "\n",
    "def join_lists(x): return list(chain.from_iterable(x))\n",
    "\n",
    "def reverse(l): return list(reversed(l))\n",
    "\n",
    "def mask(ent_str):\n",
    "    tokens = ent_str.strip().split()\n",
    "    if len(tokens) == 1:\n",
    "        return '[ %s ]' % tokens[0]\n",
    "    elif len(tokens) == 2:\n",
    "        assert tokens[0] == 'the', ent_str\n",
    "        return '%s [ %s ]' % (tokens[0], tokens[1])\n",
    "    else:\n",
    "        assert False, ent_str\n",
    "\n",
    "def negate_sent(sent):\n",
    "    assert ' is ' in sent\n",
    "    neg_sents = []\n",
    "    neg_sents.append(sent.replace(' is ', ' is not '))\n",
    "    neg_sents.append('it is unlikely that ' + sent)\n",
    "    return neg_sents\n",
    "\n",
    "def swap_entities(sent, e0='X', e1='Z'): return sent.replace(e0, 'xx').replace(e1, e0).replace('xx', e1)\n",
    "\n",
    "def make_sentences(index=-1, orig_sentence='', entities=[\"X\", \"Z\"], determiner=\"\",\n",
    "                   relation_group=[[\"big\", \"large\"], [\"small\"]], rand_relation_group=[[\"short\"], [\"tall\", \"high\"]],\n",
    "                   relation_prefix=\"\", relation_suffix=\"\", predicate_prefix=\"\",\n",
    "                   n_entity_trials=3, has_negation=True, has_neutral=True, entity_set=string.ascii_uppercase):\n",
    "    def form_As(relations):\n",
    "        return [A_template.format(dt=determiner, ent0=ent0, ent1=ent1, rel=rel, rel_prefix=relation_prefix, rel_suffix=relation_suffix)\n",
    "              for ent0, ent1, rel in [entities + relations[:1], reverse(entities) + reverse(relations)[:1]]]\n",
    "\n",
    "    As = []\n",
    "    for rel0 in relation_group[0]:\n",
    "        for rel1 in relation_group[1]:\n",
    "            relations = [\"is %s than\" % comparative(rel) for rel in [rel0, rel1]]\n",
    "            As += form_As(relations)\n",
    "    As = list(set(As))\n",
    "    negAs = join_lists([negate_sent(A)[:1] for A in As]) if has_negation else []\n",
    "\n",
    "    def form_Bs(predicates): \n",
    "        return [B_template.format(dt=determiner, ent=ent, pred=pred, pred_prefix=predicate_prefix)\n",
    "              for ent, pred in zip(entities, predicates)]\n",
    "\n",
    "    Bs, negBs = {'orig': [], 'rand': []}, {}\n",
    "    for k, group in zip(['orig', 'rand'], [relation_group, rand_relation_group]):\n",
    "        for rel0 in group[0]:\n",
    "            for rel1 in group[1]:\n",
    "                predicates = [\"is %s\" % comparative(rel) for rel in [rel0, rel1]]\n",
    "                Bs[k] += form_Bs(predicates)\n",
    "    for k in Bs:\n",
    "        Bs[k] = list(set(Bs[k]))\n",
    "        if has_negation:\n",
    "            negBs[k] = join_lists([negate_sent(B)[:1] for B in Bs[k]])\n",
    "            Bs[k], negBs[k] = Bs[k] + [swap_entities(negB) for negB in negBs[k]], negBs[k] + [swap_entities(B) for B in Bs[k]]\n",
    "        else:\n",
    "            negBs[k] = [swap_entities(B) for B in Bs[k]]\n",
    "\n",
    "    def form_sentences(sentence_template, As, Bs, conj):\n",
    "        return [\" \" + \" \".join(sentence_template.format(A=A, B=B, conj=conj).split()) for A, B in product(As, Bs)]\n",
    "\n",
    "    sentences = defaultdict(list)\n",
    "    for entailment_template in entailment_templates[-1:]:\n",
    "        for A, B, conj in [(As, Bs['orig'], 'Right'), \n",
    "                           (negAs, negBs['orig'], 'Right'), \n",
    "                           (As, negBs['orig'], 'Wrong'), \n",
    "                           (negAs, Bs['orig'], 'Wrong'),\n",
    "                           (As, Bs['rand'], 'Maybe'), \n",
    "                           (negAs, negBs['rand'], 'Maybe'), \n",
    "                           (As, negBs['rand'], 'Maybe'), \n",
    "                           (negAs, Bs['rand'], 'Maybe'),\n",
    "                          ]:\n",
    "            sentences[conj] += form_sentences(entailment_template, A, B, mask(conj))\n",
    "    assert len(sentences['Right']) == len(sentences['Wrong']), '%d %d' % (len(sentences['Right']), len(sentences['Wrong']))\n",
    "    sentences['Maybe'] = random.sample(sentences['Maybe'], len(sentences['Right']) // 2)\n",
    "    sentences = join_lists(sentences[k] for k in (sentences.keys() if has_neutral else ['Right', 'Wrong']))\n",
    "    \n",
    "    substituted_sentences = []\n",
    "    for sent in sentences:\n",
    "        for _ in range(n_entity_trials):\n",
    "            e0, e1 = random.sample(entity_set, 2)\n",
    "            substituted_sentences.append(sent.replace(entities[0], e0).replace(entities[1], e1))\n",
    "    return sentences, substituted_sentences\n",
    "\n",
    "# make_sentences(has_negation=False, has_neutral=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_masked_tokens(tokens):\n",
    "    out = []\n",
    "    while len(tokens) > 0:\n",
    "        token = tokens.pop(0)\n",
    "        if token not in ['[', ']', 'Ġ[', 'Ġ]']:\n",
    "            out.append(token)\n",
    "        else:\n",
    "            assert token in ['[', 'Ġ[']\n",
    "            next_token = tokens.pop(0)  # the maksed word\n",
    "            next_next_token = tokens.pop(0)  # \"]\" symbol\n",
    "            assert next_next_token in [']',  'Ġ]']\n",
    "            token, next_next_token = token.replace('Ġ', ''), next_next_token.replace('Ġ', '')\n",
    "            out.append(token + next_token + next_next_token)\n",
    "    return out\n",
    "\n",
    "class CHILDDataset(Dataset):\n",
    "    all_lines = {Split.train: None, Split.dev: None, Split.test: None}\n",
    "    \n",
    "    def __init__(self, all_lines, tokenizer, max_seq_len=None, split_pct=[0.7, 0.3, 0.0], max_noise_len=0, mode=Split.train):\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        if CHILDDataset.all_lines[mode] is None:\n",
    "            random.shuffle(all_lines)\n",
    "            n_dev = int(round(len(all_lines) * split_pct[1]))\n",
    "            n_test = int(round(len(all_lines) * split_pct[2]))\n",
    "            n_train = len(all_lines) - n_dev - n_test\n",
    "            \n",
    "            def flatten(lines):\n",
    "                return list(chain.from_iterable(lines)) if len(lines) > 0 and type(lines[0]) == list else lines\n",
    "            \n",
    "            CHILDDataset.all_lines[Split.train] = flatten(all_lines[:n_train])\n",
    "            CHILDDataset.all_lines[Split.dev] = flatten(all_lines[n_train: n_train + n_dev])\n",
    "            CHILDDataset.all_lines[Split.test] = flatten(all_lines[n_train + n_dev:])\n",
    "\n",
    "        examples = []\n",
    "        for i, line in enumerate(CHILDDataset.all_lines[mode]):\n",
    "            t1, t2, is_next_label = self.split_sent(line)\n",
    "            tokens_a = rejoin_masked_tokens(tokenizer.tokenize(t1))\n",
    "            tokens_b = rejoin_masked_tokens(tokenizer.tokenize(t2)) if t2 is not None else None\n",
    "            example = InputExample(guid=i, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label)\n",
    "            examples.append(example)\n",
    "\n",
    "        if max_seq_len is None:\n",
    "            max_seq_len = max([len(example.tokens_a) + len(example.tokens_b) + 3\n",
    "                if example.tokens_b is not None else len(example.tokens_a) + 2\n",
    "                for example in examples])\n",
    "\n",
    "        self.features = [convert_example_to_features(example, max_seq_len, tokenizer, max_noise_len=max_noise_len)\n",
    "             for example in examples]\n",
    "\n",
    "    def split_sent(self, line):\n",
    "        label = 0\n",
    "        if \"|||\" in line:\n",
    "            t1, t2 = [t.strip() for t in line.split(\"|||\")]\n",
    "            assert len(t1) > 0 and len(t2) > 0, \"%d %d\" % (len(t1), len(t2))\n",
    "        else:\n",
    "            # assert self.one_sent\n",
    "            t1, t2 = line.strip(), None\n",
    "        return t1, t2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /home/xd/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /home/xd/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, shortcut = RobertaForMaskedLM, RobertaTokenizer, 'roberta-large'\n",
    "model, tokenizer = None, tokenizer_class.from_pretrained(shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(frames)\n",
    "all_lines = [make_sentences(relation_group=rg, rand_relation_group=frames[(i + 1) % len(frames)], \n",
    "                            has_negation=False, has_neutral=True)[1] \n",
    "             for i, rg in enumerate(frames)]\n",
    "# all_lines = join_lists(all_lines)\n",
    "for k in CHILDDataset.all_lines: CHILDDataset.all_lines[k] = None\n",
    "train_dataset = CHILDDataset(all_lines, tokenizer, split_pct=[0.7, 0.3, 0.0], mode='train')\n",
    "eval_dataset = CHILDDataset(all_lines, tokenizer, split_pct=[0.7, 0.3, 0.0], mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/xd/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /home/xd/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "WARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:transformers.training_args:PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading state_dict took 0.992 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.trainer:You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "INFO:transformers.trainer:To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 3294\n",
      "INFO:transformers.trainer:  Num Epochs = 3\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 32\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 1\n",
      "INFO:transformers.trainer:  Total optimization steps = 309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e049e7eceb2f44bb98369775e422e6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476bf52294634040906e4eed1b54d496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=103.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1985808324813843, 'learning_rate': 1.6763754045307445e-05, 'epoch': 0.4854368932038835, 'step': 50}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b684a32a827406caeaff3714a48a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.7780519994822416, 'eval_accuracy': 0.5875946974212473, 'epoch': 0.4854368932038835, 'step': 50}\n",
      "{'loss': 0.7714587771892547, 'learning_rate': 1.3527508090614887e-05, 'epoch': 0.970873786407767, 'step': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe35743abda45a1a9a93c8a66a3e1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 1.1149881427938289, 'eval_accuracy': 0.5223958343267441, 'epoch': 0.970873786407767, 'step': 100}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bef1e19e814cf889e4df93bae028e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=103.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5574883329868316, 'learning_rate': 1.029126213592233e-05, 'epoch': 1.4563106796116505, 'step': 150}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c6f473a6f14ef8beebc14de0e3eb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.7463125004009767, 'eval_accuracy': 0.7781250016255812, 'epoch': 1.4563106796116505, 'step': 150}\n",
      "{'loss': 0.30606507778167724, 'learning_rate': 7.055016181229773e-06, 'epoch': 1.941747572815534, 'step': 200}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992fc23167f54db0be080d5e90353f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.5870019793510437, 'eval_accuracy': 0.8660511374473572, 'epoch': 1.941747572815534, 'step': 200}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e1645845547ccbf20c3f0eb964fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=103.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17191166400909424, 'learning_rate': 3.818770226537217e-06, 'epoch': 2.4271844660194173, 'step': 250}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca82a9e18c6408fba99e94350a25a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.8572778322479941, 'eval_accuracy': 0.8596590919928118, 'epoch': 2.4271844660194173, 'step': 250}\n",
      "{'loss': 0.13333035230636597, 'learning_rate': 5.825242718446603e-07, 'epoch': 2.912621359223301, 'step': 300}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1625a669014c2fa095429ae9218f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.880185604095459, 'eval_accuracy': 0.8617897738109935, 'epoch': 2.912621359223301, 'step': 300}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f7e02c1a2d43618b05e009ed984c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=22.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.8635010434822603, 'eval_accuracy': 0.8639204556291754, 'epoch': 3.0, 'step': 309}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=309, training_loss=0.5107385703817637)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_class.from_pretrained('roberta-base', model)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=32, per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5, num_train_epochs=3,\n",
    "    logging_steps=50, eval_steps=50, save_steps=1000,\n",
    "    evaluate_during_training=True,\n",
    ")\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "dataloader = trainer.get_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   534,    16,    55,  1473,    87,   468, 17487, 50264,  2156,\n",
      "           272,    16,    55,  1473,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,  1301,    16,    55,  1473,    87,   226, 17487, 50264,  2156,\n",
      "           525,    16,    55,  1473,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   530,    16,    55,  1473,    87,   384, 17487, 50264,  2156,\n",
      "           229,    16,    55,  1473,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   565,    16,    55,  1473,    87,   289, 17487, 50264,  2156,\n",
      "           289,    16,    55, 37192,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   791,    16,    55,  1473,    87,   274, 17487, 50264,  2156,\n",
      "           274,    16,    55, 37192,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   975,    16,    55,  1473,    87,   256, 17487, 50264,  2156,\n",
      "           256,    16,    55, 37192,   479,     2,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   387,    16,    55,  1473,    87,   234, 17487, 50264,  2156,\n",
      "           234,    16,    45,    55,  1473,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   104,    16,    55,  1473,    87,   468, 17487, 50264,  2156,\n",
      "           468,    16,    45,    55,  1473,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,  1000,    16,    55,  1473,    87,   381, 17487, 50264,  2156,\n",
      "           381,    16,    45,    55,  1473,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   487,    16,    55,  1473,    87,    38, 17487, 50264,  2156,\n",
      "            24,    16,  3752,    14,    38,    16,    55,  1473,   479,     2,\n",
      "             0],\n",
      "        [    0,   791,    16,    55,  1473,    87,   211, 17487, 50264,  2156,\n",
      "            24,    16,  3752,    14,   211,    16,    55,  1473,   479,     2,\n",
      "             0],\n",
      "        [    0,   574,    16,    55,  1473,    87,   211, 17487, 50264,  2156,\n",
      "            24,    16,  3752,    14,   211,    16,    55,  1473,   479,     2,\n",
      "             0],\n",
      "        [    0,   510,    16,    55,  1473,    87,   221, 17487, 50264,  2156,\n",
      "           221,    16,    45,    55, 37192,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   565,    16,    55,  1473,    87,   274, 17487, 50264,  2156,\n",
      "           255,    16,    45,    55, 37192,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   725,    16,    55,  1473,    87,   854, 17487, 50264,  2156,\n",
      "           289,    16,    45,    55, 37192,   479,     2,     0,     0,     0,\n",
      "             0],\n",
      "        [    0,   250,    16,    55,  1473,    87,   121, 17487, 50264,  2156,\n",
      "            24,    16,  3752,    14,    83,    16,    55, 37192,   479,     2,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 5143, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  0,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  0,\n",
      "          0,  0,  0],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19,  0]])}\n"
     ]
    }
   ],
   "source": [
    "for inputs in dataloader: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer._prepare_inputs(inputs, model)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax((logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = (logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1).argmax(dim=-1)\n",
    "\n",
    "labels = (inputs['labels'] * (inputs['labels'] != -100)).sum(dim=-1)\n",
    "\n",
    "(pred_labels == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = probs.topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 235, 13984, 10039]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠLeft']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['ĠWrong', 'ĠRight', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['ĠWrong', 'ĠRight', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'Ġwrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'Ġwrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n"
     ]
    }
   ],
   "source": [
    "for top_idx in indices:\n",
    "    print(top_idx.tolist())\n",
    "    print(tokenizer.convert_ids_to_tokens(top_idx.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
