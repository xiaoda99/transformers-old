{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from itertools import product, chain\n",
    "import numpy as np\n",
    "from pattern.en import comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from child_frames import frames\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/nas/xd/projects/transformers/src/transformers')\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
    "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "from transformers.data.datasets.glue import Split\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġlatter']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(' latter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' X is bigger than Z , so Z ( latter ) is smaller ( opposite ) ? [ Right ] .',\n",
       "  ' X is bigger than Z , so X ( former ) is bigger ( same ) ? [ Right ] .',\n",
       "  ' X is bigger than Z , so X ( former ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "  ' X is bigger than Z , so Z ( latter ) is not bigger ( same ) ? [ Right ] .',\n",
       "  ' Z is smaller than X , so Z ( former ) is smaller ( same ) ? [ Right ] .',\n",
       "  ' Z is smaller than X , so X ( latter ) is bigger ( opposite ) ? [ Right ] .',\n",
       "  ' Z is smaller than X , so X ( latter ) is not smaller ( same ) ? [ Right ] .',\n",
       "  ' Z is smaller than X , so Z ( former ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "  ' X is not bigger than Z , so Z ( latter ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "  ' X is not bigger than Z , so X ( former ) is not bigger ( same ) ? [ Right ] .',\n",
       "  ' X is not bigger than Z , so X ( former ) is smaller ( opposite ) ? [ Right ] .',\n",
       "  ' X is not bigger than Z , so Z ( latter ) is bigger ( same ) ? [ Right ] .',\n",
       "  ' Z is not smaller than X , so Z ( former ) is not smaller ( same ) ? [ Right ] .',\n",
       "  ' Z is not smaller than X , so X ( latter ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "  ' Z is not smaller than X , so X ( latter ) is smaller ( same ) ? [ Right ] .',\n",
       "  ' Z is not smaller than X , so Z ( former ) is bigger ( opposite ) ? [ Right ] .',\n",
       "  ' X is bigger than Z , so Z ( latter ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "  ' X is bigger than Z , so X ( former ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "  ' X is bigger than Z , so X ( former ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "  ' X is bigger than Z , so Z ( latter ) is bigger ( same ) ? [ Wrong ] .',\n",
       "  ' Z is smaller than X , so Z ( former ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "  ' Z is smaller than X , so X ( latter ) is not bigger ( opposite ) ? [ Wrong ] .',\n",
       "  ' Z is smaller than X , so X ( latter ) is smaller ( same ) ? [ Wrong ] .',\n",
       "  ' Z is smaller than X , so Z ( former ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "  ' X is not bigger than Z , so Z ( latter ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "  ' X is not bigger than Z , so X ( former ) is bigger ( same ) ? [ Wrong ] .',\n",
       "  ' X is not bigger than Z , so X ( former ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "  ' X is not bigger than Z , so Z ( latter ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "  ' Z is not smaller than X , so Z ( former ) is smaller ( same ) ? [ Wrong ] .',\n",
       "  ' Z is not smaller than X , so X ( latter ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "  ' Z is not smaller than X , so X ( latter ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "  ' Z is not smaller than X , so Z ( former ) is not bigger ( opposite ) ? [ Wrong ] .'],\n",
       " [[' K is bigger than T , so T ( latter ) is smaller ( opposite ) ? [ Right ] .',\n",
       "   ' I is bigger than A , so A ( latter ) is smaller ( opposite ) ? [ Right ] .',\n",
       "   ' C is bigger than H , so H ( latter ) is smaller ( opposite ) ? [ Right ] .'],\n",
       "  [' V is bigger than S , so V ( former ) is bigger ( same ) ? [ Right ] .',\n",
       "   ' S is bigger than A , so S ( former ) is bigger ( same ) ? [ Right ] .',\n",
       "   ' Y is bigger than V , so Y ( former ) is bigger ( same ) ? [ Right ] .'],\n",
       "  [' I is bigger than S , so I ( former ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "   ' B is bigger than Y , so B ( former ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "   ' Y is bigger than F , so Y ( former ) is not smaller ( opposite ) ? [ Right ] .'],\n",
       "  [' P is bigger than Q , so Q ( latter ) is not bigger ( same ) ? [ Right ] .',\n",
       "   ' U is bigger than O , so O ( latter ) is not bigger ( same ) ? [ Right ] .',\n",
       "   ' I is bigger than F , so F ( latter ) is not bigger ( same ) ? [ Right ] .'],\n",
       "  [' N is smaller than S , so N ( former ) is smaller ( same ) ? [ Right ] .',\n",
       "   ' P is smaller than U , so P ( former ) is smaller ( same ) ? [ Right ] .',\n",
       "   ' P is smaller than C , so P ( former ) is smaller ( same ) ? [ Right ] .'],\n",
       "  [' N is smaller than L , so L ( latter ) is bigger ( opposite ) ? [ Right ] .',\n",
       "   ' V is smaller than K , so K ( latter ) is bigger ( opposite ) ? [ Right ] .',\n",
       "   ' F is smaller than D , so D ( latter ) is bigger ( opposite ) ? [ Right ] .'],\n",
       "  [' N is smaller than K , so K ( latter ) is not smaller ( same ) ? [ Right ] .',\n",
       "   ' P is smaller than W , so W ( latter ) is not smaller ( same ) ? [ Right ] .',\n",
       "   ' V is smaller than J , so J ( latter ) is not smaller ( same ) ? [ Right ] .'],\n",
       "  [' Y is smaller than M , so Y ( former ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "   ' B is smaller than R , so B ( former ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "   ' C is smaller than O , so C ( former ) is not bigger ( opposite ) ? [ Right ] .'],\n",
       "  [' K is not bigger than I , so I ( latter ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "   ' K is not bigger than D , so D ( latter ) is not smaller ( opposite ) ? [ Right ] .',\n",
       "   ' Y is not bigger than M , so M ( latter ) is not smaller ( opposite ) ? [ Right ] .'],\n",
       "  [' Q is not bigger than A , so Q ( former ) is not bigger ( same ) ? [ Right ] .',\n",
       "   ' V is not bigger than R , so V ( former ) is not bigger ( same ) ? [ Right ] .',\n",
       "   ' O is not bigger than N , so O ( former ) is not bigger ( same ) ? [ Right ] .'],\n",
       "  [' B is not bigger than G , so B ( former ) is smaller ( opposite ) ? [ Right ] .',\n",
       "   ' Q is not bigger than L , so Q ( former ) is smaller ( opposite ) ? [ Right ] .',\n",
       "   ' T is not bigger than Y , so T ( former ) is smaller ( opposite ) ? [ Right ] .'],\n",
       "  [' P is not bigger than U , so U ( latter ) is bigger ( same ) ? [ Right ] .',\n",
       "   ' O is not bigger than Y , so Y ( latter ) is bigger ( same ) ? [ Right ] .',\n",
       "   ' B is not bigger than G , so G ( latter ) is bigger ( same ) ? [ Right ] .'],\n",
       "  [' R is not smaller than I , so R ( former ) is not smaller ( same ) ? [ Right ] .',\n",
       "   ' J is not smaller than E , so J ( former ) is not smaller ( same ) ? [ Right ] .',\n",
       "   ' W is not smaller than O , so W ( former ) is not smaller ( same ) ? [ Right ] .'],\n",
       "  [' D is not smaller than P , so P ( latter ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "   ' U is not smaller than A , so A ( latter ) is not bigger ( opposite ) ? [ Right ] .',\n",
       "   ' Z is not smaller than T , so T ( latter ) is not bigger ( opposite ) ? [ Right ] .'],\n",
       "  [' W is not smaller than H , so H ( latter ) is smaller ( same ) ? [ Right ] .',\n",
       "   ' J is not smaller than F , so F ( latter ) is smaller ( same ) ? [ Right ] .',\n",
       "   ' A is not smaller than R , so R ( latter ) is smaller ( same ) ? [ Right ] .'],\n",
       "  [' N is not smaller than R , so N ( former ) is bigger ( opposite ) ? [ Right ] .',\n",
       "   ' H is not smaller than C , so H ( former ) is bigger ( opposite ) ? [ Right ] .',\n",
       "   ' O is not smaller than D , so O ( former ) is bigger ( opposite ) ? [ Right ] .'],\n",
       "  [' D is bigger than U , so U ( latter ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' E is bigger than P , so P ( latter ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' W is bigger than J , so J ( latter ) is not smaller ( opposite ) ? [ Wrong ] .'],\n",
       "  [' Q is bigger than W , so Q ( former ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "   ' I is bigger than N , so I ( former ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "   ' P is bigger than H , so P ( former ) is not bigger ( same ) ? [ Wrong ] .'],\n",
       "  [' O is bigger than R , so O ( former ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' E is bigger than M , so E ( former ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' G is bigger than T , so G ( former ) is smaller ( opposite ) ? [ Wrong ] .'],\n",
       "  [' Q is bigger than X , so X ( latter ) is bigger ( same ) ? [ Wrong ] .',\n",
       "   ' E is bigger than C , so C ( latter ) is bigger ( same ) ? [ Wrong ] .',\n",
       "   ' I is bigger than Y , so Y ( latter ) is bigger ( same ) ? [ Wrong ] .'],\n",
       "  [' N is smaller than N , so N ( former ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "   ' Z is smaller than K , so Z ( former ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "   ' I is smaller than Q , so I ( former ) is not smaller ( same ) ? [ Wrong ] .'],\n",
       "  [' J is smaller than A , so A ( latter ) is not bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' J is smaller than X , so X ( latter ) is not bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' V is smaller than S , so S ( latter ) is not bigger ( opposite ) ? [ Wrong ] .'],\n",
       "  [' E is smaller than P , so P ( latter ) is smaller ( same ) ? [ Wrong ] .',\n",
       "   ' R is smaller than O , so O ( latter ) is smaller ( same ) ? [ Wrong ] .',\n",
       "   ' L is smaller than P , so P ( latter ) is smaller ( same ) ? [ Wrong ] .'],\n",
       "  [' R is smaller than K , so R ( former ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' R is smaller than Y , so R ( former ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' O is smaller than M , so O ( former ) is bigger ( opposite ) ? [ Wrong ] .'],\n",
       "  [' K is not bigger than G , so G ( latter ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' W is not bigger than H , so H ( latter ) is smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' S is not bigger than M , so M ( latter ) is smaller ( opposite ) ? [ Wrong ] .'],\n",
       "  [' H is not bigger than Y , so H ( former ) is bigger ( same ) ? [ Wrong ] .',\n",
       "   ' N is not bigger than B , so N ( former ) is bigger ( same ) ? [ Wrong ] .',\n",
       "   ' K is not bigger than X , so K ( former ) is bigger ( same ) ? [ Wrong ] .'],\n",
       "  [' P is not bigger than W , so P ( former ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' M is not bigger than M , so M ( former ) is not smaller ( opposite ) ? [ Wrong ] .',\n",
       "   ' M is not bigger than V , so M ( former ) is not smaller ( opposite ) ? [ Wrong ] .'],\n",
       "  [' U is not bigger than U , so U ( latter ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "   ' E is not bigger than P , so P ( latter ) is not bigger ( same ) ? [ Wrong ] .',\n",
       "   ' B is not bigger than E , so E ( latter ) is not bigger ( same ) ? [ Wrong ] .'],\n",
       "  [' S is not smaller than Q , so S ( former ) is smaller ( same ) ? [ Wrong ] .',\n",
       "   ' D is not smaller than K , so D ( former ) is smaller ( same ) ? [ Wrong ] .',\n",
       "   ' D is not smaller than O , so D ( former ) is smaller ( same ) ? [ Wrong ] .'],\n",
       "  [' O is not smaller than Q , so Q ( latter ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' X is not smaller than A , so A ( latter ) is bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' N is not smaller than E , so E ( latter ) is bigger ( opposite ) ? [ Wrong ] .'],\n",
       "  [' E is not smaller than U , so U ( latter ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "   ' P is not smaller than C , so C ( latter ) is not smaller ( same ) ? [ Wrong ] .',\n",
       "   ' I is not smaller than I , so I ( latter ) is not smaller ( same ) ? [ Wrong ] .'],\n",
       "  [' T is not smaller than K , so T ( former ) is not bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' M is not smaller than W , so M ( former ) is not bigger ( opposite ) ? [ Wrong ] .',\n",
       "   ' C is not smaller than U , so C ( former ) is not bigger ( opposite ) ? [ Wrong ] .']])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_template = \"{rel_prefix} {dt} {ent0} {rel} {dt} {ent1} {rel_suffix}\"\n",
    "B_templates = [\"{pred_prefix} {dt} {ent} {pred}\", \"{pred_prefix} {pred} {dt} {ent}\"]\n",
    "B_template = B_templates[0]\n",
    "entailment_templates = [\n",
    "    \"{A} ? {conj} , {B} .\",  # yes/no/maybe\n",
    "    \"{A} , so {B} ? {conj} .\",\n",
    "]\n",
    "\n",
    "def negate_sent(sent):\n",
    "    assert ' is ' in sent\n",
    "    neg_sents = []\n",
    "    neg_sents.append(sent.replace(' is ', ' is not '))\n",
    "    neg_sents.append('it is unlikely that ' + sent)\n",
    "    return neg_sents\n",
    "\n",
    "# def synonym(x, y): return x == y\n",
    "# def antonym(x, y): return x != y\n",
    "# def arbitrary(x, y): return True\n",
    "\n",
    "def make_sentences(index=-1, orig_sentence='', entities=[\"X\", \"Z\"], determiner=\"\",\n",
    "                   relation_group=[[\"big\",], [\"small\"]], rand_relation_group=[[\"short\"], [\"tall\", \"high\"]],\n",
    "                   relation_prefix=\"\", relation_suffix=\"\", predicate_prefix=\"\",\n",
    "                   n_entity_trials=3, has_negA=True, has_negB=True, has_neutral=False, predict_relation=True, \n",
    "                   lexical_relations=['same', 'opposite'], tag_lexical_rel=False, tag_entity=False, entity_set=string.ascii_uppercase):\n",
    "    def form_As(relations):\n",
    "        return [A_template.format(dt=determiner, ent0=ent0, ent1=ent1, rel=rel, rel_prefix=relation_prefix, rel_suffix=relation_suffix)\n",
    "              for ent0, ent1, rel in [entities + relations[:1], reverse(entities) + reverse(relations)[:1]]]\n",
    "\n",
    "    As = []\n",
    "    for rel0 in relation_group[0]:\n",
    "        for rel1 in relation_group[1]:\n",
    "            relations = [\"is %s:%d than\" % (comparative(rel), i) for i, rel in enumerate([rel0, rel1])]\n",
    "            As += form_As(relations)\n",
    "    As = list(set(As))\n",
    "    negAs = join_lists([negate_sent(A)[:1] for A in As]) if has_negA else []\n",
    "\n",
    "    def form_Bs(predicates): \n",
    "        f = (lambda x: x) if predict_relation else mask\n",
    "        return [B_template.format(dt=determiner, ent=f(ent), pred=pred, pred_prefix=predicate_prefix)\n",
    "              for ent, pred in zip(entities, predicates)]\n",
    "\n",
    "    Bs, negBs = {'orig': [], 'rand': []}, {}\n",
    "    for k, group in zip(['orig', 'rand'], [relation_group, rand_relation_group]):\n",
    "        for rel0 in group[0]:\n",
    "            for rel1 in group[1]:\n",
    "                predicates = [\"is %s:%d\" % (comparative(rel), i) for i, rel in enumerate([rel0, rel1])]\n",
    "                Bs[k] += form_Bs(predicates)\n",
    "    for k in Bs:\n",
    "        Bs[k] = list(set(Bs[k]))\n",
    "        if has_negB:\n",
    "            negBs[k] = join_lists([negate_sent(B)[:1] for B in Bs[k]])\n",
    "            Bs[k], negBs[k] = Bs[k] + [swap_entities(negB) for negB in negBs[k]], negBs[k] + [swap_entities(B) for B in Bs[k]]\n",
    "        else:\n",
    "            negBs[k] = [swap_entities(B) for B in Bs[k]]\n",
    "\n",
    "#     def form_sentences(sentence_template, As, Bs, conj, lexical_rel_filter=arbitrary):\n",
    "    def form_sentences(sentence_template, As, Bs, conj):\n",
    "        def extract_rel_id(s): return int(s[s.index(':') + 1])\n",
    "        def get_lexical_rel(rel_id_A, rel_id_B): return 'same' if rel_id_A == rel_id_B else 'opposite'\n",
    "        def strip_rel_id(s, lexical_rel_tag=''):\n",
    "            rel_id_span = s[s.index(':'): s.index(':') + 2]\n",
    "            return s.replace(rel_id_span, lexical_rel_tag)\n",
    "        def compare_and_tag_entity(B, A):\n",
    "            entity = [e for e in entities if e in B][0]\n",
    "            entity_tag = 'former' if A.strip().startswith(entity) else 'latter'\n",
    "            return B.replace(entity, entity + ' ( ' + entity_tag + ' )')                    \n",
    "        \n",
    "        if predict_relation: conj = mask(conj)\n",
    "        As_with_rel_ids = [(A, extract_rel_id(A)) for A in As]\n",
    "        Bs_with_rel_ids = [(B, extract_rel_id(B)) for B in Bs]\n",
    "            \n",
    "        sentences = []\n",
    "        for (A, rel_id_A), (B, rel_id_B) in product(As_with_rel_ids, Bs_with_rel_ids):\n",
    "            lexical_rel = get_lexical_rel(rel_id_A, rel_id_B)\n",
    "            if lexical_rel in lexical_relations:\n",
    "                lexical_rel_tag = ' ( ' + lexical_rel + ' )' if tag_lexical_rel else ''\n",
    "                if tag_entity: B = compare_and_tag_entity(B, A)\n",
    "                sent = sentence_template.format(A=strip_rel_id(A), B=strip_rel_id(B, lexical_rel_tag=lexical_rel_tag), conj=conj)\n",
    "                sent = \" \" + \" \".join(sent.split())\n",
    "                sentences.append(sent)\n",
    "        return sentences\n",
    "#         return [\" \" + \" \".join(sentence_template.format(A=strip_rel_id(A), B=strip_rel_id(B), conj=conj).split()) \n",
    "#                 for A, B in product(As, Bs) if lexical_rel_filter(extract_rel_id(A), extract_rel_id(B))]\n",
    "\n",
    "    sentences = defaultdict(list)\n",
    "    for entailment_template in entailment_templates[-1:]:\n",
    "        for A, B, conj in [(As, Bs['orig'], 'Right'), \n",
    "                           (negAs, negBs['orig'], 'Right'), \n",
    "                           (As, negBs['orig'], 'Wrong'), \n",
    "                           (negAs, Bs['orig'], 'Wrong'),\n",
    "                           (As, Bs['rand'], 'Maybe'), \n",
    "                           (negAs, negBs['rand'], 'Maybe'), \n",
    "                           (As, negBs['rand'], 'Maybe'), \n",
    "                           (negAs, Bs['rand'], 'Maybe'),\n",
    "                          ]:\n",
    "            sentences[conj] += form_sentences(entailment_template, A, B, conj)\n",
    "    assert len(sentences['Right']) == len(sentences['Wrong']), '%d %d' % (len(sentences['Right']), len(sentences['Wrong']))\n",
    "    if has_neutral: sentences['Maybe'] = random.sample(sentences['Maybe'], len(sentences['Right']))\n",
    "    sentences = join_lists(sentences[k] for k in (sentences.keys() if has_neutral else ['Right', 'Wrong']))\n",
    "    \n",
    "    substituted_sent_groups = []\n",
    "    for sent in sentences:\n",
    "        sent_group = []\n",
    "        for _ in range(n_entity_trials):\n",
    "            e0, e1 = random.sample(entity_set, 2)\n",
    "            sent_group.append(sent.replace(entities[0], e0).replace(entities[1], e1))\n",
    "        substituted_sent_groups.append(sent_group)\n",
    "    return sentences, substituted_sent_groups\n",
    "\n",
    "# make_sentences(has_negA=True, has_negB=True, tag_lexical_rel=True, tag_entity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHILDDataset(Dataset):\n",
    "    all_lines = {Split.train: None, Split.dev: None, Split.test: None}\n",
    "    \n",
    "    def __init__(self, all_lines, tokenizer, max_seq_len=None, max_noise_len=0, split_pct=[0.7, 0.3, 0.0], mode=Split.train):\n",
    "        if isinstance(mode, str): mode = Split[mode]\n",
    "        if CHILDDataset.all_lines[mode] is None:\n",
    "            random.shuffle(all_lines)\n",
    "            n_dev = int(round(len(all_lines) * split_pct[1]))\n",
    "            n_test = int(round(len(all_lines) * split_pct[2]))\n",
    "            n_train = len(all_lines) - n_dev - n_test\n",
    "            \n",
    "            def flatten(lines):\n",
    "                if len(lines) > 0 and type(lines[0]) == list: lines = join_lists(lines)\n",
    "                return join_lists(lines) if len(lines) > 0 and type(lines[0]) == list else lines\n",
    "            \n",
    "            CHILDDataset.all_lines[Split.train] = flatten(all_lines[:n_train])\n",
    "            CHILDDataset.all_lines[Split.dev] = flatten(all_lines[n_train: n_train + n_dev])\n",
    "            CHILDDataset.all_lines[Split.test] = flatten(all_lines[n_train + n_dev:])\n",
    "\n",
    "        examples = []\n",
    "        for i, line in enumerate(CHILDDataset.all_lines[mode]):\n",
    "            t1, t2, is_next_label = self.split_sent(line)\n",
    "            tokens_a = rejoin_masked_tokens(tokenizer.tokenize(t1))\n",
    "            tokens_b = rejoin_masked_tokens(tokenizer.tokenize(t2)) if t2 is not None else None\n",
    "            example = InputExample(guid=i, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label)\n",
    "            examples.append(example)\n",
    "\n",
    "        if max_seq_len is None:\n",
    "            max_seq_len = max([len(example.tokens_a) + len(example.tokens_b) + 3\n",
    "                if example.tokens_b is not None else len(example.tokens_a) + 2\n",
    "                for example in examples])\n",
    "\n",
    "        self.features = [convert_example_to_features(example, max_seq_len, tokenizer, max_noise_len=max_noise_len)\n",
    "             for example in examples]\n",
    "\n",
    "    def split_sent(self, line):\n",
    "        label = 0\n",
    "        if \"|||\" in line:\n",
    "            t1, t2 = [t.strip() for t in line.split(\"|||\")]\n",
    "            assert len(t1) > 0 and len(t2) > 0, \"%d %d\" % (len(t1), len(t2))\n",
    "        else:\n",
    "            # assert self.one_sent\n",
    "            t1, t2 = line.strip(), None\n",
    "        return t1, t2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, shortcut = RobertaForMaskedLM, RobertaTokenizer, 'roberta-large'\n",
    "model, tokenizer = None, tokenizer_class.from_pretrained(shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nTrain = 35280, nValid = 15120\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(frames)\n",
    "all_lines = [make_sentences(relation_group=rg, rand_relation_group=frames[(i + 1) % len(frames)], n_entity_trials=10, \n",
    "                            has_negA=True, has_negB=True, tag_lexical_rel=True, tag_entity=True,\n",
    "                            has_neutral=False, predict_relation=True)[1] \n",
    "             for i, rg in enumerate(frames)]\n",
    "all_lines = join_lists(all_lines)\n",
    "# all_lines = join_lists(all_lines)\n",
    "for k in CHILDDataset.all_lines: CHILDDataset.all_lines[k] = None\n",
    "train_dataset = CHILDDataset(all_lines, tokenizer, max_noise_len=0, split_pct=[0.7, 0.3, 0.0], mode='train')\n",
    "eval_dataset = CHILDDataset(all_lines, tokenizer, max_noise_len=0, split_pct=[0.7, 0.3, 0.0], mode='dev')\n",
    "print('nTrain = %d, nValid = %d' % (len(train_dataset), len(eval_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ac8273caa419ba1e9193428168f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43eae0d5d134b95b20e45480dd10095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1103.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.856, 'learning_rate': 1.9395587790873378e-05, 'epoch': 0.091, 'step': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf53e724482141638586621be3e0c8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=237.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.703, 'eval_acc': 0.47, 'eval_stat': 'ĠRight 0.94 0.53, ĠWrong 0.06 0.51, ', 'epoch': 0.091, 'step': 100}\n",
      "{'loss': 0.73, 'learning_rate': 1.8791175581746754e-05, 'epoch': 0.181, 'step': 200}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f54a303744d13bed09862527f653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=237.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.716, 'eval_acc': 0.471, 'eval_stat': 'ĠRight 1.00 0.57, ', 'epoch': 0.181, 'step': 200}\n",
      "{'loss': 0.721, 'learning_rate': 1.818676337262013e-05, 'epoch': 0.272, 'step': 300}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028bbf80f1d2409e909351bdd9c8178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=237.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_class.from_pretrained('roberta-base', model=model)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./models/model_name\", \n",
    "    overwrite_output_dir=True, do_train=True, do_eval=True,\n",
    "    per_device_train_batch_size=32, per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5, num_train_epochs=3,\n",
    "    logging_steps=100, eval_steps=100, save_steps=0,\n",
    "    evaluate_during_training=True,\n",
    ")\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "trainer.tokenizer = tokenizer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataloader = trainer.get_eval_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in dataloader: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 530,  16,  ..., 479,   2,   0],\n",
       "         [  0, 791,  16,  ..., 479,   2,   0],\n",
       "         [  0, 574,  16,  ..., 479,   2,   0],\n",
       "         ...,\n",
       "         [  0, 975,  16,  ...,   2,   0,   0],\n",
       "         [  0, 574,  16,  ...,   2,   0,   0],\n",
       "         [  0, 495,  16,  ..., 479,   2,   0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  ..., 17, 18,  0],\n",
       "         [ 0,  1,  2,  ..., 17, 18,  0],\n",
       "         [ 0,  1,  2,  ..., 17, 18,  0],\n",
       "         ...,\n",
       "         [ 0,  1,  2,  ..., 17,  0,  0],\n",
       "         [ 0,  1,  2,  ..., 17,  0,  0],\n",
       "         [ 0,  1,  2,  ..., 17, 18,  0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer._prepare_inputs(inputs, model)\n",
    "loss, logits = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax((logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = (logits * (inputs['labels'] != -100).unsqueeze(-1)).sum(dim=1).argmax(dim=-1)\n",
    "\n",
    "labels = (inputs['labels'] * (inputs['labels'] != -100)).sum(dim=-1)\n",
    "\n",
    "(pred_labels == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.4\n"
     ]
    }
   ],
   "source": [
    "print('%.1f' % 94.433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠRight 0.6260414719581604\n",
      "ĠRight 0.6174842119216919\n",
      "ĠRight 0.627226710319519\n",
      "ĠRight 0.6074517965316772\n",
      "ĠRight 0.6034893989562988\n",
      "ĠRight 0.5928120613098145\n",
      "ĠMaybe 0.9999963045120239\n",
      "ĠMaybe 0.9999955892562866\n",
      "ĠMaybe 0.9999963045120239\n",
      "ĠRight 0.6209385395050049\n",
      "ĠRight 0.6153814196586609\n",
      "ĠRight 0.6189284324645996\n",
      "ĠRight 0.6173365712165833\n",
      "ĠRight 0.6170286536216736\n",
      "ĠRight 0.6148619651794434\n",
      "ĠMaybe 0.9999971389770508\n",
      "ĠMaybe 0.9999972581863403\n",
      "ĠMaybe 0.9999969005584717\n",
      "ĠRight 0.6081119179725647\n",
      "ĠRight 0.6188942790031433\n",
      "ĠRight 0.6106655597686768\n",
      "ĠRight 0.6149485111236572\n",
      "ĠRight 0.6151918172836304\n",
      "ĠRight 0.6127419471740723\n",
      "ĠRight 0.6444377303123474\n",
      "ĠRight 0.6497629284858704\n",
      "ĠRight 0.6418505311012268\n",
      "ĠRight 0.6264133453369141\n",
      "ĠRight 0.6273400783538818\n",
      "ĠRight 0.6344654560089111\n",
      "ĠRight 0.6158509254455566\n",
      "ĠRight 0.6200970411300659\n",
      "ĠRight 0.6208559274673462\n",
      "ĠMaybe 0.9999988079071045\n",
      "ĠMaybe 0.9999986886978149\n",
      "ĠMaybe 0.9999988079071045\n",
      "ĠMaybe 0.9999974966049194\n",
      "ĠMaybe 0.9999971389770508\n",
      "ĠMaybe 0.9999974966049194\n",
      "ĠMaybe 0.9999963045120239\n",
      "ĠMaybe 0.999996542930603\n",
      "ĠMaybe 0.9999966621398926\n",
      "ĠRight 0.623246967792511\n",
      "ĠRight 0.6270768046379089\n",
      "ĠRight 0.618299663066864\n",
      "ĠMaybe 0.9999973773956299\n",
      "ĠMaybe 0.9999972581863403\n",
      "ĠMaybe 0.9999977350234985\n",
      "ĠMaybe 0.9999961853027344\n",
      "ĠMaybe 0.9999959468841553\n",
      "ĠMaybe 0.9999960660934448\n",
      "ĠRight 0.6204316020011902\n",
      "ĠRight 0.6157919764518738\n",
      "ĠRight 0.6199951171875\n",
      "ĠMaybe 0.9999948740005493\n",
      "ĠMaybe 0.9999945163726807\n",
      "ĠMaybe 0.9999958276748657\n",
      "ĠRight 0.6182829141616821\n",
      "ĠRight 0.6187686324119568\n",
      "ĠRight 0.6175054907798767\n",
      "ĠRight 0.6209660768508911\n",
      "ĠRight 0.6262878775596619\n",
      "ĠRight 0.6350439786911011\n",
      "ĠRight 0.6232182383537292\n"
     ]
    }
   ],
   "source": [
    "for i, label_id in enumerate(pred_labels):\n",
    "    print(tokenizer._convert_id_to_token(label_id.item()), probs[i, label_id].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([40, 24,  0], device='cuda:0'),\n",
       "indices=tensor([5143, 5359,    0], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels.bincount().topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([24, 22, 18], device='cuda:0'),\n",
       "indices=tensor([ 5359,  5143, 31273], device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.bincount().topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĠMaybe'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(5359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = probs.topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 235, 13984, 10039]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠLeft']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['ĠWrong', 'ĠRight', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[31273, 5143, 235, 13984, 38103]\n",
      "['ĠWrong', 'ĠRight', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'Ġwrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 13984, 235, 37234]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠCorrect']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'ĠRIGHT']\n",
      "[5143, 31273, 235, 13984, 1593]\n",
      "['ĠRight', 'ĠWrong', 'Ġright', 'Right', 'Ġwrong']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n",
      "[5143, 31273, 13984, 235, 38103]\n",
      "['ĠRight', 'ĠWrong', 'Right', 'Ġright', 'ĠRIGHT']\n"
     ]
    }
   ],
   "source": [
    "for top_idx in indices:\n",
    "    print(top_idx.tolist())\n",
    "    print(tokenizer.convert_ids_to_tokens(top_idx.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
