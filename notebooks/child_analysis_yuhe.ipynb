{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/xd/projects/transformers/notebooks\n",
      "/nas/xd/transformers/src\n",
      "/home/yuhe/Application/snippets\n",
      "/nas/xd/projects/transformers/notebooks\n",
      "/nas/xd/transformers/src\n",
      "/home/yuhe/Application/snippets\n",
      "/home/yuhe/Application/python36/lib/python36.zip\n",
      "/home/yuhe/Application/python36/lib/python3.6\n",
      "/home/yuhe/Application/python36/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/yuhe/Application/python36/lib/python3.6/site-packages\n",
      "/home/yuhe/Application/python36/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/yuhe/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "custom_path = [\n",
    "'/nas/xd/projects/transformers/notebooks',\n",
    "'/nas/xd/transformers/src',\n",
    "'/home/yuhe/Application/snippets'\n",
    "]\n",
    "\n",
    "sys.path = custom_path + sys.path\n",
    "print('\\n'.join(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/raid/xd/.cache/torch'\n",
    "from types import MethodType\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers.data.data_collator import DataCollator, default_data_collator\n",
    "from transformers import AutoConfig, pipeline\n",
    "from transformers import (\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPTNeoForCausalLM,\n",
    ")\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import HfArgumentParser, Trainer, TrainingArguments, set_seed\n",
    "# from transformers.trainer_utils import EvaluationStrategy\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "from functional.pipeline import Sequence\n",
    "from fn import _\n",
    "from collections import namedtuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from child_utils import *\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " set and i want to use the result set in\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# openai.api_key = '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'\n",
    "openai.api_key = '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'\n",
    "\n",
    "text = 'i want to query some gpt3 result'\n",
    "response = openai.Completion.create(engine=\"davinci\", prompt=text, temperature=0.1, max_tokens=10)\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "cache_dir = '/nas/xd/.cache/torch/transformers/'  # for models besides t5-3b/11b\n",
    "proxies = {'http': '192.168.50.1:1081'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-2.7B/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/3c80ef2946e1aacc6dd37cb986ea989c29c92775701655bedf14d8791825a30b.f1ede5af01beb85af6cba189a5671dbac3fe256282f737ff0fedf1db882ca729\n",
      "In cached_path: url_or_filename = https://huggingface.co/EleutherAI/gpt-neo-2.7B/resolve/main/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/0839a11efa893f2a554f8f540f904b0db0e5320a2b1612eb02c3fd25471c189a.a144c17634fa6a7823e398888396dd623e204dce9e33c3175afabfbf24bd8f56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/vocab.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/merges.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/added_tokens.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = https://huggingface.co/gpt2/resolve/main/tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name, proxies=proxies, cache_dir=cache_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "models[model_name] = model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'roberta-large'\n",
    "# model_name = 'gpt2-xl'\n",
    "model_name = 'EleutherAI/gpt-neo-2.7B'\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "model, tokenizer = models[model_name]\n",
    "\n",
    "masked_lm = tokenizer.mask_token is not None and len(tokenizer.additional_special_tokens) == 0\n",
    "if masked_lm:\n",
    "    mask_token = tokenizer.mask_token  # '<mask>' for roberta\n",
    "elif len(tokenizer.additional_special_tokens) > 0:\n",
    "    mask_token = tokenizer.additional_special_tokens[0]  # '<sxtra_id_0>' for t5\n",
    "else:\n",
    "    mask_token = ''  # for gpt2\n",
    "if masked_lm: nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blocks = model.transformer.h\n",
    "L, H = model.config.num_layers, model.config.num_heads\n",
    "hidden_size = model.config.hidden_size\n",
    "all_attrs, all_embs = defaultdict(dict), defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adapted from attattr\n",
    "def scaled_input(emb, num_points, baseline=None):\n",
    "    # shape of emb: (bsz, num_head, seq_len, seq_len)\n",
    "    assert emb.size(0) == 1\n",
    "    if baseline is None: baseline = torch.zeros_like(emb)   \n",
    "    step = (emb - baseline) / num_points\n",
    "    res = torch.cat([baseline + step * (i + 1) for i in range(num_points)], dim=0)  # XD\n",
    "    return res, step\n",
    "\n",
    "# from https://discuss.pytorch.org/t/get-top-k-indices-values-of-all-rows/89354\n",
    "def unravel_index(index, shape):\n",
    "    out = []\n",
    "    for dim in reversed(shape):\n",
    "        out.append(index % dim)\n",
    "        index = index // dim\n",
    "    r = tuple(reversed(out))\n",
    "    return torch.cat([i.unsqueeze(-1) for i in r], dim=-1).cpu().tolist() if type(index) in [torch.Tensor] else r\n",
    "\n",
    "def h2topk(h, k=4, return_probs=True):\n",
    "    if not hasattr(h2topk, 'ln') or h2topk.ln.normalized_shape[0] != h.size(-1):\n",
    "        h2topk.ln = nn.LayerNorm(h.size(-1))\n",
    "#     r = model.lm_head(h2topk.ln(h))\n",
    "    r = model.lm_head(h)\n",
    "    if return_probs: r = r.softmax(-1)\n",
    "    return r.topk(k, dim=-1) if k > 0 else r\n",
    "\n",
    "def globalize(tensor):\n",
    "    if tensor.dim() == 4: return tensor  # global attention\n",
    "    assert tensor.dim() == 5, str(tensor.dim())\n",
    "    assert tensor.size(1) == 1, str(tensor.size(1))  # num_blocks\n",
    "    seq_len = tensor.size(3)\n",
    "    return tensor.squeeze(1)[:, :, :, -seq_len:]  # (bsz, num_blocks, H, seq_len, block_len) -> (bsz, H, seq_len, seq_len)\n",
    "\n",
    "def append_tokens_to_positions(position_tensor):\n",
    "    positions = numpy(position_tensor)\n",
    "    return ['%d %s' % (p, tokens[p]) for p in positions]\n",
    "\n",
    "def getdelattr(obj, name):\n",
    "    r = getattr(obj, name, None)\n",
    "    if hasattr(obj, name): delattr(obj, name)\n",
    "    return r\n",
    "\n",
    "def try_delattr(obj, name):\n",
    "    if hasattr(obj, name): delattr(obj, name)\n",
    "\n",
    "def get_attn_module(block):\n",
    "    m = block.attn\n",
    "    if hasattr(m, 'attention'): m = m.attention  # for gpt-neo\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heatmap(a, figsize=(20, 1), cbar=False):\n",
    "    _ = plt.figure(figsize=figsize)\n",
    "    _ = sns.heatmap(numpy(a, decimals=None), cbar=cbar)\n",
    "    plt.show()\n",
    "    \n",
    "def plot(a, figsize=(20, 2)):\n",
    "    _ = plt.figure(figsize=figsize)\n",
    "    _ = plt.plot(numpy(a))\n",
    "    \n",
    "def plot_hidden(hidden, topk=4):\n",
    "    if hidden.dim() == 3 and hidden.size(0) == 1: hidden = hidden.squeeze(0)\n",
    "    assert hidden.dim() == 2, str(hidden.dim())\n",
    "    heatmap(hidden, figsize=(20, 5))\n",
    "    hidden_mean = hidden.mean(dim=0)\n",
    "    _ = plt.figure(figsize=(20, 2)); plt.xlim((0, hidden.size(1))); plt.plot(numpy(hidden_mean))\n",
    "    return hidden_mean.topk(topk), hidden_mean.topk(topk, largest=False)\n",
    "\n",
    "def plot_top_weight(weight, topk=4):\n",
    "    wm = weight.norm(dim=-1)\n",
    "    plot(wm, figsize=(20, 2))\n",
    "    values, indices = wm.topk(topk)\n",
    "    heatmap(weight[indices], figsize=(20, 1))\n",
    "    return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unravel(i): return i // hidden_size, i % hidden_size\n",
    "def indices_fn(indices): return [unravel(i) for i in numpy(indices)]\n",
    "\n",
    "# wvo = wo.matmul(wv)\n",
    "# show_topk(*wvo.view(-1).topk(5), indices_fn=indices_fn)\n",
    "# show_topk(*wvo.view(-1).topk(5, largest=False), indices_fn=indices_fn)\n",
    "\n",
    "def attn_out_transform(self, attn_out, alpha=1.0):\n",
    "    wv = self.v_proj.weight.view(H, -1, hidden_size)[head]\n",
    "    i = wv.norm(dim=0).argmax().item()\n",
    "    w0, w1 = wv[:, i], attn_out[0, head, src]\n",
    "    attn_out[0, head, src] = w0 * (w1.max() / w0.max() + w1.min() / w0.min()) / 2 * alpha\n",
    "    return attn_out\n",
    "\n",
    "def get_detach_fn(pos=None):\n",
    "    def detach(hidden):\n",
    "        if pos is None: return hidden.detach()\n",
    "        h0, h1, h2 = hidden[:, :pos], hidden[:, pos: pos + 1], hidden[:, pos + 1:]\n",
    "        h1 = h1.detach()\n",
    "        return torch.cat([h0, h1, h2], dim=1)\n",
    "    return detach\n",
    "\n",
    "def get_detach_heads_fn(kept_head=None):\n",
    "    def detach_heads(attn_weights):\n",
    "        if kept_head is None: return attn_weights.detach()\n",
    "        assert attn_weights.dim() == 4\n",
    "        h0, h1, h2 = (\n",
    "            attn_weights[:, :kept_head],\n",
    "            attn_weights[:, kept_head : kept_head + 1],\n",
    "            attn_weights[:, kept_head + 1 :],\n",
    "        )\n",
    "        h0, h2 = h0.detach(), h2.detach() \n",
    "        return torch.cat([h0, h1, h2], dim=1)\n",
    "    return detach_heads\n",
    "\n",
    "def get_scale_fn(factor=0):\n",
    "    def scale(hidden): return hidden * factor\n",
    "    return scale\n",
    "\n",
    "def plot_attn(attn, annot=False, figsize=(10, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    res = sns.heatmap(\n",
    "        numpy(attn),\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        annot=annot,\n",
    "        fmt=\"d\",\n",
    "        linewidths=0.1,\n",
    "        linecolor=\"grey\",\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "    )\n",
    "    _ = res.set_xticklabels(res.get_xmajorticklabels(), fontsize=8, rotation=0)\n",
    "    _ = res.set_yticklabels(res.get_ymajorticklabels(), fontsize=8, rotation=0)\n",
    "    # _ = plt.xlabel('%d-%d    %.4f' % (layer, head, v), fontsize=14)\n",
    "    res.tick_params(top=True, right=True, labeltop=True, labelright=True)\n",
    "    plt.show()\n",
    "\n",
    "def cluster(emb, labels, n_clusters=3):\n",
    "    assert emb.shape[0] == labels.shape[0], '%d ！= %d' % (emb.shape[0], labels.shape[0])\n",
    "    centroids = emb.reshape(n_clusters, len(labels) // n_clusters, emb.shape[-1]).mean(axis=1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters)#, init=centroids)\n",
    "    labels_ = kmeans.fit(emb).labels_\n",
    "    for label in list(set(labels)):\n",
    "        if Counter(labels_[labels == label]).most_common()[0][1] < (labels == label).sum():# - abs(label):\n",
    "#             print(label)\n",
    "            return False, labels_\n",
    "    return True, labels_\n",
    "\n",
    "def visualize_by_pca(emb, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    data = pca.fit_transform(emb)\n",
    "    _ = plt.scatter(\n",
    "        data[:, 0], data[:, 1], c=labels, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"jet\", 3))\n",
    "    _ = plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def get_query(self, h):\n",
    "    query = self.q_proj(h)\n",
    "    query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "    query = query[0, head2, src:src+1]\n",
    "    return query\n",
    "\n",
    "def get_key(self, h):\n",
    "    key = self.k_proj(h)\n",
    "    key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "    key = key[0, head2, :]\n",
    "    return key\n",
    "\n",
    "def get_head_weights(layer, head):\n",
    "    m = get_attn_module(blocks[layer])\n",
    "    wq = m.q_proj.weight.view(H, -1, hidden_size)[head]\n",
    "    wk = m.k_proj.weight.view(H, -1, hidden_size)[head]\n",
    "    wv = m.v_proj.weight.view(H, -1, hidden_size)[head]\n",
    "    wo = m.out_proj.weight.view(hidden_size, H, -1)[:, head]\n",
    "#     return wq, wk, wv, wo\n",
    "    return wq.t(), wk, wv.t(), wo.t()\n",
    "\n",
    "def plot_tgt_attn(a, ax=None, title=None):\n",
    "#     print(a.view(-1)[tgt_positions[4:]].mean())\n",
    "    labels = np.array(tokens).reshape(nrows, -1)\n",
    "    relative_tgt_positions = tgt_positions % a.size(1) # == ncols + 3\n",
    "    right_attn = a.argmax(1) == relative_tgt_positions\n",
    "    yticklabels = ['' if i else 'x' for i in right_attn]\n",
    "    if ax is None:\n",
    "        _ = plt.figure(figsize=(2.5 * a.size(1) / 9, 5 * a.size(0) / 24))\n",
    "        _ = sns.heatmap(numpy(a) ,cbar=False, annot=labels, fmt='', xticklabels=False, yticklabels=yticklabels)\n",
    "        if title is not None: plt.title(title)\n",
    "    else:\n",
    "        _ = sns.heatmap(numpy(a), cbar=False, annot=labels, fmt=\"\", xticklabels=False, yticklabels=yticklabels, ax=ax)\n",
    "        if title is not None: ax.set_title(title)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_detach_pairs(module, exit_module, detach_type='output'):\n",
    "    assert detach_type in ['output', 'residual']\n",
    "    pairs = []\n",
    "    for block in blocks:\n",
    "        if module in [block, get_attn_module(block)]: pairs += [(block, 'ffn_%s_transform' % detach_type)]\n",
    "        elif block == exit_module: break\n",
    "        elif pairs: pairs += [(block, 'attn_%s_transform' % detach_type), (block, 'ffn_%s_transform' % detach_type)]\n",
    "    return pairs\n",
    "\n",
    "def gen_detach_heads_tuples(module, exit_module, kept_layer, kept_head):\n",
    "    tuples = None\n",
    "    for i, block in enumerate(blocks):\n",
    "        if module in [block, get_attn_module(block)]: tuples = []\n",
    "        elif block == exit_module: break\n",
    "        elif tuples is not None:\n",
    "            tuples.append((get_attn_module(block), 'attn_weights_transform',\n",
    "                          get_detach_heads_fn(kept_head=kept_head if i == kept_layer else None)))\n",
    "    return tuples\n",
    "\n",
    "def forward(module, names, values=None, exit_module=None, extra_tuples=None,\n",
    "            detach_type=None, detach_pos=None, kept_layer=None, kept_head=None):\n",
    "    if type(names) != list: names, values = [names], [values]\n",
    "    if type(names) == list and type(values) != list: values = [values for _ in range(len(names))]\n",
    "    for name, value in zip(names, values): setattr(module, name, value)\n",
    "    if exit_module is not None: setattr(exit_module, 'exit', True)\n",
    "    if extra_tuples is not None:\n",
    "        for m, name, val in extra_tuples: setattr(m, name, val)\n",
    "    if detach_type is not None:\n",
    "        detach_pairs = gen_detach_pairs(module, exit_module, detach_type=detach_type)\n",
    "        for m, name in detach_pairs: setattr(m, name, get_detach_fn(detach_pos))\n",
    "    if kept_head is not None:\n",
    "        detach_tuples = gen_detach_heads_tuples(module, exit_module, kept_layer=kept_layer, kept_head=kept_head)\n",
    "        for m, name, fn in detach_tuples: setattr(m, name, fn)\n",
    "    try: outputs = model(**inputs, output_attentions=True, output_hidden_states=exit_module is not None)\n",
    "    finally:\n",
    "        if values[0] is None: embs = [getattr(module, name) for name in names]\n",
    "        for name in names: try_delattr(module, name)\n",
    "        if exit_module is not None: try_delattr(exit_module, 'exit')\n",
    "        if detach_type is not None:\n",
    "            for m, name in detach_pairs: try_delattr(m, name)\n",
    "        if kept_head is not None:\n",
    "            for m, name, _ in detach_tuples: try_delattr(m, name)\n",
    "        if extra_tuples is not None:\n",
    "            for m, name, _ in extra_tuples: try_delattr(m, name)\n",
    "    if values[0] is None and len(names) == 1: embs = embs[0]\n",
    "    return embs if values[0] is None else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(hidden, query, key=None, logits=None, always_show=False):\n",
    "    if logits is None:\n",
    "        if key is None:\n",
    "            key = self.k_proj(hidden)\n",
    "            key = self._split_heads(key, self.num_heads, self.head_dim)[0, head2]\n",
    "        logits = (query * key).sum(dim=-1)\n",
    "    else:\n",
    "        always_show = True\n",
    "    cand_pos = torch.LongTensor(cand_positions).view(-1, n_candidates)\n",
    "    is_extremal = [logits[p] == logits[cand_pos[i]].max() for i, p in enumerate(tgt_positions)]\n",
    "    if always_show or sum(is_extremal[1:]) / len(tgt_positions[1:]) > 0.9:\n",
    "        logits[0] = logits[1]\n",
    "        plot(logits)\n",
    "        _ = plt.xticks(range(len(logits)), tokens)\n",
    "        for p, b in zip(tgt_positions, is_extremal): plt.axvline(x=p, color='gray' if b else 'r')\n",
    "        plt.show()\n",
    "        probs = logits[cand_positions].view(-1, n_candidates).softmax(-1)[cand_is_tgt]\n",
    "        print(numpy(probs), '\\n', probs.mean())\n",
    "        return True\n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_mask(from_positions, to_positions, accum=False):\n",
    "    mask = torch.zeros(1, seq_len, seq_len)\n",
    "    for i in range(0, nrows):\n",
    "        if not accum:\n",
    "            mask[:, from_positions[i], to_positions[i]] = 1\n",
    "        else:\n",
    "            mask[:, from_positions[i], to_positions[:i]] = 1 / i if i > 0 else 0\n",
    "    return mask\n",
    "\n",
    "combined_weights = {}\n",
    "\n",
    "def get_combined_w(layer, head, qk=False):\n",
    "    if (layer, head, qk) in combined_weights: return combined_weights[(layer, head, qk)]\n",
    "    wq, wk, wv, wo = get_head_weights(layer, head)\n",
    "    w = torch.matmul(wq, wk) if qk else torch.matmul(wv, wo)\n",
    "    combined_weights[(layer, head, qk)] = w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_tgt_attn_losses(labels, losses, losses1):\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=(20, 4))\n",
    "    losses, losses1 = [int(l*100) for l in losses], [int(l*100) for l in losses1]\n",
    "    rects1 = ax.bar(x - width/2, losses, width, label='loss')\n",
    "    rects2 = ax.bar(x + width/2, losses1, width, label='loss1')\n",
    "    _ = ax.set_xticks(x)\n",
    "    _ = ax.set_xticklabels(labels)\n",
    "    _ = ax.legend()\n",
    "    _ = ax.bar_label(rects1, padding=3)\n",
    "    _ = ax.bar_label(rects2, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['identity', 'lower', 'upper', 'double', 'x10', 'to_cardinal', 'to_ordinal'])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_fns = {\n",
    "    identity.__name__: identity, lower.__name__: upper, upper.__name__: lower, \n",
    "    double.__name__: single, x10.__name__: d10,\n",
    "    to_cardinal.__name__: to_digit, to_ordinal.__name__: to_digit}\n",
    "inverse_fns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nl_des = [(1, \"One\", \"1st\", \"First\"), (2, \"Two\", \"2nd\", \"Second\"),\n",
    "          (3, \"Three\", \"3rd\", \"Third\"), (4, \"Four\", \"4th\", \"Fourth\"),\n",
    "          (5, \"Five\", \"5th\", \"Fifth\"), (6, \"Six\", \"6th\", \"Sixth\"),\n",
    "          (7, \"Seven\", \"7th\", \"Seventh\"), (8, \"Eight\", \"8th\", \"Eighth\"),\n",
    "          (9, \"Nine\", \"9th\", \"Ninth\"), (10, \"Ten\", \"10th\", \"Tenth\"),\n",
    "          (11, \"Eleven\", \"11th\", \"Eleventh\"), (12, \"Twelve\", \"12th\", \"Twelfth\"),\n",
    "          (13, \"Thirteen\", \"13th\", \"Thirteenth\"), (14, \"Fourteen\", \"14th\", \"Fourteenth\"),\n",
    "          (15, \"Fifteen\", \"15th\", \"Fifteenth\"), (16, \"Sixteen\", \"16th\", \"Sixteenth\"),\n",
    "          (17, \"Seventeen\", \"17th\", \"Seventeenth\"), (18, \"Eighteen\", \"18th\", \"Eighteenth\"),\n",
    "          (19, \"Nineteen\", \"19th\", \"Nineteenth\"), (20, \"Twenty\", \"20th\", \"Twentieth\"),\n",
    "          (30, \"Thirty\", \"30th\", \"Thirtieth\"), (40, \"Forty\", \"40th\", \"Fortieth\"),\n",
    "          (50, \"Fifty\", \"50th\", \"Fiftieth\"), (60, \"Sixty\", \"60th\", \"Sixtieth\"),\n",
    "          (70, \"Seventy\", \"70th\", \"Seventieth\"), (80, \"Eighty\", \"80th\", \"Eightieth\"),\n",
    "          (90, \"Ninety\", \"90th\", \"Ninetieth\"), (100, \"One hundred\", \"100th\", \"Hundredth\"),\n",
    "          (1000, \"One thousand\", \"1000th\", \"Thousandth\")]\n",
    "order_str_2_int = {i[3]: i[0] for i in nl_des}\n",
    "order_int_2_str = {i[0]: i[3] for i in nl_des}\n",
    "\n",
    "def order2int(order_str): return order_str_2_int.get(order_str, 2) - 1\n",
    "\n",
    "def order2str(order_int): return order_int_2_str.get(order_int + 1, 'Second') # index begin from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_letters = upper_letters + lower_letters + [double(l) for l in upper_letters]\n",
    "all_digits = list(chain.from_iterable([[fn(i) for i in digits] for fn in [identity, x10, double]]))\n",
    "\n",
    "digit_fns = [identity, identity, to_cardinal, to_ordinal, double, x10]\n",
    "upper_letter_fns = [identity, lower,]# double]\n",
    "lower_letter_fns = [identity, upper,]# double]\n",
    "vocabs = [(upper_letters, upper_letter_fns, to_rand_digit), \n",
    "          (lower_letters, lower_letter_fns, to_rand_digit), \n",
    "          (digits, digit_fns, to_rand_letter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_query_str(instruction, query):\n",
    "    if instruction is None and query is None: return ''\n",
    "    s = '.'\n",
    "    if instruction is not None: s = s + ' ' + instruction\n",
    "    if query is not None:\n",
    "        if type(query) in [int, bool, str]: query = [query]\n",
    "        if type(query) == dict:\n",
    "            s = s + \" \" + \"{\" + \",\".join(\n",
    "                [\" replace %s with %s\" % (str(k), str(v)) for k, v in query.items()]) + \" }\"\n",
    "        elif type(query) in [list,]:\n",
    "            s = s + ' ' + ' '.join([str(i) for i in query])\n",
    "    return s\n",
    "\n",
    "def make_example_str(example, with_instruction=False):\n",
    "    instruction, l, query, ans = example\n",
    "    if type(ans) not in [Sequence, list]: ans = [ans]\n",
    "    ans = [str(i) for i in ans]\n",
    "    return '%s -> %s' % (' '.join(l) + make_query_str(\n",
    "        instruction if with_instruction else None, query), ' '.join(ans))\n",
    "\n",
    "def sample_rand_len(vocab, k): return sample(vocab, k=randint(1, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def ith_element(l, query=None): return seq(l).slice(2, 3)\n",
    "def ith_element(l, query=None): return seq(l).enumerate().filter(_[0] == order2int(query)).select(_[1])\n",
    "def ith_group(l, query=None): return seq(l).group_by(_).select(_[1]).slice(1, 2).flatten() #.distinct()# davinci F w/ and wo dist\n",
    "# def element_at_index(l, query): return seq(l).slice(query, query + 1) # davinci F\n",
    "def element_at_index(l, query): return seq(l).enumerate().filter(_[0] == 1).select(_[1])\n",
    "def replace(l, query): return seq(l).map(lambda x: query.get(x, x))\n",
    "def replace_with_the_other(l, query): # davinci F\n",
    "    query = {k: (set(l) - {k}).pop() for k in l}\n",
    "    return replace(l, query)\n",
    "def replace_all_with(l, query): return seq(l).map(lambda x: query)  # davinci F?!\n",
    "def interleave_with(l, query): return seq(l).flat_map(lambda x: [x, query])  # davinci T!!\n",
    "def unique_elements(l, query=None): return seq(l).distinct() # davinci F\n",
    "def how_many_unique_elements(l, query=None): return seq(l).distinct().len()  # davinci F\n",
    "def how_many(l, query): return seq(l).filter(_ == query).len() # davinci F\n",
    "def select_same_as(l, query): return seq(l).filter(_ == query) # simpler version of how_many. davinci F\n",
    "def select_same_number_as(l, query):\n",
    "    return seq(l).group_by(_).select(_[1]).filter(lambda x: len(x) == len(query)).flatten()  # F\n",
    "def includes(l, query): return seq(l).union(seq(query)).distinct().len() == seq(l).distinct().len() # davinci F\n",
    "def is_included_by(l, query): return seq(l).difference(seq(query)).empty() # davinci F\n",
    "\n",
    "tasks = [\n",
    "    (ith_element, None, sample, lambda l, vocab, k: \"Second\"),\n",
    "    (ith_element, None, sample, lambda l, vocab, k: order2str(randint(0, 1))),\n",
    "    (\n",
    "        ith_group,\n",
    "        None,\n",
    "        lambda vocab, k: seq(sample(vocab, k)).map(lambda x: [x] * randint(1, 3)).flatten().list(),\n",
    "        None,\n",
    "    ),\n",
    "    (element_at_index, lambda: upper_letters, sample, lambda l, vocab, k: randint(0, min(2, len(l) - 1))),\n",
    "    (replace, None, sample, lambda l, vocab, k: {choice(l): choice(vocab)}),\n",
    "    (\n",
    "        replace_with_the_other,\n",
    "        lambda: sample(upper_letters, 2),\n",
    "        lambda vocab, k: sample(vocab + choices(vocab, k=k - 2), k),\n",
    "        None,\n",
    "    ),\n",
    "    (replace_all_with, None, sample_rand_len, lambda l, vocab, k: choice(vocab)),\n",
    "    (interleave_with, None, sample_rand_len, lambda l, vocab, k: choice(vocab)),\n",
    "    (unique_elements, lambda: sample(upper_letters, 3), choices, None),\n",
    "    (how_many_unique_elements, lambda: sample(upper_letters, 3), choices, None),\n",
    "    (how_many, lambda: sample(upper_letters, 3), choices, lambda l, vocab, k: choice(list(set(l)))),\n",
    "    (select_same_as, lambda: sample(upper_letters, 3), choices, lambda l, vocab, k: choice(list(set(l)))),\n",
    "    (\n",
    "        select_same_number_as,\n",
    "        None,\n",
    "        lambda vocab, k: seq(sample(vocab, k)).map(lambda x: [x] * randint(1, 3)).flatten().list(),\n",
    "        lambda l, vocab, k: [choice(vocab)] * randint(1, 3),\n",
    "    ),\n",
    "    (includes, lambda: sample(upper_letters, 6), sample, lambda l, vocab, k: sample(vocab, 3)),\n",
    "    (is_included_by, lambda: sample(upper_letters, 6), sample, lambda l, vocab, k: sample(vocab, 5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ith element', ['6', 'R', 'L', '0'], 'Second', ['R']],\n",
       " ['ith element', ['0', 'O', 'Q', 'G'], 'Second', ['O']],\n",
       " ['ith element', ['A', 'W', 'X', '9'], 'First', ['A']],\n",
       " ['ith element', ['N', 'D', 'P', 'X'], 'Second', ['D']],\n",
       " ['ith element', ['J', '5', 'L', 'Z'], 'First', ['J']],\n",
       " ['ith element', ['Q', '9', 'V', 'B'], 'First', ['Q']],\n",
       " ['ith element', ['M', 'W', 'V', 'B'], 'Second', ['W']],\n",
       " ['ith element', ['8', '9', '0', 'L'], 'Second', ['9']],\n",
       " ['ith element', ['2', 'B', '1', 'Q'], 'First', ['2']],\n",
       " ['ith element', ['E', 'O', '5', '7'], 'Second', ['O']],\n",
       " ['ith element', ['6', 'S', 'Q', 'X'], 'Second', ['S']],\n",
       " ['ith element', ['A', 'U', 'W', 'N'], 'Second', ['U']],\n",
       " ['ith element', ['X', 'P', 'S', '3'], 'Second', ['P']],\n",
       " ['ith element', ['5', 'E', 'Q', '3'], 'First', ['5']],\n",
       " ['ith element', ['8', 'Y', 'E', 'M'], 'Second', ['Y']],\n",
       " ['ith element', ['E', 'A', 'V', 'Q'], 'First', ['E']]]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6 R L 0. Second -> R\n",
      "0 O Q G. Second -> O\n",
      "A W X 9. First -> A\n",
      "N D P X. Second -> D\n",
      "J 5 L Z. First -> J\n",
      "Q 9 V B. First -> Q\n",
      "M W V B. Second -> W\n",
      "8 9 0 L. Second -> 9\n",
      "2 B 1 Q. First -> 2\n",
      "E O 5 7. Second -> O\n",
      "6 S Q X. Second -> S\n",
      "A U W N. Second -> U\n",
      "X P S 3. Second -> P\n",
      "5 E Q 3. First -> 5\n",
      "8 Y E M. Second -> Y\n",
      "E A V Q. First -> E\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_vocab = string.ascii_uppercase + string.digits\n",
    "transform_fn, vocab_fn, sample_fn, query_fn = tasks[1]\n",
    "instruction = transform_fn.__name__.replace('_', ' ')\n",
    "if vocab_fn is None: vocab_fn = lambda: full_vocab\n",
    "if query_fn is None: query_fn = lambda *_: None\n",
    "nrows, ncols = 16, 4\n",
    "examples = []\n",
    "query = None\n",
    "for i in range(nrows):\n",
    "    vocab = vocab_fn()\n",
    "    l = sample_fn(vocab, k=ncols)\n",
    "    query = query_fn(l, vocab, ncols)\n",
    "    examples.append([instruction, l, query, transform_fn(l, query=query)])\n",
    "examples\n",
    "\n",
    "text = '\\n'.join([make_example_str(e, with_instruction=False) for e in examples])\n",
    "text = '\\n' + text + '\\n'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 6 R L 0. Second -> R\t ĠR 0.019 {'ĠSecond': 0.067, 'Ġ0': 0.045, 'Ġ1': 0.041, 'Ċ': 0.033, 'ĠFirst': 0.03}\n",
      "1\t 0 O Q G. Second -> O\t*ĠO 0.515 {'ĠO': 0.515, 'ĠQ': 0.209, 'ĠG': 0.071, 'Ġ0': 0.033, 'ĠR': 0.031}\n",
      "2\t A W X 9. First -> A\t*ĠA 0.554 {'ĠA': 0.554, 'ĠW': 0.104, 'ĠQ': 0.044, 'ĠO': 0.028, 'ĠG': 0.027}\n",
      "3\t N D P X. Second -> D\t ĠD 0.043 {'ĠN': 0.87, 'ĠD': 0.043, 'ĠQ': 0.019, 'ĠX': 0.019, 'ĠP': 0.013}\n",
      "4\t J 5 L Z. First -> J\t*ĠJ 0.589 {'ĠJ': 0.589, 'ĠL': 0.165, 'ĠZ': 0.126, 'Ġ5': 0.036, 'ĠA': 0.007}\n",
      "5\t Q 9 V B. First -> Q\t*ĠQ 0.983 {'ĠQ': 0.983, 'ĠB': 0.003, 'ĠR': 0.002, 'ĠJ': 0.001, 'ĠO': 0.001}\n",
      "6\t M W V B. Second -> W\t*ĠW 0.648 {'ĠW': 0.648, 'ĠV': 0.193, 'ĠM': 0.091, 'ĠB': 0.02, 'ĠQ': 0.011}\n",
      "7\t 8 9 0 L. Second -> 9\t Ġ9 0.105 {'Ġ8': 0.438, 'Ġ9': 0.105, 'ĠQ': 0.09, 'Ġ0': 0.084, 'ĠZ': 0.04}\n",
      "8\t 2 B 1 Q. First -> 2\t*Ġ2 0.646 {'Ġ2': 0.646, 'ĠB': 0.279, 'Ġ1': 0.018, 'Ġ8': 0.007, 'Ġ3': 0.006}\n",
      "9\t E O 5 7. Second -> O\t ĠO 0.317 {'ĠE': 0.656, 'ĠO': 0.317, 'ĠQ': 0.002, 'Ġ0': 0.002, 'ĠB': 0.002}\n",
      "10\t 6 S Q X. Second -> S\t*ĠS 0.745 {'ĠS': 0.745, 'ĠQ': 0.194, 'Ġ6': 0.037, 'Ġ7': 0.003, 'Ġ8': 0.003}\n",
      "11\t A U W N. Second -> U\t*ĠU 0.645 {'ĠU': 0.645, 'ĠA': 0.334, 'ĠW': 0.009, 'ĠB': 0.001, 'ĠQ': 0.001}\n",
      "12\t X P S 3. Second -> P\t ĠP 0.262 {'ĠX': 0.717, 'ĠP': 0.262, 'ĠS': 0.006, 'ĠQ': 0.002, 'ĠN': 0.001}\n",
      "13\t 5 E Q 3. First -> 5\t*Ġ5 0.929 {'Ġ5': 0.929, 'ĠE': 0.038, 'Ġ3': 0.008, 'ĠQ': 0.005, 'ĠS': 0.002}\n",
      "14\t 8 Y E M. Second -> Y\t*ĠY 0.429 {'ĠY': 0.429, 'ĠE': 0.312, 'Ġ8': 0.214, 'Ġ7': 0.006, 'Ġ9': 0.006}\n",
      "15\t E A V Q. First -> E\t*ĠE 0.644 {'ĠE': 0.644, 'ĠA': 0.336, 'Ġ8': 0.002, 'Ġ4': 0.001, 'Ġ6': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8558)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task_name = 'find majority'  ##?\n",
    "# task_name = 'find special kind'  ****\n",
    "# task_name = 'is same' / 'is same kind'  ****\n",
    "# task_name = 'find special easy' ## 6-2\n",
    "# task_name = 'A B C -> B' ##\n",
    "# task_name = 'set diff' ##?\n",
    "# task_name = 'A(BC->B' ##  6-1\n",
    "# task_name = 'ABC,AXC->X' ##?\n",
    "# task_name = 'reverse set diff' ##, *failed only on first position, GPT-3 has this problem, too\n",
    "# task_name = 'reverse set diff v2' ## A*C,ABC->B\n",
    "# task_name = 'find next easy' ## ABCDEF,\\nBC->D, 6+2\n",
    "inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "inputs = prepare_inputs(inputs, model.device)\n",
    "with torch.no_grad(): outputs = model(**inputs, output_attentions=True)\n",
    "input_ids = inputs.input_ids\n",
    "logits = outputs.logits\n",
    "bos_id = tokenizer._convert_token_to_id('Ġ->')\n",
    "crlf_id = tokenizer._convert_token_to_id('Ċ')\n",
    "bsz = input_ids.size(0); assert bsz == 1\n",
    "labels = torch.ones_like(input_ids) * (-100)\n",
    "for bi in range(bsz):\n",
    "    bos_indices = (input_ids[bi] == bos_id).nonzero().squeeze(1)\n",
    "    eos_indices = (input_ids[bi] == crlf_id).nonzero()[-nrows:].squeeze(1)\n",
    "    for i, (example, bos_i, eos_i) in enumerate(zip(examples, bos_indices.tolist(), eos_indices.tolist())):\n",
    "        print(i, end='\\t')\n",
    "        print(' ' + make_example_str(example), end='\\t')\n",
    "        ans_ids = input_ids[bi, bos_i + 1: eos_i]\n",
    "        if i >= 2: labels[bi, bos_i: eos_i - 1] = ans_ids\n",
    "        ans_prob_dist = logits[bi, bos_i: eos_i - 1].softmax(-1)\n",
    "        ans_probs = ans_prob_dist[torch.arange(ans_prob_dist.size(0)), ans_ids]\n",
    "        # for bi sample in every batch, fetch answer prob\n",
    "        ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids)\n",
    "        for ans_id, ans_token, ans_prob, dist in zip(ans_ids, ans_tokens, numpy(ans_probs, decimals=3), ans_prob_dist):\n",
    "            top1_correct = (dist.argmax() == ans_id).item()\n",
    "            print(('*' if top1_correct else ' ') + ans_token, ans_prob, \n",
    "                  show_topk(*dist.topk(5), indices_fn=tokenizer.convert_ids_to_tokens)) \n",
    "loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "bos_positions = (input_ids == bos_id).nonzero()[:,1]\n",
    "# the position before ans\n",
    "ans_positions = bos_positions + 1\n",
    "\n",
    "bos_watch_ind = 1\n",
    "\n",
    "src = bos_positions[1].item()\n",
    "# select answer position\n",
    "pred_label = outputs.logits[0, src].argmax().item()\n",
    "# predicted char on final answer position\n",
    "tokens = [token.replace('Ġ', '').replace('Ċ', '^') for token in tokenizer.tokenize(text)]\n",
    "seq_len = len(tokens)\n",
    "answer = tokens[src + 1]\n",
    "# standard final answer\n",
    "cand_range = range(eos_indices[bos_watch_ind - 1] + 1, bos_indices[bos_watch_ind])\n",
    "# condidates chars appreared in current sample (same line)\n",
    "n_candidates = len(cand_range); assert n_candidates >= 1, str(n_candidates)\n",
    "ans_fn = lambda x: x\n",
    "tgt = [i for i in cand_range if ans_fn(tokens[i]) == answer][0] if n_candidates > 1 else cand_range[0]\n",
    "# cand_positions = [i for i, token in enumerate(tokens[:-1]) if '^' in tokens[max(0, i - n_candidaes): i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_positions = (input_ids[bi] == crlf_id).nonzero()[:-1, 0]\n",
    "dot_id = tokenizer._convert_token_to_id('.')\n",
    "dot_positions = (input_ids[bi] == dot_id).nonzero()[:, 0]\n",
    "ans_fn = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tgt_positions = []\n",
    "for i in range(len(start_positions)):\n",
    "    start_pos, end_pos, ans_pos = start_positions[i], dot_positions[i], ans_positions[i]\n",
    "    for pos in range(start_pos, end_pos):\n",
    "        if ans_fn(tokens[pos]) == tokens[ans_pos]: tgt_positions.append(pos)\n",
    "tgt_positions = torch.LongTensor(tgt_positions)\n",
    "assert len(tgt_positions) == len(ans_positions), '%d != %d' % (len(tgt_positions), len(ans_positions))\n",
    "# cand_is_tgt = torch.LongTensor(cand_positions).view(-1, n_candidates) == tgt_positions.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, block in enumerate(blocks[:]):\n",
    "    block.attn_output, block.ffn_output = None, None\n",
    "    am = get_attn_module(block)\n",
    "    am.attention_mask, am.head_output, am.attn_out = None, None, None\n",
    "# get_attn_module(blocks[10]).hidden_states_mask = h_mask  ######\n",
    "# get_attn_module(blocks[layer2]).return_attn_logits = True\n",
    "try: \n",
    "    with torch.no_grad(): outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "finally:\n",
    "    attn_outputs, ffn_outputs, attn_hidden_states, attention_masks, head_outputs, attn_outs = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "    for i, block in enumerate(blocks[:]):\n",
    "        attn_outputs.append(getdelattr(block, 'attn_output'))\n",
    "        ffn_outputs.append(getdelattr(block, 'ffn_output'))\n",
    "        am = get_attn_module(block)\n",
    "        attention_masks.append(getdelattr(am, 'attention_mask'))\n",
    "        head_outputs.append(getdelattr(am, 'head_output'))\n",
    "        attn_outs.append(getdelattr(am, 'attn_out'))\n",
    "\n",
    "#     try_delattr(get_attn_module(blocks[10]), 'hidden_states_mask')   ######\n",
    "#     try_delattr(get_attn_module(blocks[layer2]), 'return_attn_logits')\n",
    "hidden_states = outputs.hidden_states\n",
    "attentions = outputs.attentions\n",
    "\n",
    "outputs.attn_outputs = attn_outputs\n",
    "outputs.ffn_outputs = ffn_outputs\n",
    "outputs.attn_outs = attn_outs\n",
    "outputs.head_outputs = head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'£': 0.022, '\\\\\\\\\\\\\\\\': 0.021, 'Ħ¢': 0.014, 'Ġ\"': 0.012, 'ł': 0.01}\n",
      "1 {'Ġcontraceptives': 0.016, 'Ġ_': 0.004, '.--': 0.004, 'Ġthe': 0.004, 'Ġher': 0.004}\n",
      "2 {'Ġcontraceptives': 0.003, 'Ġthe': 0.003, 'Ġa': 0.003, 'ĠâĢ': 0.003, 'Ġme': 0.003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {'Ġcontraceptives': 0.003, 'Ġa': 0.003, 'ĠâĢ': 0.002, 'Ġthe': 0.002, 'Ġme': 0.002}\n",
      "4 {'\\\\\\\\\\\\\\\\': 0.009, 'Ġcontraceptives': 0.005, 'Ġpart': 0.003, 'Ġ': 0.003, 'Ġa': 0.003}\n",
      "5 {'\\\\\\\\\\\\\\\\': 0.011, \"'/\": 0.003, 'Ġme': 0.003, 'Ġpart': 0.003, 'ĠâĢ': 0.003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 {'\\\\\\\\\\\\\\\\': 0.008, '_-': 0.003, \"'/\": 0.003, 'Ġme': 0.003, 'Ġ': 0.002}\n",
      "7 {'\\\\\\\\\\\\\\\\': 0.009, \"'/\": 0.004, 'Ġme': 0.003, 'Īè': 0.003, 'Ġ': 0.002}\n",
      "8 {'\\\\\\\\\\\\\\\\': 0.008, \"'/\": 0.004, 'Ġme': 0.003, '.--': 0.003, '_-': 0.003}\n",
      "9 {'\\\\\\\\\\\\\\\\': 0.011, \"'/\": 0.004, 'Īè': 0.003, 'Ġ<+': 0.002, 'Ľ': 0.002}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 {'\\\\\\\\\\\\\\\\': 0.008, \"'/\": 0.004, 'Īè': 0.004, 'Ľ': 0.003, 'ļé': 0.003}\n",
      "11 {'\\\\\\\\\\\\\\\\': 0.017, \"'/\": 0.008, '¹': 0.006, 'Ľ': 0.005, 'Ĳ': 0.004}\n",
      "12 {'\\\\\\\\\\\\\\\\': 0.015, 'Ĳ': 0.008, '¹': 0.008, \"'/\": 0.007, 'Ľ': 0.006}\n",
      "13 {'\\\\\\\\\\\\\\\\': 0.017, 'Ĳ': 0.009, 'Ľ': 0.008, 'Ġ<+': 0.007, '¹': 0.006}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 {'\\\\\\\\\\\\\\\\': 0.013, 'Ľ': 0.009, 'Ĳ': 0.007, \"'/\": 0.006, 'µ': 0.005}\n",
      "15 {'\\\\\\\\\\\\\\\\': 0.01, 'Ġ<+': 0.009, \"'/\": 0.008, 'Ġ': 0.004, 'ĠI': 0.004}\n",
      "16 {'Ġ<+': 0.009, 'Ġ_': 0.008, 'Ġ': 0.006, '\\\\\\\\\\\\\\\\': 0.006, \"'/\": 0.005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 {'Ġ_': 0.011, 'Ġ<+': 0.007, 'Ġ': 0.006, 'ĠR': 0.006, 'ĠP': 0.005}\n",
      "18 {'Ġ_': 0.018, 'ĠO': 0.011, 'Ġ': 0.008, 'ĠR': 0.007, 'Ġ{': 0.006}\n",
      "19 {'ĠO': 0.057, 'Ġ_': 0.018, 'ĠR': 0.013, 'ĠP': 0.007, 'Ġ': 0.007}\n",
      "20 {'ĠO': 0.092, 'ĠQ': 0.024, 'Ġ_': 0.022, 'ĠR': 0.021, 'ĠG': 0.019}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 {'ĠO': 0.324, 'ĠQ': 0.038, 'ĠG': 0.034, 'ĠR': 0.032, 'Ġ_': 0.013}\n",
      "22 {'ĠO': 0.336, 'ĠQ': 0.057, 'ĠG': 0.037, 'ĠR': 0.033, 'Ġ_': 0.017}\n",
      "23 {'ĠO': 0.609, 'ĠQ': 0.112, 'ĠG': 0.023, 'ĠR': 0.018, 'Ġ_': 0.011}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 {'ĠO': 0.609, 'ĠQ': 0.101, 'ĠG': 0.035, 'ĠR': 0.017, 'Ġ_': 0.012}\n",
      "25 {'ĠO': 0.621, 'ĠQ': 0.101, 'ĠG': 0.04, 'ĠR': 0.021, 'Ġ_': 0.011}\n",
      "26 {'ĠO': 0.711, 'ĠQ': 0.082, 'ĠG': 0.03, 'ĠR': 0.017, 'Ġo': 0.009}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 {'ĠO': 0.694, 'ĠQ': 0.1, 'ĠG': 0.032, 'ĠR': 0.017, 'Ġo': 0.008}\n",
      "28 {'ĠQ': 0.319, 'ĠO': 0.177, 'ĠG': 0.084, 'ĠR': 0.048, 'Ġ0': 0.028}\n",
      "29 {'ĠO': 0.323, 'ĠQ': 0.275, 'ĠG': 0.087, 'ĠR': 0.041, 'Ġ0': 0.031}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 {'ĠO': 0.515, 'ĠQ': 0.209, 'ĠG': 0.071, 'Ġ0': 0.033, 'ĠR': 0.031}\n",
      "31 {'ĠO': 0.515, 'ĠQ': 0.209, 'ĠG': 0.071, 'Ġ0': 0.033, 'ĠR': 0.031}\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "layer_out = L\n",
    "for i in range(0, L - 2 + 2):\n",
    "    h = hidden_states[i]\n",
    "    if i < L - 1: h = blocks[-2](h, attention_mask=attention_masks[-2])[0]\n",
    "    h = blocks[-1](h, attention_mask=attention_masks[-1])[0]\n",
    "    h = model.transformer.ln_f(h)\n",
    "    h = h[0, src]\n",
    "    logits = model.lm_head(h)\n",
    "    print(i, show_topk(*logits.softmax(-1).topk(5), indices_fn=tokenizer.convert_ids_to_tokens))\n",
    "    if logits.argmax() == input_ids[0, src + 1] and layer_out == L: layer_out = i\n",
    "print(layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attr_names = ['attn_output', 'ffn_output', 'attention_mask', 'attn_out']#, 'head_out']\n",
    "for i, block in enumerate(blocks):\n",
    "    for name in attr_names:\n",
    "        m = block if name.endswith('output') else get_attn_module(block)\n",
    "        setattr(m, name, None)\n",
    "try: \n",
    "    with torch.no_grad(): o = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "finally:\n",
    "    for i, block in enumerate(blocks):\n",
    "        for name in attr_names:\n",
    "            m = block if name.endswith('output') else get_attn_module(block)\n",
    "            if not hasattr(o, name): setattr(o, name, [])\n",
    "            getattr(o, name).append(getdelattr(m, name))\n",
    "hidden_states = o.hidden_states\n",
    "attentions = o.aw = o.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/19 [00:09<02:56,  9.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 2/19 [00:19<02:46,  9.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 3/19 [00:29<02:34,  9.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 4/19 [00:38<02:23,  9.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▋       | 5/19 [00:47<02:11,  9.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 6/19 [00:56<01:58,  9.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 7/19 [01:04<01:46,  8.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 8/19 [01:12<01:35,  8.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 9/19 [01:20<01:23,  8.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 10/19 [01:28<01:13,  8.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 11/19 [01:35<01:02,  7.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 12/19 [01:42<00:53,  7.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 13/19 [01:49<00:44,  7.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▎  | 14/19 [01:55<00:35,  7.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 15/19 [02:01<00:27,  6.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 16/19 [02:07<00:19,  6.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 17/19 [02:13<00:12,  6.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▍| 18/19 [02:18<00:05,  5.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 19/19 [02:23<00:00,  5.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 19/19 [02:23<00:00,  7.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task_name = 'find majority'  ##?\n",
    "\n",
    "\n",
    "# grad attribution\n",
    "pred_attn = False\n",
    "keys = ['aw'] #  'attn_out', 'head_out', 'attn_output', 'ffn_output'\n",
    "keys2 = [] #['head_output', 'attn_output']\n",
    "layer_range = (0, layer1) if pred_attn else (0, layer_out)\n",
    "# layer_out: total layers\n",
    "attrs, grads = defaultdict(list), defaultdict(list)\n",
    "num_points, batch_size = 5, 5\n",
    "\n",
    "layer0 = 9\n",
    "    \n",
    "for i in tqdm(range(*layer_range)):\n",
    "    am = get_attn_module(blocks[i])\n",
    "\n",
    "    scaled_emb, step, grad = {}, {}, {}\n",
    "    embs = [getattr(o, keys[0])[i]]\n",
    "    # model.attentions all attentions, [i], layer_i\n",
    "    if len(embs) == 1 and keys[0] != 'aw': all_embs[task_name][keys[0]] = embs[0][0]\n",
    "    # embs[0] batch_ind 0, embs[0][0], batch_ind 0, head 0\n",
    "        \n",
    "    for key, emb in zip(keys, embs):\n",
    "        scaled_emb[key], step[key] = scaled_input(emb, num_points)\n",
    "        _ = scaled_emb[key].requires_grad_(True)\n",
    "        grad[key] = None\n",
    "    if i == layer0: ys = []\n",
    "    # why layer 9 need deal with special\n",
    "    for j in range(0, num_points, batch_size):\n",
    "        sliced_scaled_emb = [scaled_emb[key][j: j + batch_size] for key in keys]\n",
    "        outputs = forward(am, keys, values=sliced_scaled_emb, exit_module=blocks[layer1+1] if pred_attn else None)\n",
    "        # tocreate: exit_module ? only forward one layers?\n",
    "        y = (\n",
    "            globalize(outputs.attentions[layer2])[:, head2, src, tgt]\n",
    "            if pred_attn\n",
    "            else outputs.logits.softmax(-1)[:, src, pred_label]\n",
    "        )\n",
    "        # after change attention, get src predicted result\n",
    "        if i == layer0: ys.append(y);\n",
    "#         if keys2:\n",
    "#             sliced_scaled_emb2 = [getdelattr(am if key in ['head_output'] else blocks[i], key) for key in keys2]\n",
    "#             sliced_scaled_emb += sliced_scaled_emb2\n",
    "#             if j == num_points - batch_size: step.update({key: emb[-1:]/num_points for key, emb in zip(keys2, sliced_scaled_emb2)})\n",
    "        sliced_grads = torch.autograd.grad(y.flatten().unbind(), sliced_scaled_emb)\n",
    "        for gi, key in enumerate(keys + keys2):\n",
    "            # you wrap key and grad in dict format at forward, so extract it here\n",
    "            sliced_grad = sliced_grads[gi].sum(dim=0, keepdim=True)\n",
    "            # sum across head\n",
    "            grad[key] = sliced_grad if key not in grad or grad[key] is None else grad[key] + sliced_grad\n",
    "    for key in keys + keys2:\n",
    "        attr = grad[key] * step[key]\n",
    "        attrs[key].append(attr.data)\n",
    "        grads[key].append(grad[key].data)\n",
    "\n",
    "\n",
    "if len(keys) == 1:\n",
    "    key = keys[0]\n",
    "    all_attrs[task_name][key + str(int(pred_attn))] = torch.cat([globalize(a) for a in attrs[key]]) \\\n",
    "        if key == 'aw' else attrs[key][0][0]\n",
    "#     for key in keys2: attrs[key] = torch.cat(attrs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   1 6          2 R          3 L          4 0          5 .          6 Second     7 ->         8 R       \n",
      "  10 0         11 O         12 Q         13 G         14 .         15 Second    16 ->        17 O       \n",
      "  19 A         20 W         21 X         22 9         23 .         24 First     25 ->        26 A       \n",
      "  28 N         29 D         30 P         31 X         32 .         33 Second    34 ->        35 D       \n",
      "  37 J         38 5         39 L         40 Z         41 .         42 First     43 ->        44 J       \n",
      "  46 Q         47 9         48 V         49 B         50 .         51 First     52 ->        53 Q       \n",
      "  55 M         56 W         57 V         58 B         59 .         60 Second    61 ->        62 W       \n",
      "  64 8         65 9         66 0         67 L         68 .         69 Second    70 ->        71 9       \n",
      "  73 2         74 B         75 1         76 Q         77 .         78 First     79 ->        80 2       \n",
      "  82 E         83 O         84 5         85 7         86 .         87 Second    88 ->        89 O       \n",
      "  91 6         92 S         93 Q         94 X         95 .         96 Second    97 ->        98 S       \n",
      " 100 A        101 U        102 W        103 N        104 .        105 Second   106 ->       107 U       \n",
      " 109 X        110 P        111 S        112 3        113 .        114 Second   115 ->       116 P       \n",
      " 118 5        119 E        120 Q        121 3        122 .        123 First    124 ->       125 5       \n",
      " 127 8        128 Y        129 E        130 M        131 .        132 Second   133 ->       134 Y       \n",
      " 136 E        137 A        138 V        139 Q        140 .        141 First    142 ->       143 E       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  2,  11,  19,  29,  37,  46,  56,  65,  73,  83,  92, 101, 110, 118,\n",
       "        128, 136])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, token in enumerate(tokens):\n",
    "    if token in ['Ċ', '^']: print()\n",
    "    else: print('%4d %-6s' %(i, token), end='  ')\n",
    "tgt_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_top_heads(values, indices, src_indices=None, tgt_indices=None, topk=15):\n",
    "    val, ind = values.sum(dim=-1).view(-1).topk(topk)\n",
    "    # sort by importance across layer and head\n",
    "    val, ind = numpy(val), unravel_index(ind, values.size()[:-1])\n",
    "    # get topk head importance and it's index\n",
    "    for (l, h), v in zip(ind, val):\n",
    "        _l = l + layer_range[0]\n",
    "        if _l <= 3: continue\n",
    "        top_links = list(zip(unravel_index(indices[l, h], (seq_len, seq_len)), numpy(values[l, h], decimals=3)))\n",
    "        # for each head, which position is important, and deserve to attend\n",
    "        if src_indices is not None: top_links = [([src_indices[_s], _t], _v) for [_s, _t], _v in top_links]\n",
    "        if tgt_indices is not None: top_links = [([_s, tgt_indices[_t]], _v) for [_s, _t], _v in top_links]\n",
    "        top_links = [\n",
    "            ([_s, _t], _v, numpy(globalize(attentions[_l]) * 100, decimals=1)[0, h, _s, _t])\n",
    "            for [_s, _t], _v in top_links\n",
    "        ]\n",
    "        _top_links = [([_s, _t], _v, _a) if len(src_indices) > 1 else (_t, _v, _a) for [_s, _t], _v, _a in top_links]\n",
    "        print('%d-%d\\t%.3f' % (_l, h, v), _top_links, end='\\t') \n",
    "        if len(top_links) == 1:\n",
    "            probs = numpy(globalize(attentions[_l])[0, h, src])\n",
    "            for i in cand_range:\n",
    "                # consider the candidate position\n",
    "                if i == tgt: print('*', end='')\n",
    "                # if only concern the total attention, whether current head attend to tgt position\n",
    "                print('%.10f' % probs[i], end='  ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = all_attrs[task_name]['aw' + str(int(pred_attn))]\n",
    "# layers, heads, src, tgt aggreation importance\n",
    "a = a / a.view(a.size(0), -1).norm(dim=1)[:, None, None, None] #.view(a.size(0), 1, 1, 1)\n",
    "# why view as (layer, None) without consider head, and then norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-2\t0.740 [(11, 0.74, 51.4)]\t0.2974568009  *0.5141043663  0.0332061015  0.0075412504  0.0062532043  0.0632689372  \n",
      "17-13\t0.720 [(11, 0.72, 40.6)]\t0.1338693500  *0.4062977135  0.2799373269  0.0488652773  0.0570787601  0.0469219834  \n",
      "18-6\t0.564 [(11, 0.564, 38.3)]\t0.0059909946  *0.3829750717  0.0287115872  0.0404837653  0.0275268182  0.0021626095  \n",
      "14-0\t0.434 [(11, 0.434, 35.4)]\t0.2564381659  *0.3544862270  0.0144302659  0.0112256641  0.0091148661  0.0026933218  \n",
      "16-15\t0.321 [(11, 0.321, 38.4)]\t0.0221272521  *0.3840020597  0.0436470658  0.0632935166  0.0222951639  0.0056860875  \n",
      "17-4\t0.276 [(11, 0.276, 16.2)]\t0.1624967456  *0.1618607640  0.1096638665  0.0851531997  0.2876665592  0.0091682784  \n",
      "13-3\t0.160 [(11, 0.16, 35.2)]\t0.3054032624  *0.3522364795  0.1022410989  0.0409076922  0.0236934461  0.0492714904  \n",
      "17-19\t0.141 [(11, 0.141, 33.6)]\t0.0271344967  *0.3356562257  0.1744240373  0.1040123999  0.0244711805  0.0855213851  \n",
      "13-12\t0.118 [(11, 0.118, 31.7)]\t0.0705380142  *0.3166122139  0.1842771471  0.0755118728  0.0828817040  0.0929684043  \n",
      "17-17\t0.114 [(11, 0.114, 15.2)]\t0.2349504530  *0.1519919783  0.1648976505  0.0964411274  0.0267880913  0.2298793495  \n",
      "15-8\t0.101 [(11, 0.101, 13.5)]\t0.0898014307  *0.1348850727  0.1734550446  0.4582815468  0.0196586959  0.0516327955  \n",
      "13-5\t0.091 [(11, 0.091, 10.5)]\t0.0328817964  *0.1046225801  0.1434654593  0.0125035867  0.5428848267  0.0173163880  \n",
      "14-2\t0.059 [(11, 0.059, 6.4)]\t0.0185153913  *0.0641627833  0.0342495330  0.1025158763  0.0173246656  0.0149492528  \n",
      "17-1\t0.049 [(11, 0.049, 27.2)]\t0.1303088814  *0.2721346021  0.1346465200  0.2957781851  0.0598284043  0.0218802188  \n",
      "13-19\t0.045 [(11, 0.045, 19.7)]\t0.0237843916  *0.1972315460  0.1354620755  0.0505681075  0.1798789203  0.0120232189  \n",
      "\n",
      "14-6\t0.830 [(8, 0.798, 84.2), (3, 0.011, 2.4), (12, 0.009, 1.5), (11, 0.005, 0.9), (2, 0.004, 1.0), (13, 0.003, 1.4), (0, 0.001, 1.8), (4, 0.001, 0.2)]\t\n",
      "17-13\t0.761 [(11, 0.72, 40.6), (10, 0.04, 13.4), (3, 0.001, 0.1), (9, 0.0, 0.7), (0, 0.0, 0.1), (97, -0.0, 0.0), (99, -0.0, 0.0), (100, 0.0, 0.0)]\t\n",
      "13-2\t0.747 [(11, 0.74, 51.4), (9, 0.007, 4.0), (14, 0.0, 0.6), (3, 0.0, 0.1), (97, 0.0, 0.0), (100, 0.0, 0.0), (99, 0.0, 0.0), (95, 0.0, 0.0)]\t\n",
      "12-18\t0.676 [(8, 0.676, 83.3), (3, 0.0, 0.0), (15, 0.0, 0.1), (2, 0.0, 0.0), (11, 0.0, 0.0), (99, -0.0, 0.0), (100, -0.0, 0.0), (98, 0.0, 0.0)]\t\n",
      "18-6\t0.574 [(11, 0.564, 38.3), (7, 0.004, 1.3), (0, 0.001, 30.7), (10, 0.001, 0.6), (4, 0.001, 1.7), (6, 0.001, 2.9), (9, 0.001, 0.9), (16, 0.0, 0.4)]\t\n",
      "17-4\t0.557 [(11, 0.276, 16.2), (14, 0.097, 28.8), (12, 0.092, 11.0), (10, 0.035, 16.2), (9, 0.031, 6.4), (13, 0.018, 8.5), (3, 0.006, 0.9), (0, 0.002, 1.2)]\t\n",
      "10-8\t0.552 [(8, 0.515, 93.0), (0, 0.02, 3.4), (1, 0.004, 0.7), (10, 0.004, 0.7), (16, 0.003, 0.7), (15, 0.003, 0.5), (6, 0.002, 0.4), (7, 0.001, 0.2)]\t\n",
      "13-5\t0.502 [(14, 0.283, 54.3), (12, 0.115, 14.3), (11, 0.091, 10.5), (10, 0.006, 3.3), (13, 0.003, 1.3), (7, 0.003, 1.5), (3, 0.001, 0.6), (1, 0.0, 0.0)]\t\n",
      "15-1\t0.474 [(12, 0.208, 28.7), (13, 0.194, 27.7), (11, 0.04, 8.3), (14, 0.013, 2.3), (15, 0.012, 6.5), (8, 0.005, 2.3), (10, 0.002, 0.5), (4, 0.001, 0.1)]\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-0\t0.454 [(11, 0.434, 35.4), (0, 0.013, 30.5), (9, 0.006, 1.5), (3, 0.002, 0.3), (7, 0.0, 0.1), (16, 0.0, 0.3), (1, 0.0, 0.0), (100, 0.0, 0.0)]\t\n",
      "13-9\t0.442 [(16, 0.264, 46.1), (15, 0.127, 9.7), (8, 0.022, 3.5), (7, 0.009, 2.3), (9, 0.006, 6.2), (6, 0.006, 0.7), (14, 0.004, 6.0), (2, 0.004, 0.4)]\t\n",
      "12-15\t0.399 [(9, 0.385, 86.0), (10, 0.006, 3.1), (6, 0.006, 1.7), (16, 0.001, 0.9), (4, 0.001, 0.1), (15, 0.0, 1.2), (14, 0.0, 0.1), (3, 0.0, 0.0)]\t\n",
      "16-15\t0.359 [(11, 0.321, 38.4), (0, 0.012, 24.1), (16, 0.01, 2.0), (9, 0.007, 0.9), (14, 0.007, 2.2), (5, 0.001, 0.8), (3, 0.0, 2.6), (1, 0.0, 0.1)]\t\n",
      "8-19\t0.328 [(8, 0.326, 74.3), (15, 0.001, 0.3), (6, 0.0, 0.2), (1, 0.0, 0.0), (11, 0.0, 0.0), (5, 0.0, 0.0), (12, 0.0, 0.0), (13, 0.0, 0.0)]\t\n",
      "13-3\t0.325 [(11, 0.16, 35.2), (10, 0.103, 30.5), (15, 0.023, 4.9), (12, 0.023, 10.2), (16, 0.005, 1.9), (13, 0.004, 4.1), (8, 0.004, 1.1), (9, 0.003, 1.3)]\t\n"
     ]
    }
   ],
   "source": [
    "if not pred_attn:\n",
    "    src_indices, tgt_indices = [src], [tgt]\n",
    "    _a = a[:, :, src_indices, tgt_indices]\n",
    "    values, indices = _a.view(_a.size(0), H, -1).topk(1, dim=-1)\n",
    "    # value is importance, and indices is direct from src_indices to tgt_indices, in this case always 0 -> 0\n",
    "    # cause these two list have only one element\n",
    "    show_top_heads(values, indices, src_indices=src_indices, tgt_indices=tgt_indices)\n",
    "    # use integrate gradient method, which head is most important\n",
    "    # layer-head, head-importance, (tgt_attention, head_importance, attention_score * 100), '\\t'.join(candidate's scores)\n",
    "    print()\n",
    "# src_indices = numpy(ans_positions[:])\n",
    "# src_indices = numpy(tgt_positions + 1)\n",
    "# tgt_indices = tgt_positions\n",
    "_a = a[:, :, src_indices, :]\n",
    "values, indices = _a.view(_a.size(0), H, -1).topk(nrows // 2, dim=-1)\n",
    "show_top_heads(values, indices, src_indices=src_indices)#, tgt_indices=tgt_indices)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.12.0"
   }
  },
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "child_analysis_yuhe.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
