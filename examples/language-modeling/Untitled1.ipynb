{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "device_mappings = {0: 1, 1: 5, 2: 6, 3: 7, 4: 2, 5: 3, 6: 0, 7: 4}\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_mappings[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/nas/xd/.cache/torch/transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_language_modeling import DataTrainingArguments, get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"WARNING\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0053a0358c2d4290831e220191d17fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33faf9f1756a4b33a37902742591545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1202.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f310ce70e8694cefbb83e95c00091510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=11406547936.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading state_dict took 189.682 sec\n"
     ]
    }
   ],
   "source": [
    "proxies = {'https': '192.168.50.1:1081'}\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-3b', proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = /nas/xd/.cache/torch/transformers/t5-11b-config.json\n"
     ]
    }
   ],
   "source": [
    "config11b = AutoConfig.from_pretrained('/nas/xd/.cache/torch/transformers/t5-11b-config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cached_path: url_or_filename = /nas/xd/.cache/torch/transformers/t5-11b-pytorch_model.bin\n",
      "loading state_dict took 750.661 sec\n"
     ]
    }
   ],
   "source": [
    " model11b = T5ForConditionalGeneration.from_pretrained('/nas/xd/.cache/torch/transformers/t5-11b-pytorch_model.bin', config=config11b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model11b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Z', '▁is', '▁larger', '▁than', '▁Z', '?', '<extra_id_0>', '▁Z', '▁is', '▁smaller', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text = 'Z is larger than Z? <extra_id_0> Z is smaller. </s>'\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "# outputs = model.generate(input_ids)\n",
    "# tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Z is smaller than X? <extra_id_0> Z is smaller. </s>',\n",
       " 'Z is smaller than X? <extra_id_0> X is larger. </s>',\n",
       " 'X is larger than Z? <extra_id_0> Z is smaller. </s>',\n",
       " 'X is larger than Z? <extra_id_0> X is larger. </s>',\n",
       " 'Z is smaller than X? <extra_id_0> X is smaller. </s>',\n",
       " 'Z is smaller than X? <extra_id_0> Z is larger. </s>',\n",
       " 'X is larger than Z? <extra_id_0> X is smaller. </s>',\n",
       " 'X is larger than Z? <extra_id_0> Z is larger. </s>')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [('Z is smaller than X? <extra_id_0> Z is smaller. </s>', 'Right'),\n",
    " ('Z is smaller than X? <extra_id_0> X is larger. </s>', 'Right'),\n",
    " ('X is larger than Z? <extra_id_0> Z is smaller. </s>', 'Right'),\n",
    " ('X is larger than Z? <extra_id_0> X is larger. </s>', 'Right'),\n",
    " ('Z is smaller than X? <extra_id_0> X is smaller. </s>', 'Wrong'),\n",
    " ('Z is smaller than X? <extra_id_0> Z is larger. </s>', 'Wrong'),\n",
    " ('X is larger than Z? <extra_id_0> X is smaller. </s>', 'Wrong'),\n",
    " ('X is larger than Z? <extra_id_0> Z is larger. </s>', 'Wrong')]\n",
    "texts, labels = zip(*examples)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Z', '▁is', '▁smaller', '▁than', '▁', 'X', '?', '<extra_id_0>', '▁Z', '▁is', '▁smaller', '.', '</s>', '<pad>']\n",
      "['▁Z', '▁is', '▁smaller', '▁than', '▁', 'X', '?', '<extra_id_0>', '▁', 'X', '▁is', '▁larger', '.', '</s>']\n",
      "['▁', 'X', '▁is', '▁larger', '▁than', '▁Z', '?', '<extra_id_0>', '▁Z', '▁is', '▁smaller', '.', '</s>', '<pad>']\n",
      "['▁', 'X', '▁is', '▁larger', '▁than', '▁Z', '?', '<extra_id_0>', '▁', 'X', '▁is', '▁larger', '.', '</s>']\n",
      "['▁Z', '▁is', '▁smaller', '▁than', '▁', 'X', '?', '<extra_id_0>', '▁', 'X', '▁is', '▁smaller', '.', '</s>']\n",
      "['▁Z', '▁is', '▁smaller', '▁than', '▁', 'X', '?', '<extra_id_0>', '▁Z', '▁is', '▁larger', '.', '</s>', '<pad>']\n",
      "['▁', 'X', '▁is', '▁larger', '▁than', '▁Z', '?', '<extra_id_0>', '▁', 'X', '▁is', '▁smaller', '.', '</s>']\n",
      "['▁', 'X', '▁is', '▁larger', '▁than', '▁Z', '?', '<extra_id_0>', '▁Z', '▁is', '▁larger', '.', '</s>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.batch_encode_plus(texts[:], add_special_tokens=False, padding=True, return_tensors='pt')\n",
    "# inputs['input_ids'].size()\n",
    "# inputs['input_ids']\n",
    "# inputs['attention_mask']\n",
    "input_ids = inputs['input_ids']\n",
    "for input_id in input_ids: print(tokenizer.convert_ids_to_tokens(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_length=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,  2163,     6, 32098,     3],\n",
       "        [    0, 32099,   465,     6, 32098,   145],\n",
       "        [    0, 32099,    37,    29, 32098,     3],\n",
       "        [    0, 32099,    37,    29, 32098,     3],\n",
       "        [    0, 32099,   465,     6, 32098,    19],\n",
       "        [    0, 32099,   465,     6, 32098,   145],\n",
       "        [    0, 32099,   465,     6, 32098,     3],\n",
       "        [    0, 32099,   465,     6, 32098,     5]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = outputs[2]\n",
    "output[0]\n",
    "assert torch.all(outputs[:, 0] == tokenizer.pad_token_id)\n",
    "assert torch.all(outputs[:, 1] == tokenizer.additional_special_tokens_ids[0])  # '<extra_id_0>'\n",
    "assert (outputs == tokenizer.additional_special_tokens_ids[1]).sum() == outputs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[2:]).split(tokenizer.additional_special_tokens[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z is smaller than X? <extra_id_0> Z is smaller. </s> Right <extra_id_0> Yes,<extra_id_1> \n",
      "['<pad>', '<extra_id_0>', '▁Yes', ',', '<extra_id_1>', '▁']\n",
      "Z is smaller than X? <extra_id_0> X is larger. </s> Right <extra_id_0> No,<extra_id_1> than\n",
      "['<pad>', '<extra_id_0>', '▁No', ',', '<extra_id_1>', '▁than']\n",
      "X is larger than Z? <extra_id_0> Z is smaller. </s> Right <extra_id_0> Then<extra_id_1> \n",
      "['<pad>', '<extra_id_0>', '▁The', 'n', '<extra_id_1>', '▁']\n",
      "X is larger than Z? <extra_id_0> X is larger. </s> Right <extra_id_0> Then<extra_id_1> \n",
      "['<pad>', '<extra_id_0>', '▁The', 'n', '<extra_id_1>', '▁']\n",
      "Z is smaller than X? <extra_id_0> X is smaller. </s> Wrong <extra_id_0> No,<extra_id_1> is\n",
      "['<pad>', '<extra_id_0>', '▁No', ',', '<extra_id_1>', '▁is']\n",
      "Z is smaller than X? <extra_id_0> Z is larger. </s> Wrong <extra_id_0> No,<extra_id_1> than\n",
      "['<pad>', '<extra_id_0>', '▁No', ',', '<extra_id_1>', '▁than']\n",
      "X is larger than Z? <extra_id_0> X is smaller. </s> Wrong <extra_id_0> No,<extra_id_1> \n",
      "['<pad>', '<extra_id_0>', '▁No', ',', '<extra_id_1>', '▁']\n",
      "X is larger than Z? <extra_id_0> Z is larger. </s> Wrong <extra_id_0> No,<extra_id_1>.\n",
      "['<pad>', '<extra_id_0>', '▁No', ',', '<extra_id_1>', '.']\n"
     ]
    }
   ],
   "source": [
    "for text, label, output in zip(texts, labels, outputs):\n",
    "    print(text, label, tokenizer.decode(output, clean_up_tokenization_spaces=False))\n",
    "    print(tokenizer.convert_ids_to_tokens(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE = /nas/xd/.cache/torch/transformers\n",
      "In cached_path: url_or_filename = https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-config.json\n",
      "In url_to_filename: url = https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-config.json\n",
      "In url_to_filename: filename = 5eece39a4bfbb8cc0ffa95a83bb96cf22116bd653e3dc631f5a61cb7485b2231\n",
      "In url_to_filename: filename.etag = 5eece39a4bfbb8cc0ffa95a83bb96cf22116bd653e3dc631f5a61cb7485b2231.330574dd7bece9f3ac99bd88cd0150f5bb560cf6a994df98965924d91cc87471\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/5eece39a4bfbb8cc0ffa95a83bb96cf22116bd653e3dc631f5a61cb7485b2231.330574dd7bece9f3ac99bd88cd0150f5bb560cf6a994df98965924d91cc87471\n",
      "In cached_path: url_or_filename = https://cdn.huggingface.co/t5-3b-pytorch_model.bin\n",
      "In url_to_filename: url = https://cdn.huggingface.co/t5-3b-pytorch_model.bin\n",
      "In url_to_filename: filename = 17dc32cd6f6b528a7ab83521971a0ca1c81962ed03f703f88c6d7483f00f8d15\n",
      "In url_to_filename: filename.etag = 17dc32cd6f6b528a7ab83521971a0ca1c81962ed03f703f88c6d7483f00f8d15.e9c0369f4fb16cc443713441903efe567ad25b3d37123e1fb517b6131d0aa673\n",
      "In cached_path: output_path = /nas/xd/.cache/torch/transformers/17dc32cd6f6b528a7ab83521971a0ca1c81962ed03f703f88c6d7483f00f8d15.e9c0369f4fb16cc443713441903efe567ad25b3d37123e1fb517b6131d0aa673\n"
     ]
    }
   ],
   "source": [
    "model2 = T5ForConditionalGeneration.from_pretrained('t5-3b', proxies=proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/xd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name, config=config)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataTrainingArguments()\n",
    "data_args.eval_data_file = '/nas/xd/data/wikitext-103-raw/wiki.test.raw.detok'\n",
    "data_args.block_size = tokenizer.max_len\n",
    "data_args\n",
    "\n",
    "eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability)\n",
    "dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "= Robert Boulter =\n",
      "\n",
      "Robert Boulter is an English film, television and theatre actor. He had a guest-starring role on the television series The Bill in 2000. This was followed by a starring role in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre. He had a guest role in the television series Judge John Deed in 2002. In 2004 Boulter landed a role as \"Craig\" in the episode \"Teddy's Story\" of the television series The Long Firm; he starred alongside actors Mark Strong and Derek Jacobi. He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur, which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London. He was directed by John Tiffany and starred alongside Ben Whishaw, Shane Zaza, Harry Kent, Fraser Ayres, Sophie Stanton and Dominic Hall.\n",
      "In 2006, Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill. He appeared on a 2006 episode of the television series, Doctors, followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke. How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham. Boulter starred in two films in 2008, Daylight Robbery by filmmaker Paris Leonti, and Donkey Punch directed by Olly Blackburn. In May 2008, Boulter made a guest appearance on a two-part episode arc of the television series Waking the Dead, followed by an appearance on the television series Survivors in November 2008. He had a recurring role in ten episodes of the television series Casualty in 2010, as \"Kieron Fletcher\". Boulter starred in the 2011 film Mercenaries directed by Paris Leonti.\n",
      "\n",
      "= = Career = =\n",
      "\n",
      "\n",
      "= = = 2000 – 2005 = = =\n",
      "\n",
      "In 2000 Boulter had a guest-starring role on the television series The Bill; he portrayed \"Scott Parry\" in the episode, \"In Safe Hands\". Boulter starred as \"Scott\" in the play Herons written by Simon Stephens, which was performed in 2001 at the Royal Court Theatre. A review of Boulter's performance in The Independent on Sunday described him as \"horribly menacing\" in the role, and he received critical reviews in The Herald, and Evening Standard. He appeared in the television series Judge John Deed in 2002 as \"Addem Armitage\" in the episode \"Political Expediency\", and had a role as a different character \"Toby Steele\" on The Bill.\n",
      "He had a recurring role in 2003 on two episodes of The Bill, as character \"Connor Price\". In 2004 Boulter landed a role as \"Craig\" in the episode \"Teddy's Story\" of the television series The Long Firm; he starred alongside actors Mark Strong and Derek Jacobi. Boulter starred as \"Darren\", in the 2005 theatre productions of the Philip Ridley play Mercury Fur. It was performed at the Drum Theatre in Plymouth, and the Menier Chocolate Factory in London. He was directed by John Tiffany and starred alongside Ben Whishaw, Shane Zaza, Harry Kent, Fraser Ayres, Sophie Stanton and Dominic Hall. Boulter received a favorable review in The Daily Telegraph: \"The acting is shatteringly intense, with wired performances from Ben Whishaw (now unrecognisable from his performance as Trevor Nunn's Hamlet), Robert Boulter, Shane Zaza and Fraser Ayres.\" The Guardian noted, \"Ben Whishaw and Robert Boulter offer tenderness amid the savagery.\"\n",
      "\n",
      "= = = 2006 – present = = =\n",
      "\n",
      "In 2006 Boulter starred in the play Citizenship written by Mark Ravenhill. The play was part of a series which featured different playwrights, titled Burn / Chatroom / Citizenship. In a 2006 interview, fellow actor Ben Whishaw identified Boulter as one of his favorite co-stars: \"I loved working with a guy called Robert Boulter, who was in the triple bill of Burn, Chatroom and Citizenship at the National. He played my brother in Mercury Fur.\" He portrayed \"Jason Tyler\" on the 2006 episode of the television series, Doctors, titled \"Something I Ate\". Boulter starred as \"William\" in the 2007 production of How to Curse directed by Josie Rourke. How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham. In a review of the production for The Daily Telegraph, theatre critic Charles Spencer noted, \"Robert Boulter brings a touching vulnerability to the stage as William.\"\n",
      "Boulter starred in two films in 2008, Daylight Robbery by filmmaker Paris Leonti, and Donkey Punch directed by Olly Blackburn. Boulter portrayed a character named \"Sean\" in Donkey Punch, who tags along with character \"Josh\" as the \"quiet brother...\n"
     ]
    }
   ],
   "source": [
    "for i, inputs in enumerate(dataloader):\n",
    "    if i == 1: break\n",
    "    print(i)\n",
    "    print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
